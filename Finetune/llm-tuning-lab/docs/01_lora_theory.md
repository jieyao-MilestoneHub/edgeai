# ğŸ“™ LoRA ç†è«–

> Low-Rank Adaptation of Large Language Models

## ğŸ¯ å­¸ç¿’ç›®æ¨™

é–±è®€æœ¬æ–‡å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

- âœ… ç†è§£ç‚ºä»€éº¼éœ€è¦ LoRA
- âœ… æŒæ¡ä½ç§©åˆ†è§£ (Low-Rank Decomposition) åŸç†
- âœ… è¨ˆç®— LoRA çš„åƒæ•¸é‡èˆ‡è¨˜æ†¶é«”ç¯€çœ
- âœ… æ‰‹å¯« LoRA æ¨¡çµ„å¯¦ä½œ
- âœ… èª¿æ•´ rank èˆ‡ alpha è¶…åƒæ•¸

---

## ğŸ¤” ç‚ºä»€éº¼éœ€è¦ LoRAï¼Ÿ

### å…¨åƒæ•¸å¾®èª¿çš„æŒ‘æˆ°

å‡è¨­æˆ‘å€‘è¦å¾®èª¿ LLaMA-7B æ¨¡å‹ï¼š

```
æ¨¡å‹åƒæ•¸ï¼š7B (70 å„„)
ç²¾åº¦ï¼šFP16 (2 bytes/param)
è¨˜æ†¶é«”éœ€æ±‚ï¼š7B Ã— 2 = 14GB (åƒ…æ¬Šé‡)

è¨“ç·´æ™‚é¡å¤–éœ€æ±‚ï¼š
- Optimizer states (Adam): 2Ã— weights = 28GB
- Gradients: 1Ã— weights = 14GB
- Activations: ~20GB

ç¸½è¨ˆï¼š~76GB
```

**å•é¡Œï¼š**
- âŒ å–®å¼µ A100 (40GB) ç„¡æ³•è¨“ç·´
- âŒ æ¯å€‹ä¸‹æ¸¸ä»»å‹™éƒ½éœ€è¦å®Œæ•´æ¨¡å‹å‰¯æœ¬
- âŒ éƒ¨ç½²æ™‚éœ€è¦è¼‰å…¥æ•´å€‹æ¨¡å‹

---

## ğŸ’¡ LoRA æ ¸å¿ƒæ€æƒ³

### é—œéµæ´å¯Ÿ

> **å‡è¨­ï¼šé è¨“ç·´æ¨¡å‹çš„æ¬Šé‡æ›´æ–°å­˜åœ¨æ–¼ä½ç§©å­ç©ºé–“**

æ•¸å­¸è¡¨è¿°ï¼š

```
åŸå§‹å…¨åƒæ•¸æ›´æ–°ï¼š
W' = Wâ‚€ + Î”W

å…¶ä¸­ Î”W âˆˆ â„^(dÃ—k) æ˜¯ä½ç§©çš„ï¼šrank(Î”W) << min(d, k)

LoRA è¿‘ä¼¼ï¼š
W' = Wâ‚€ + BA

å…¶ä¸­ï¼š
- B âˆˆ â„^(dÃ—r)
- A âˆˆ â„^(rÃ—k)
- r << min(d, k)  (r æ˜¯ rank)
```

### è¦–è¦ºåŒ–ç†è§£

```
Full Fine-tuning:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Wâ‚€ (dÃ—k)  â”‚  â”€â”€â”€â”€â”€>  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (frozen)   â”‚           â”‚  Wâ‚€ + Î”W    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚  (dÃ—k)      â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          å¯è¨“ç·´åƒæ•¸ï¼šd Ã— k

LoRA:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Wâ‚€ (dÃ—k)  â”‚ (frozen)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       +
    â”Œâ”€â”€â”€â”     â”Œâ”€â”€â”€â”
    â”‚ B â”‚  Ã—  â”‚ A â”‚
    â”‚dÃ—râ”‚     â”‚rÃ—kâ”‚
    â””â”€â”€â”€â”˜     â””â”€â”€â”€â”˜
    å¯è¨“ç·´åƒæ•¸ï¼šdÃ—r + rÃ—k
```

---

## ğŸ”¬ æ•¸å­¸åŸç†

### 1. ä½ç§©åˆ†è§£ (Low-Rank Decomposition)

**å®šç†ï¼š** ä»»ä½•çŸ©é™£ M âˆˆ â„^(mÃ—n) éƒ½å¯ä»¥åˆ†è§£ç‚ºï¼š

```
M = UÎ£Váµ€  (SVD)

å…¶ä¸­ï¼š
- U âˆˆ â„^(mÃ—r): å·¦å¥‡ç•°å‘é‡
- Î£ âˆˆ â„^(rÃ—r): å¥‡ç•°å€¼å°è§’çŸ©é™£
- V âˆˆ â„^(nÃ—r): å³å¥‡ç•°å‘é‡
- r = rank(M)
```

**ä½ç§©è¿‘ä¼¼ï¼š** ä¿ç•™å‰ k å€‹æœ€å¤§å¥‡ç•°å€¼ï¼š

```
M â‰ˆ M_k = U_k Î£_k V_k^T

å…¶ä¸­ k << r
```

### 2. LoRA å‰å‘å‚³æ’­

```python
# åŸå§‹ç·šæ€§å±¤
h = Wâ‚€x

# LoRA ä¿®æ”¹å¾Œ
h = Wâ‚€x + BAx
  = Wâ‚€x + (BA)x

å…¶ä¸­ï¼š
- x âˆˆ â„^k: è¼¸å…¥
- Wâ‚€ âˆˆ â„^(dÃ—k): å‡çµçš„åŸå§‹æ¬Šé‡
- B âˆˆ â„^(dÃ—r): å¯è¨“ç·´
- A âˆˆ â„^(rÃ—k): å¯è¨“ç·´
```

### 3. ç¸®æ”¾å› å­ Alpha

```python
h = Wâ‚€x + (Î±/r) Ã— BAx

å…¶ä¸­ï¼š
- Î±: ç¸®æ”¾è¶…åƒæ•¸ (é€šå¸¸è¨­ç‚º rank çš„ 1-2 å€)
- r: rank
```

**ç‚ºä»€éº¼éœ€è¦ Î±/rï¼Ÿ**
- æ§åˆ¶ LoRA æ¬Šé‡çš„å½±éŸ¿ç¨‹åº¦
- ä¸åŒ rank ä¹‹é–“çš„å­¸ç¿’ç‡æ¨™æº–åŒ–
- é¡ä¼¼ Layer Normalization çš„æ¦‚å¿µ

---

## ğŸ“Š åƒæ•¸é‡èˆ‡è¨˜æ†¶é«”åˆ†æ

### åƒæ•¸é‡è¨ˆç®—

å‡è¨­å° LLaMA-7B çš„ä¸€å€‹ Attention å±¤ä½¿ç”¨ LoRAï¼š

```
åŸå§‹æ¬Šé‡ï¼š
- Q: 4096 Ã— 4096 = 16M
- K: 4096 Ã— 4096 = 16M
- V: 4096 Ã— 4096 = 16M
- O: 4096 Ã— 4096 = 16M
ç¸½è¨ˆï¼š64M åƒæ•¸

LoRA (r=8):
æ¯å€‹çŸ©é™£ï¼š
- B: 4096 Ã— 8 = 32,768
- A: 8 Ã— 4096 = 32,768
- å°è¨ˆï¼š65,536

å››å€‹çŸ©é™£ï¼š65,536 Ã— 4 = 262,144 (0.26M)

åƒæ•¸æ¸›å°‘ï¼š64M / 0.26M â‰ˆ 246Ã—
```

### è¨˜æ†¶é«”è¨ˆç®—

```
Full Fine-tuning (7B model):
- Weights: 14GB
- Optimizer: 28GB
- Gradients: 14GB
- Total: ~56GB

LoRA (r=8):
- Frozen weights: 14GB (no grad)
- LoRA weights: ~30MB
- LoRA optimizer: ~60MB
- LoRA gradients: ~30MB
- Total: ~14.12GB

è¨˜æ†¶é«”æ¸›å°‘ï¼š~4Ã—
```

---

## ğŸ’» å¯¦ä½œç´°ç¯€

### 1. LoRA Layer å¯¦ä½œ

```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        alpha: float = 16,
        dropout: float = 0.0,
    ):
        super().__init__()
        self.rank = rank
        self.alpha = alpha

        # LoRA å¯è¨“ç·´æ¬Šé‡
        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))
        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))

        # Dropout
        self.dropout = nn.Dropout(p=dropout) if dropout > 0 else nn.Identity()

        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

        # ç¸®æ”¾
        self.scaling = alpha / rank

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        x: (batch, seq_len, in_features)
        output: (batch, seq_len, out_features)
        """
        # LoRA è·¯å¾‘ï¼šx â†’ A â†’ dropout â†’ B
        lora_out = self.lora_B @ (self.lora_A @ x.T)
        lora_out = self.dropout(lora_out.T)

        return lora_out * self.scaling
```

### 2. æ‡‰ç”¨åˆ° Linear Layer

```python
class LinearWithLoRA(nn.Module):
    def __init__(
        self,
        linear: nn.Linear,
        rank: int = 8,
        alpha: float = 16,
    ):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features,
            linear.out_features,
            rank=rank,
            alpha=alpha,
        )

        # å‡çµåŸå§‹æ¬Šé‡
        self.linear.weight.requires_grad = False
        if self.linear.bias is not None:
            self.linear.bias.requires_grad = False

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # åŸå§‹è¼¸å‡º + LoRA è¼¸å‡º
        return self.linear(x) + self.lora(x)
```

### 3. æ‡‰ç”¨åˆ° Transformer

```python
def apply_lora_to_model(model, rank=8, alpha=16):
    """å°‡ LoRA æ‡‰ç”¨åˆ°æ¨¡å‹çš„æ‰€æœ‰ Linear å±¤"""
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            # é€šå¸¸åªå° Q, K, V, O ä½¿ç”¨ LoRA
            if any(key in name for key in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]

                parent = model.get_submodule(parent_name)
                setattr(
                    parent,
                    child_name,
                    LinearWithLoRA(module, rank=rank, alpha=alpha)
                )
    return model
```

---

## ğŸ›ï¸ è¶…åƒæ•¸èª¿æ•´

### Rank (r)

**å½±éŸ¿ï¼š**
- â¬†ï¸ æ›´é«˜çš„ rank â†’ æ›´å¼·çš„è¡¨é”èƒ½åŠ›
- â¬†ï¸ æ›´é«˜çš„ rank â†’ æ›´å¤šåƒæ•¸
- â¬‡ï¸ éé«˜çš„ rank â†’ å¯èƒ½éæ“¬åˆ

**ç¶“é©—æ³•å‰‡ï¼š**
```
Small models (< 1B):  r = 4-8
Medium models (1-7B): r = 8-16
Large models (> 7B):  r = 16-64
```

### Alpha (Î±)

**å½±éŸ¿ï¼š**
- â¬†ï¸ æ›´é«˜çš„ Î± â†’ LoRA æ¬Šé‡å½±éŸ¿æ›´å¤§
- â¬‡ï¸ éé«˜çš„ Î± â†’ å¯èƒ½ç ´å£é è¨“ç·´çŸ¥è­˜

**ç¶“é©—æ³•å‰‡ï¼š**
```
Î± = r     # æ¨™æº–è¨­ç½®
Î± = 2r    # å¢å¼· LoRA å½±éŸ¿
Î± = r/2   # ä¿å®ˆè¨­ç½®
```

### Dropout

**å»ºè­°ï¼š**
```
Small dataset: 0.1-0.2
Large dataset: 0.0-0.05
```

---

## ğŸ“ˆ æ•ˆèƒ½å°æ¯”

### å¯¦é©—çµæœ (è«–æ–‡æ•¸æ“š)

| æ¨¡å‹ | æ–¹æ³• | åƒæ•¸é‡ | GLUE Score |
|------|------|--------|------------|
| GPT-3 175B | Full FT | 175B | 89.5 |
| GPT-3 175B | Adapter | 40M | 88.2 |
| GPT-3 175B | LoRA (r=4) | 4.7M | 89.3 |
| GPT-3 175B | LoRA (r=64) | 37.7M | **89.7** |

**çµè«–ï¼š**
- âœ… LoRA ç”¨ 0.02% çš„åƒæ•¸é”åˆ°å…¨åƒæ•¸å¾®èª¿çš„æ•ˆæœ
- âœ… ç”šè‡³åœ¨æŸäº›ä»»å‹™ä¸Šè¶…è¶Šå…¨åƒæ•¸å¾®èª¿

---

## ğŸ” é€²éšè©±é¡Œ

### 1. LoRA çš„ç†è«–ä¿è­‰

**å‡è¨­ï¼š** é è¨“ç·´æ¨¡å‹å·²ç¶“å­¸ç¿’äº†ä¸€å€‹é«˜ç¶­ç©ºé–“çš„é€šç”¨è¡¨ç¤º

**å¾®èª¿æ™‚ï¼š** åªéœ€è¦åœ¨é€™å€‹è¡¨ç¤ºçš„ä½ç¶­å­ç©ºé–“ä¸­é€²è¡Œèª¿æ•´

**æ•¸å­¸è­‰æ˜ï¼š** (ç°¡åŒ–ç‰ˆ)
```
è¨­ Wâ‚€ æ˜¯é è¨“ç·´æ¬Šé‡
å¾®èª¿ç›®æ¨™ï¼šmin_W L(W)

æ³°å‹’å±•é–‹ï¼š
L(Wâ‚€ + Î”W) â‰ˆ L(Wâ‚€) + âˆ‡L(Wâ‚€)áµ€Î”W + ...

å¦‚æœ âˆ‡L(Wâ‚€) å­˜åœ¨æ–¼ä½ç§©å­ç©ºé–“ï¼Œ
å‰‡ Î”W ä¹Ÿå¯ä»¥ä½ç§©è¡¨ç¤º
```

### 2. LoRA èˆ‡å…¶ä»–æ–¹æ³•çš„é—œä¿‚

```
Adapter âŠ‚ LoRA âŠ‚ Full Fine-tuning

å…¶ä¸­ï¼š
- Adapter: ä¸²è¡Œæ¶æ§‹ï¼Œå¢åŠ æ¨è«–å»¶é²
- LoRA: ä¸¦è¡Œæ¶æ§‹ï¼Œé›¶æ¨è«–å»¶é²
- Full FT: æ‰€æœ‰åƒæ•¸å¯è¨“ç·´
```

### 3. åˆä½µ LoRA æ¬Šé‡

```python
def merge_lora_weights(model):
    """è¨“ç·´å¾Œåˆä½µ LoRA æ¬Šé‡åˆ°åŸå§‹æ¬Šé‡"""
    for name, module in model.named_modules():
        if isinstance(module, LinearWithLoRA):
            # W' = Wâ‚€ + BA
            merged_weight = (
                module.linear.weight.data +
                (module.lora.lora_B @ module.lora.lora_A) * module.lora.scaling
            )
            module.linear.weight.data = merged_weight

            # ç§»é™¤ LoRA å±¤
            module.lora = nn.Identity()
```

---

## ğŸ§ª å¯¦é©—å»ºè­°

### æœ€ä½³å¯¦è¸

1. **é¸æ“‡åˆé©çš„å±¤**
   - âœ… Attention çš„ Q, K, V, O
   - âœ… FFN çš„ up_proj, down_proj
   - âŒ Embedding, LayerNorm

2. **Rank é¸æ“‡ç­–ç•¥**
   ```python
   # å¾å°é–‹å§‹ï¼Œé€æ­¥å¢åŠ 
   ranks_to_try = [4, 8, 16, 32]

   for r in ranks_to_try:
       model = apply_lora(base_model, rank=r)
       score = evaluate(model)
       print(f"Rank {r}: {score}")
   ```

3. **å­¸ç¿’ç‡èª¿æ•´**
   ```python
   # LoRA å±¤éœ€è¦æ›´é«˜çš„å­¸ç¿’ç‡
   optimizer = AdamW([
       {'params': lora_params, 'lr': 1e-4},  # LoRA å±¤
       {'params': other_params, 'lr': 1e-5}, # å…¶ä»–å±¤ï¼ˆå¦‚æœæœ‰ï¼‰
   ])
   ```

---

## ğŸ“š å»¶ä¼¸é–±è®€

### å¿…è®€è«–æ–‡

1. **LoRA åŸå§‹è«–æ–‡**
   - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
   - Microsoft, 2021

2. **ç†è«–åˆ†æ**
   - [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)

3. **æ”¹é€²ç‰ˆæœ¬**
   - [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)
   - [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

---

## â“ å¸¸è¦‹å•é¡Œ

### Q1: LoRA æœƒé™ä½æ¨¡å‹æ€§èƒ½å—ï¼Ÿ

**A**: ä¸ä¸€å®šã€‚åœ¨å¤§å¤šæ•¸æƒ…æ³ä¸‹ï¼š
- å°æ•¸æ“šé›†ï¼šLoRA å¯èƒ½æ›´å¥½ï¼ˆé¿å…éæ“¬åˆï¼‰
- å¤§æ•¸æ“šé›†ï¼šLoRA â‰ˆ Full Fine-tuning
- æ¥µå¤§æ•¸æ“šé›†ï¼šFull Fine-tuning å¯èƒ½ç•¥å‹

### Q2: ç‚ºä»€éº¼ LoRA æœ‰æ•ˆï¼Ÿ

**A**: æ ¸å¿ƒåŸå› ï¼š
1. **å…§åœ¨ç¶­åº¦å‡èªª**ï¼šä»»å‹™é©æ‡‰åªéœ€è¦ä½ç¶­å­ç©ºé–“
2. **éæ“¬åˆé˜²è­·**ï¼šåƒæ•¸é™åˆ¶æä¾›æ­£å‰‡åŒ–
3. **é è¨“ç·´çŸ¥è­˜ä¿ç•™**ï¼šå‡çµåŸå§‹æ¬Šé‡

### Q3: rank å¦‚ä½•å½±éŸ¿æ•ˆèƒ½ï¼Ÿ

**A**:
- å¤ªå° (r < 4)ï¼šè¡¨é”èƒ½åŠ›ä¸è¶³
- é©ä¸­ (r = 8-16)ï¼šå¹³è¡¡æ€§èƒ½èˆ‡æ•ˆç‡
- å¤ªå¤§ (r > 64)ï¼šé‚Šéš›æ”¶ç›Šéæ¸›ï¼Œæ¥è¿‘å…¨åƒæ•¸

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®Œæˆ LoRA ç†è«–å­¸ç¿’å¾Œï¼š

1. âœ… **å¯¦ä½œç·´ç¿’**ï¼š[Task 01 - LoRA åŸºç¤å¯¦ä½œ](../lab_tasks/task01_lora_basic/)
2. ğŸ“– **é€²éšå­¸ç¿’**ï¼š[QLoRA èˆ‡é‡åŒ–](02_qlora_quantization.md)
3. ğŸ”¬ **å¯¦é©—èª¿åƒ**ï¼šå˜—è©¦ä¸åŒ rank èˆ‡ alpha çµ„åˆ

---

<div align="center">

**ç†è§£åŸç†ï¼ŒæŒæ¡å¯¦ä½œï¼ğŸ’¡**

[â† è¿”å›ç¸½è¦½](00_overview.md) | [ä¸‹ä¸€ç¯‡ï¼šQLoRA é‡åŒ– â†’](02_qlora_quantization.md)

</div>
