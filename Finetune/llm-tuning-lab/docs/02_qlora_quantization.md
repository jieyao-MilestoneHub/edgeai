# ğŸ“• QLoRA èˆ‡é‡åŒ–

> 4-bit é‡åŒ–æŠ€è¡“å¯¦ç¾è¨˜æ†¶é«”é«˜æ•ˆå¾®èª¿

## æ ¸å¿ƒæ¦‚å¿µ

QLoRA = **Q**uantized + Lo**RA**

### é—œéµå‰µæ–°
1. **4-bit NormalFloat (NF4)**ï¼šé‡å°æ­£æ…‹åˆ†ä½ˆå„ªåŒ–çš„é‡åŒ–æ ¼å¼
2. **é›™é‡é‡åŒ–**ï¼šå°é‡åŒ–å¸¸æ•¸å†é‡åŒ–
3. **åˆ†é å„ªåŒ–å™¨**ï¼šè™•ç†è¨˜æ†¶é«”å³°å€¼

## è¨˜æ†¶é«”ç¯€çœ

```
LLaMA-65B Full Fine-tuning: >780GB
LLaMA-65B LoRA (FP16):     ~120GB  
LLaMA-65B QLoRA (4-bit):    ~48GB  âœ… å–®å¡å¯è¨“ç·´ï¼
```

## å¯¦ä½œè¦é»

```python
from transformers import BitsAndBytesConfig

# 4-bit é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)
```

è©³è¦‹ [Task 02 - QLoRA å¯¦æˆ°](../lab_tasks/task02_qlora/)
