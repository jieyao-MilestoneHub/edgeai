# Task 01 è©³ç´°æ•™å­¸æŒ‡å¼•

> æ‰‹æŠŠæ‰‹æ•™ä½ å¯¦ä½œ LoRA - å¾ç†è«–åˆ°ç¨‹å¼ç¢¼

## ğŸ“– å°è®€

æœ¬æŒ‡å¼•å°‡å¸¶ä½ å®Œæˆä»¥ä¸‹æ­¥é©Ÿï¼š

1. **ç†è§£ LoRA åŸç†** - æ•¸å­¸æ¨å°èˆ‡è¦–è¦ºåŒ–
2. **å¯¦ä½œ LoRA æ¨¡çµ„** - é€è¡Œè§£é‡‹ç¨‹å¼ç¢¼
3. **æ•´åˆåˆ°æ¨¡å‹** - æ‡‰ç”¨åˆ° Transformer
4. **è¨“ç·´èˆ‡è©•ä¼°** - å®Œæ•´è¨“ç·´æµç¨‹
5. **å¯¦é©—èˆ‡åˆ†æ** - åƒæ•¸å½±éŸ¿èˆ‡èª¿å„ª

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šLoRA åŸç†æ·±å…¥ç†è§£

### 1.1 ç‚ºä»€éº¼éœ€è¦ LoRAï¼Ÿ

å‡è¨­ä½ è¦å¾®èª¿ GPT-2 (124M åƒæ•¸)ï¼š

```python
# å…¨åƒæ•¸å¾®èª¿
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.train()  # æ‰€æœ‰ 124M åƒæ•¸éƒ½å¯è¨“ç·´

# å•é¡Œï¼š
# - è¨˜æ†¶é«”ï¼šéœ€è¦å„²å­˜ 124M å€‹æ¢¯åº¦
# - å„²å­˜ï¼šæ¯å€‹ä»»å‹™éƒ½éœ€è¦å®Œæ•´æ¨¡å‹å‰¯æœ¬ (500MB+)
# - æ•ˆç‡ï¼šè¨“ç·´é€Ÿåº¦æ…¢ï¼Œå®¹æ˜“éæ“¬åˆ
```

**LoRA è§£æ³•ï¼š**

```python
# åªè¨“ç·´é¡å¤–çš„å°çŸ©é™£
# å‡çµåŸå§‹ 124M åƒæ•¸
# åªè¨“ç·´ ~300K æ–°åƒæ•¸ (rank=8)

# å„ªå‹¢ï¼š
# âœ… è¨˜æ†¶é«”æ¸›å°‘ 4-10Ã—
# âœ… æ¯å€‹ä»»å‹™åªéœ€å„²å­˜ ~2MB
# âœ… è¨“ç·´æ›´å¿«ï¼Œæ›´ä¸æ˜“éæ“¬åˆ
```

### 1.2 æ•¸å­¸åŸç†æ‹†è§£

#### åŸå§‹ç·šæ€§å±¤

```python
# ç·šæ€§è®Šæ›
y = Wx

å…¶ä¸­ï¼š
- x âˆˆ â„^k: è¼¸å…¥å‘é‡
- W âˆˆ â„^(dÃ—k): æ¬Šé‡çŸ©é™£
- y âˆˆ â„^d: è¼¸å‡ºå‘é‡
```

#### å…¨åƒæ•¸å¾®èª¿

```python
# è¨“ç·´å¾Œæ¬Šé‡è®Šç‚º
W' = Wâ‚€ + Î”W

å…¶ä¸­ï¼š
- Wâ‚€: é è¨“ç·´æ¬Šé‡ (å‡çµ)
- Î”W: è¨“ç·´ä¸­å­¸åˆ°çš„è®ŠåŒ– (dÃ—kï¼Œå…¨éƒ¨å¯è¨“ç·´)
```

#### LoRA ä½ç§©è¿‘ä¼¼

```python
# æ ¸å¿ƒå‡è¨­ï¼šÎ”W å¯ä»¥ç”¨ä½ç§©çŸ©é™£è¿‘ä¼¼
Î”W â‰ˆ BA

å…¶ä¸­ï¼š
- B âˆˆ â„^(dÃ—r): ä¸‹æŠ•å½±çŸ©é™£
- A âˆˆ â„^(rÃ—k): ä¸ŠæŠ•å½±çŸ©é™£
- r << min(d, k): rankï¼Œé€šå¸¸ 4-64

# åƒæ•¸é‡å°æ¯”ï¼š
# å…¨åƒæ•¸ï¼šd Ã— k
# LoRAï¼š  dÃ—r + rÃ—k = r(d+k)

# ä¾‹å¦‚ d=k=1024, r=8:
# å…¨åƒæ•¸ï¼š1024 Ã— 1024 = 1,048,576
# LoRAï¼š  8 Ã— (1024 + 1024) = 16,384
# æ¸›å°‘ï¼š  ~64 å€ï¼
```

### 1.3 è¦–è¦ºåŒ–ç†è§£

```
è¼¸å…¥ x (batch, seq, k)
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚                  â”‚
â”‚ è·¯å¾‘1: åŸºç¤è¼¸å‡º  â”‚  è·¯å¾‘2: LoRA è¼¸å‡º â”‚
â”‚                 â”‚                  â”‚
â”‚   Wâ‚€ (å‡çµ)      â”‚     A (rÃ—k)      â”‚
â”‚   (dÃ—k)         â”‚       â†“          â”‚
â”‚     â†“           â”‚    x @ Aáµ€        â”‚
â”‚   Wâ‚€x           â”‚   (batch,seq,r)  â”‚
â”‚                 â”‚       â†“          â”‚
â”‚                 â”‚     B (dÃ—r)      â”‚
â”‚                 â”‚       â†“          â”‚
â”‚                 â”‚    (Â·) @ Báµ€      â”‚
â”‚                 â”‚   (batch,seq,d)  â”‚
â”‚                 â”‚       â†“          â”‚
â”‚                 â”‚   Ã— (Î±/r)        â”‚
â”‚                 â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€ + â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
            y = Wâ‚€x + (Î±/r)Â·BAx
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šLoRA æ¨¡çµ„å¯¦ä½œ

### 2.1 æ ¸å¿ƒ LoRA Layer

è®“æˆ‘å€‘é€æ­¥å¯¦ä½œ `LoRALayer`ï¼š

```python
import torch
import torch.nn as nn
import math
from typing import Optional

class LoRALayer(nn.Module):
    """
    LoRA (Low-Rank Adaptation) å±¤å¯¦ä½œ

    æ•¸å­¸å½¢å¼ï¼š
        h = Wx + (Î±/r) Â· BAx

    å…¶ä¸­ï¼š
        - W: å‡çµçš„é è¨“ç·´æ¬Šé‡
        - B, A: å¯è¨“ç·´çš„ä½ç§©çŸ©é™£
        - Î±: ç¸®æ”¾è¶…åƒæ•¸
        - r: rank
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
    ):
        super().__init__()

        # ========== è¶…åƒæ•¸å„²å­˜ ==========
        self.rank = rank
        self.alpha = alpha
        self.in_features = in_features
        self.out_features = out_features

        # ========== LoRA æ¬Šé‡å®šç¾© ==========
        # æ³¨æ„ç¶­åº¦ï¼A æ˜¯ rÃ—inï¼ŒB æ˜¯ outÃ—r
        # é€™æ¨£æ‰èƒ½æ­£ç¢ºè¨ˆç®—ï¼šx @ A^T @ B^T
        self.lora_A = nn.Parameter(
            torch.zeros(rank, in_features)
        )
        self.lora_B = nn.Parameter(
            torch.zeros(out_features, rank)
        )

        # ========== Dropout ==========
        # é˜²æ­¢éæ“¬åˆ
        self.dropout = nn.Dropout(p=dropout) if dropout > 0.0 else nn.Identity()

        # ========== ç¸®æ”¾å› å­ ==========
        # é€™å¾ˆé‡è¦ï¼ç¢ºä¿ä¸åŒ rank çš„è¨“ç·´ç©©å®šæ€§
        self.scaling = alpha / rank

        # ========== æ¬Šé‡åˆå§‹åŒ– ==========
        self.reset_parameters()

    def reset_parameters(self):
        """
        åˆå§‹åŒ–ç­–ç•¥ï¼š
        - A: Kaiming uniform (é¡ä¼¼é è¨“ç·´æ¬Šé‡)
        - B: é›¶åˆå§‹åŒ– (ç¢ºä¿åˆå§‹è¼¸å‡ºç‚º 0)

        ç‚ºä»€éº¼ B åˆå§‹åŒ–ç‚º 0ï¼Ÿ
        - é€™æ¨£ BA åˆå§‹å°±æ˜¯é›¶çŸ©é™£
        - è¨“ç·´ä¸€é–‹å§‹ï¼ŒLoRA ä¸å½±éŸ¿åŸå§‹æ¨¡å‹
        - é€æ­¥å­¸ç¿’ï¼Œæ…¢æ…¢åŠ å…¥ adapter çš„å½±éŸ¿
        """
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­

        Args:
            x: è¼¸å…¥ tensorï¼Œshape = (batch, seq_len, in_features)

        Returns:
            LoRA è¼¸å‡ºï¼Œshape = (batch, seq_len, out_features)

        è¨ˆç®—æµç¨‹ï¼š
            x â†’ dropout â†’ A â†’ B â†’ scaling â†’ output
        """
        # æ­¥é©Ÿ 1: Dropout (å¯é¸)
        x_drop = self.dropout(x)

        # æ­¥é©Ÿ 2: ç¬¬ä¸€æ¬¡æŠ•å½± (é™ç¶­åˆ° rank)
        # x_drop: (B, S, in_features)
        # lora_A: (rank, in_features)
        # result: (B, S, rank)
        lora_intermediate = x_drop @ self.lora_A.T

        # æ­¥é©Ÿ 3: ç¬¬äºŒæ¬¡æŠ•å½± (å‡ç¶­åˆ° out_features)
        # lora_intermediate: (B, S, rank)
        # lora_B: (out_features, rank)
        # result: (B, S, out_features)
        lora_output = lora_intermediate @ self.lora_B.T

        # æ­¥é©Ÿ 4: ç¸®æ”¾
        lora_output = lora_output * self.scaling

        return lora_output
```

### 2.2 æ•´åˆåˆ° Linear Layer

```python
class LinearWithLoRA(nn.Module):
    """
    å°‡ LoRA æ•´åˆåˆ°æ¨™æº– Linear å±¤

    å¯¦ç¾æ–¹å¼ï¼š
        output = Linear(x) + LoRA(x)
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        rank: int = 8,
        alpha: float = 16.0,
        bias: bool = True,
        dropout: float = 0.0,
    ):
        super().__init__()

        # ========== åŸºç¤ Linear å±¤ (å‡çµ) ==========
        self.linear = nn.Linear(in_features, out_features, bias=bias)

        # å‡çµåŸå§‹æ¬Šé‡ï¼éå¸¸é‡è¦ï¼
        self.linear.weight.requires_grad = False
        if bias and self.linear.bias is not None:
            self.linear.bias.requires_grad = False

        # ========== LoRA å±¤ ==========
        self.lora = LoRALayer(
            in_features=in_features,
            out_features=out_features,
            rank=rank,
            alpha=alpha,
            dropout=dropout,
        )

        # ========== æ¨™è¨˜ ==========
        self.merged = False

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­ï¼šä¸¦è¡Œè¨ˆç®—å…©æ¢è·¯å¾‘

        Args:
            x: shape = (batch, seq_len, in_features)

        Returns:
            shape = (batch, seq_len, out_features)
        """
        if self.merged:
            # å¦‚æœå·²åˆä½µï¼Œç›´æ¥ä½¿ç”¨ linear
            return self.linear(x)
        else:
            # ä¸¦è¡Œè¨ˆç®—ï¼šåŸºç¤è¼¸å‡º + LoRA è¼¸å‡º
            base_output = self.linear(x)
            lora_output = self.lora(x)
            return base_output + lora_output

    def merge_weights(self):
        """
        è¨“ç·´å¾Œåˆä½µæ¬Šé‡ï¼šW' = Wâ‚€ + BA

        å„ªé»ï¼š
        - æ¨è«–æ™‚é›¶é¡å¤–é–‹éŠ·
        - å¯ä»¥ç›´æ¥éƒ¨ç½²åŸå§‹æ¨¡å‹æ ¼å¼
        """
        if not self.merged:
            # è¨ˆç®— BA
            delta_w = self.lora.lora_B @ self.lora.lora_A
            delta_w = delta_w * self.lora.scaling

            # æ›´æ–°æ¬Šé‡
            self.linear.weight.data += delta_w

            self.merged = True

    def unmerge_weights(self):
        """åå‘æ“ä½œï¼šå°‡æ¬Šé‡æ‹†åˆ†å›å»"""
        if self.merged:
            delta_w = self.lora.lora_B @ self.lora.lora_A
            delta_w = delta_w * self.lora.scaling

            self.linear.weight.data -= delta_w

            self.merged = False
```

### 2.3 æ‡‰ç”¨åˆ° GPT-2

```python
from transformers import GPT2LMHeadModel
import torch.nn as nn

def apply_lora_to_gpt2(
    model: GPT2LMHeadModel,
    rank: int = 8,
    alpha: float = 16.0,
    target_modules: list = None,
) -> GPT2LMHeadModel:
    """
    å°‡ LoRA æ‡‰ç”¨åˆ° GPT-2 æ¨¡å‹

    Args:
        model: é è¨“ç·´çš„ GPT-2 æ¨¡å‹
        rank: LoRA rank
        alpha: LoRA alpha
        target_modules: è¦æ‡‰ç”¨ LoRA çš„æ¨¡çµ„åç¨±

    Returns:
        ä¿®æ”¹å¾Œçš„æ¨¡å‹
    """

    # é è¨­å° Q, K, V, O æ‡‰ç”¨ LoRA
    if target_modules is None:
        target_modules = ['c_attn', 'c_proj']

    # éæ­·æ‰€æœ‰å±¤
    for name, module in model.named_modules():
        # æª¢æŸ¥æ˜¯å¦æ˜¯ç›®æ¨™æ¨¡çµ„
        if any(target in name for target in target_modules):
            if isinstance(module, nn.Linear):
                # ç²å–çˆ¶æ¨¡çµ„
                parent_name = '.'.join(name.split('.')[:-1])
                child_name = name.split('.')[-1]
                parent_module = model.get_submodule(parent_name)

                # æ›¿æ›ç‚º LoRA ç‰ˆæœ¬
                lora_module = LinearWithLoRA(
                    in_features=module.in_features,
                    out_features=module.out_features,
                    rank=rank,
                    alpha=alpha,
                    bias=module.bias is not None,
                )

                # è¤‡è£½åŸå§‹æ¬Šé‡
                lora_module.linear.weight.data = module.weight.data.clone()
                if module.bias is not None:
                    lora_module.linear.bias.data = module.bias.data.clone()

                # æ›¿æ›æ¨¡çµ„
                setattr(parent_module, child_name, lora_module)

    return model
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¨“ç·´è…³æœ¬å¯¦ä½œ

### 3.1 å®Œæ•´è¨“ç·´æµç¨‹

```python
"""
train_lora_basic.py - LoRA è¨“ç·´è…³æœ¬
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    GPT2LMHeadModel,
    GPT2Tokenizer,
    get_linear_schedule_with_warmup,
)
from datasets import load_dataset
import matplotlib.pyplot as plt
from tqdm import tqdm
import argparse

# å‡è¨­ lora_linear.py åœ¨åŒä¸€ç›®éŒ„
from lora_linear import apply_lora_to_gpt2


def train_one_epoch(model, dataloader, optimizer, scheduler, device):
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()
    total_loss = 0

    progress_bar = tqdm(dataloader, desc="Training")
    for batch in progress_bar:
        # ç§»å‹•åˆ° GPU
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        # å‰å‘å‚³æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=input_ids,  # èªè¨€æ¨¡å‹è‡ªå›æ­¸ä»»å‹™
        )
        loss = outputs.loss

        # åå‘å‚³æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # æ›´æ–°åƒæ•¸
        optimizer.step()
        scheduler.step()

        # è¨˜éŒ„
        total_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return total_loss / len(dataloader)


def evaluate(model, dataloader, device):
    """è©•ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=input_ids,
            )
            total_loss += outputs.loss.item()

    return total_loss / len(dataloader)


def count_parameters(model):
    """è¨ˆç®—åƒæ•¸é‡"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    return {
        'total': total_params,
        'trainable': trainable_params,
        'frozen': total_params - trainable_params,
        'percentage': 100 * trainable_params / total_params,
    }


def main(args):
    # ========== 1. è¨­å®š Device ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # ========== 2. è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer ==========
    print("Loading model...")
    model = GPT2LMHeadModel.from_pretrained(args.model_name)
    tokenizer = GPT2Tokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    # ========== 3. æ‡‰ç”¨ LoRA ==========
    print(f"Applying LoRA (rank={args.rank}, alpha={args.alpha})...")
    model = apply_lora_to_gpt2(
        model,
        rank=args.rank,
        alpha=args.alpha,
    )
    model = model.to(device)

    # ========== 4. é¡¯ç¤ºåƒæ•¸çµ±è¨ˆ ==========
    param_stats = count_parameters(model)
    print("\n" + "="*50)
    print("Parameter Statistics:")
    print(f"  Total:     {param_stats['total']:,}")
    print(f"  Trainable: {param_stats['trainable']:,}")
    print(f"  Frozen:    {param_stats['frozen']:,}")
    print(f"  Percentage: {param_stats['percentage']:.4f}%")
    print("="*50 + "\n")

    # ========== 5. è¼‰å…¥æ•¸æ“š ==========
    print("Loading dataset...")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.max_length,
            padding="max_length",
        )

    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
    )

    train_dataloader = DataLoader(
        tokenized_datasets["train"],
        batch_size=args.batch_size,
        shuffle=True,
    )
    eval_dataloader = DataLoader(
        tokenized_datasets["validation"],
        batch_size=args.batch_size,
    )

    # ========== 6. è¨­å®š Optimizer èˆ‡ Scheduler ==========
    # åªå„ªåŒ–å¯è¨“ç·´åƒæ•¸ï¼ˆLoRA æ¬Šé‡ï¼‰
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.learning_rate,
        weight_decay=args.weight_decay,
    )

    total_steps = len(train_dataloader) * args.num_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps,
    )

    # ========== 7. è¨“ç·´å¾ªç’° ==========
    print("Start training...")
    train_losses = []
    eval_losses = []

    for epoch in range(args.num_epochs):
        print(f"\nEpoch {epoch + 1}/{args.num_epochs}")

        # è¨“ç·´
        train_loss = train_one_epoch(
            model, train_dataloader, optimizer, scheduler, device
        )
        train_losses.append(train_loss)

        # è©•ä¼°
        eval_loss = evaluate(model, eval_dataloader, device)
        eval_losses.append(eval_loss)

        print(f"Train Loss: {train_loss:.4f}")
        print(f"Eval Loss:  {eval_loss:.4f}")

    # ========== 8. å„²å­˜çµæœ ==========
    # 8.1 å„²å­˜ LoRA æ¬Šé‡
    torch.save({
        'lora_state_dict': {
            name: param for name, param in model.named_parameters()
            if 'lora' in name
        },
        'config': {
            'rank': args.rank,
            'alpha': args.alpha,
            'model_name': args.model_name,
        }
    }, 'adapter_model.bin')
    print("âœ… Saved adapter_model.bin")

    # 8.2 ç¹ªè£½ Loss æ›²ç·š
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss', marker='o')
    plt.plot(eval_losses, label='Eval Loss', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('LoRA Training Progress')
    plt.legend()
    plt.grid(True)
    plt.savefig('training_loss_curve.png', dpi=300, bbox_inches='tight')
    print("âœ… Saved training_loss_curve.png")

    print("\nğŸ‰ Training completed!")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_name", default="gpt2", type=str)
    parser.add_argument("--rank", default=8, type=int)
    parser.add_argument("--alpha", default=16.0, type=float)
    parser.add_argument("--num_epochs", default=3, type=int)
    parser.add_argument("--batch_size", default=8, type=int)
    parser.add_argument("--learning_rate", default=2e-4, type=float)
    parser.add_argument("--weight_decay", default=0.01, type=float)
    parser.add_argument("--max_length", default=512, type=int)

    args = parser.parse_args()
    main(args)
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šå¯¦é©—èˆ‡åˆ†æ

### 4.1 é©—è­‰ LoRA å¯¦ä½œæ­£ç¢ºæ€§

```python
def test_lora_layer():
    """å–®å…ƒæ¸¬è©¦ï¼šé©—è­‰ LoRA å±¤"""
    from lora_linear import LoRALayer

    # å‰µå»ºæ¸¬è©¦è³‡æ–™
    batch_size, seq_len = 4, 10
    in_features, out_features = 512, 512
    rank = 8

    lora = LoRALayer(in_features, out_features, rank=rank)
    x = torch.randn(batch_size, seq_len, in_features)

    # å‰å‘å‚³æ’­
    output = lora(x)

    # æª¢æŸ¥è¼¸å‡ºå½¢ç‹€
    assert output.shape == (batch_size, seq_len, out_features)

    # æª¢æŸ¥åˆå§‹è¼¸å‡ºç‚ºé›¶ï¼ˆå› ç‚º B åˆå§‹åŒ–ç‚º 0ï¼‰
    assert torch.allclose(output, torch.zeros_like(output), atol=1e-6)

    print("âœ… LoRA Layer æ¸¬è©¦é€šéï¼")

test_lora_layer()
```

### 4.2 åƒæ•¸é‡é©—è­‰

```python
def verify_parameter_reduction():
    """é©—è­‰åƒæ•¸æ¸›å°‘æ•ˆæœ"""
    from transformers import GPT2LMHeadModel
    from lora_linear import apply_lora_to_gpt2

    # åŸå§‹æ¨¡å‹
    original_model = GPT2LMHeadModel.from_pretrained('gpt2')
    original_params = sum(p.numel() for p in original_model.parameters())

    # LoRA æ¨¡å‹
    lora_model = GPT2LMHeadModel.from_pretrained('gpt2')
    lora_model = apply_lora_to_gpt2(lora_model, rank=8)
    lora_trainable = sum(
        p.numel() for p in lora_model.parameters() if p.requires_grad
    )

    # è¨ˆç®—æ¸›å°‘æ¯”ä¾‹
    reduction = original_params / lora_trainable

    print(f"åŸå§‹åƒæ•¸ï¼š{original_params:,}")
    print(f"LoRA å¯è¨“ç·´åƒæ•¸ï¼š{lora_trainable:,}")
    print(f"æ¸›å°‘ï¼š{reduction:.1f}Ã—")

verify_parameter_reduction()
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šå¸¸è¦‹å•é¡Œé™¤éŒ¯

### å•é¡Œ 1ï¼šLoRA æ²’æœ‰ç”Ÿæ•ˆ

**ç—‡ç‹€**ï¼šè¨“ç·´ loss ä¸ä¸‹é™

**é™¤éŒ¯æ­¥é©Ÿ**ï¼š

```python
# 1. æª¢æŸ¥ LoRA åƒæ•¸æ˜¯å¦å¯è¨“ç·´
for name, param in model.named_parameters():
    if 'lora' in name:
        print(f"{name}: requires_grad={param.requires_grad}")  # æ‡‰è©²æ˜¯ True

# 2. æª¢æŸ¥åŸå§‹æ¬Šé‡æ˜¯å¦å‡çµ
for name, param in model.named_parameters():
    if 'lora' not in name and 'weight' in name:
        print(f"{name}: requires_grad={param.requires_grad}")  # æ‡‰è©²æ˜¯ False

# 3. æª¢æŸ¥æ¢¯åº¦æ˜¯å¦å‚³æ’­
loss.backward()
for name, param in model.named_parameters():
    if param.requires_grad and param.grad is None:
        print(f"âš ï¸ {name} æ²’æœ‰æ¢¯åº¦ï¼")
```

### å•é¡Œ 2ï¼šè¨˜æ†¶é«”æº¢å‡º

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šæ¸›å°‘ batch size
train_dataloader = DataLoader(dataset, batch_size=4)  # åŸæœ¬ 8

# æ–¹æ¡ˆ 2ï¼šæ¢¯åº¦ç´¯ç©
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(**batch).loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# æ–¹æ¡ˆ 3ï¼šæ··åˆç²¾åº¦è¨“ç·´
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    outputs = model(**batch)
    loss = outputs.loss

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®

å®Œæˆå¾Œï¼Œç¢ºèªä½ èƒ½å¤ ï¼š

- [ ] è§£é‡‹ LoRA çš„æ•¸å­¸åŸç†
- [ ] æ‰‹å¯« LoRALayer é¡åˆ¥
- [ ] è¨ˆç®—åƒæ•¸é‡æ¸›å°‘æ¯”ä¾‹
- [ ] æ‡‰ç”¨ LoRA åˆ°ä»»æ„ Linear å±¤
- [ ] å®Œæ•´è¨“ç·´ä¸€å€‹ LoRA æ¨¡å‹
- [ ] ç¹ªè£½ä¸¦åˆ†æ loss æ›²ç·š
- [ ] èª¿æ•´ rank å’Œ alpha è¶…åƒæ•¸
- [ ] åˆä½µ LoRA æ¬Šé‡åˆ°åŸå§‹æ¨¡å‹

---

<div align="center">

**æ­å–œå®Œæˆ Task 01ï¼ğŸ‰**

æº–å‚™å¥½æŒ‘æˆ°ä¸‹ä¸€å€‹ä»»å‹™äº†å—ï¼Ÿ

[Task 02: QLoRA å¯¦æˆ° â†’](../task02_qlora/)

</div>
