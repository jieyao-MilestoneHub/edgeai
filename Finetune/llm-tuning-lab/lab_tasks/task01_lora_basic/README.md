# Task 01: LoRA åŸºç¤å¯¦ä½œ - BERT æƒ…æ„Ÿåˆ†é¡å¾®èª¿

> ä¸€ä»½å¹«åŠ©ä½ é †åˆ©å®Œæˆç¬¬ä¸€æ¬¡ LoRA å¾®èª¿çš„å¯¦ç”¨ç­†è¨˜

## ğŸ¯ æˆ‘å€‘è¦åšä»€éº¼ï¼Ÿ

ä½¿ç”¨ **LoRA (Low-Rank Adaptation)** å¾®èª¿ BERT æ¨¡å‹ï¼Œåœ¨ **SST-2 æƒ…æ„Ÿåˆ†é¡**ä»»å‹™ä¸Šé”åˆ° 87-90% æº–ç¢ºç‡ã€‚

**ç‚ºä»€éº¼ç”¨ LoRAï¼Ÿ**
- åªè¨“ç·´ **0.27%** çš„åƒæ•¸ï¼ˆ294K / 109Mï¼‰
- è¨“ç·´é€Ÿåº¦å¿« 3 å€
- LoRA adapter åªæœ‰ **2MB**ï¼ˆå®Œæ•´æ¨¡å‹ 420MBï¼‰
- å¯ä»¥ä¸€å€‹åŸºç¤æ¨¡å‹é…å¤šå€‹ä»»å‹™ adapter

**ä»»å‹™èªªæ˜ï¼š**
- æ•¸æ“šé›†ï¼šGLUE SST-2ï¼ˆé›»å½±è©•è«–æƒ…æ„Ÿåˆ†é¡ï¼‰
- è¨“ç·´æ¨£æœ¬ï¼š67,349 æ¢è©•è«–
- é©—è­‰æ¨£æœ¬ï¼š872 æ¢è©•è«–
- åˆ†é¡ï¼šæ­£é¢ (1) / è² é¢ (0)

---

## ğŸ“ æ ¸å¿ƒæª”æ¡ˆèªªæ˜

### **lora_linear.py** - LoRA æ ¸å¿ƒå¯¦ä½œ

é€™æ˜¯æ•´å€‹ task æœ€é‡è¦çš„æª”æ¡ˆï¼Œå¯¦ä½œäº† LoRA çš„æ•¸å­¸åŸç†ã€‚

#### ğŸ”‘ æ ¸å¿ƒå…ƒä»¶

**1. `LoRALayer` (ç¬¬ 27-84 è¡Œ)**
```python
# å¯¦ä½œ y = (Î±/r) * B(Ax)
lx = F.linear(x, self.lora_A)      # é™ç¶­: (*, in) â†’ (*, rank)
lx = F.linear(lx, self.lora_B)     # å‡ç¶­: (*, rank) â†’ (*, out)
return lx * self.scaling           # ç¸®æ”¾: Î±/r
```

**ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ**
- ç”¨å…©å€‹å°çŸ©é™£ A (rankÃ—in) å’Œ B (outÃ—rank) è¿‘ä¼¼å¤§çŸ©é™£ Î”W (outÃ—in)
- B åˆå§‹åŒ–ç‚º 0 â†’ åˆå§‹æ™‚ LoRA è¼¸å‡ºç‚º 0ï¼Œä¸å¹²æ“¾é è¨“ç·´æ¬Šé‡
- A ç”¨ Kaiming åˆå§‹åŒ– â†’ è¨“ç·´æ™‚é€æ­¥å­¸ç¿’å¢é‡

**é—œéµåƒæ•¸ï¼š**
- `rank`: ä½ç§©ç¶­åº¦ï¼Œè¶Šå¤§èƒ½åŠ›è¶Šå¼·ä½†åƒæ•¸è¶Šå¤šï¼ˆå»ºè­° 4-16ï¼‰
- `alpha`: ç¸®æ”¾å› å­ï¼Œæ§åˆ¶ LoRA å½±éŸ¿åŠ›ï¼ˆé€šå¸¸ = 2 Ã— rankï¼‰
- `dropout`: é˜²æ­¢éæ“¬åˆï¼ˆåˆ†é¡ä»»å‹™å»ºè­° 0.05ï¼‰

**2. `LinearWithLoRA` (ç¬¬ 89-146 è¡Œ)**

æŠŠ LoRA æ›åœ¨åŸå§‹ Linear å±¤ä¸Šï¼š
```python
output = Linear(x) + LoRA(x)
```

**é‡è¦æ–¹æ³•ï¼š**
- `_freeze_linear()`: å‡çµåŸå§‹æ¬Šé‡ï¼Œåªè¨“ç·´ LoRA
- `merge_weights()`: éƒ¨ç½²å‰åˆä½µæ¬Šé‡ â†’ W' = W + BA
- `unmerge_weights()`: æ¢å¾©åˆ†é›¢ç‹€æ…‹ï¼ˆçºŒè¨“æ™‚ç”¨ï¼‰

**3. `apply_lora_to_model` (ç¬¬ 180-238 è¡Œ)**

è‡ªå‹•æ‰¾åˆ°æ¨¡å‹ä¸­çš„ç›®æ¨™å±¤ä¸¦æ›¿æ›æˆ LoRA ç‰ˆæœ¬ã€‚

```python
apply_lora_to_model(
    model,
    target_modules=["query", "key", "value", "dense"],  # é¸å“ªäº›å±¤
    rank=8,
    alpha=16.0
)
```

**target_modules æ€éº¼é¸ï¼Ÿ**
- BERT: `query`, `key`, `value`, `dense` (æ³¨æ„åŠ›æ©Ÿåˆ¶)
- GPT-2: `c_attn`, `c_proj`
- LLaMA: `q_proj`, `k_proj`, `v_proj`, `o_proj`

**4. å·¥å…·å‡½æ•¸ (ç¬¬ 244-313 è¡Œ)**

```python
mark_only_lora_as_trainable(model)     # åªè¨“ç·´ LoRA åƒæ•¸
count_lora_parameters(model)           # çµ±è¨ˆåƒæ•¸é‡
get_lora_state_dict(model)             # åªå– LoRA æ¬Šé‡
load_lora_state_dict(model, state)     # è¼‰å…¥ LoRA æ¬Šé‡
merge_lora_weights(model)              # åˆä½µæ‰€æœ‰ LoRA åˆ°åŸºç¤æ¨¡å‹
```

---

### **train_lora_basic.py** - è¨“ç·´è…³æœ¬

å®Œæ•´çš„è¨“ç·´æµç¨‹ï¼Œå¾è³‡æ–™è¼‰å…¥åˆ°æ¨¡å‹å„²å­˜ã€‚

#### ğŸ“‹ è¨“ç·´æµç¨‹

```
1. è¼‰å…¥é…ç½® (config.yaml)
   â†“
2. è¼‰å…¥ BERT + å¥—ç”¨ LoRA
   â†“
3. å‡çµ BERT æ¬Šé‡ï¼Œåªè¨“ç·´ LoRA
   â†“
4. è¼‰å…¥ SST-2 æ•¸æ“šé›†
   â†“
5. è¨“ç·´å¾ªç’° (train â†’ eval â†’ save best)
   â†“
6. å„²å­˜çµæœ (æ¬Šé‡ã€æ›²ç·šåœ–ã€æ—¥èªŒ)
```

#### ğŸ” é—œéµå‡½æ•¸

**`prepare_dataset` (ç¬¬ 54-88 è¡Œ)**
- è¼‰å…¥ SST-2: `{sentence, label}`
- Tokenize: è½‰æˆ BERT è¼¸å…¥æ ¼å¼ `{input_ids, attention_mask}`
- è¨­å®š max_length=128ï¼ˆSST-2 å¥å­è¼ƒçŸ­ï¼‰

**`train_one_epoch` (ç¬¬ 99-162 è¡Œ)**
- å‰å‘å‚³æ’­ â†’ è¨ˆç®— loss
- åå‘å‚³æ’­ â†’ è¨ˆç®—æ¢¯åº¦
- æ¢¯åº¦è£å‰ª â†’ é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- æ›´æ–°åƒæ•¸ â†’ AdamW optimizer
- å³æ™‚é¡¯ç¤º loss å’Œ accuracy

**`evaluate` (ç¬¬ 165-208 è¡Œ)**
- é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹
- è¨ˆç®—æº–ç¢ºç‡ï¼ˆAccuracyï¼‰
- ä¸è¨ˆç®—æ¢¯åº¦ï¼ˆ`torch.no_grad()`ï¼‰

**`plot_training_curves` (ç¬¬ 211-242 è¡Œ)**
- ç¹ªè£½ Loss å’Œ Accuracy æ›²ç·š
- å·¦åœ–ï¼šè¨“ç·´/é©—è­‰ Loss
- å³åœ–ï¼šè¨“ç·´/é©—è­‰ Accuracy

---

### **inference_example.py** - æ¨è«–æ¸¬è©¦

è¨“ç·´å®Œæˆå¾Œæ¸¬è©¦æ¨¡å‹æ•ˆæœã€‚

#### ğŸ® ä¸‰ç¨®æ¨¡å¼

**1. Interactive äº’å‹•æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰**
```bash
python inference_example.py --mode interactive
```
è¼¸å…¥å¥å­å³æ™‚åˆ†ææƒ…æ„Ÿï¼Œé©åˆé«”é©—æ¨¡å‹æ•ˆæœã€‚

**2. Demo ç¯„ä¾‹æ¨¡å¼**
```bash
python inference_example.py --mode demo
```
æŸ¥çœ‹é è¨­ç¯„ä¾‹çš„åˆ†æçµæœï¼Œå¿«é€Ÿé©—è­‰æ¨¡å‹ã€‚

**3. Text å–®æ¬¡é æ¸¬**
```bash
python inference_example.py --mode text --text "This is amazing!"
```
åˆ†æå–®ä¸€å¥å­ï¼Œé©åˆæ•´åˆåˆ°å…¶ä»–ç¨‹å¼ã€‚

#### ğŸ’¡ è¼‰å…¥æµç¨‹

```python
# 1. è¼‰å…¥ checkpoint
checkpoint = torch.load("output/best_lora_model.pt")

# 2. è¼‰å…¥åŸºç¤ BERT æ¨¡å‹
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

# 3. å¥—ç”¨ LoRA çµæ§‹ï¼ˆèˆ‡è¨“ç·´æ™‚ç›¸åŒé…ç½®ï¼‰
apply_lora_to_model(model, target_modules=["query", "key", "value", "dense"], rank=8)

# 4. è¼‰å…¥è¨“ç·´å¥½çš„ LoRA æ¬Šé‡
load_lora_state_dict(model, checkpoint['lora_state_dict'])
```

---

### **config.yaml** - é…ç½®æª”

æ‰€æœ‰è¶…åƒæ•¸éƒ½åœ¨é€™è£¡èª¿æ•´ã€‚

```yaml
# æ¨¡å‹
model_name: "bert-base-uncased"
num_labels: 2

# LoRA è¶…åƒæ•¸
lora:
  rank: 8              # ä½ç§©ç¶­åº¦ï¼ˆå½±éŸ¿åƒæ•¸é‡ï¼‰
  alpha: 16.0          # ç¸®æ”¾å› å­ï¼ˆå½±éŸ¿å­¸ç¿’å¼·åº¦ï¼‰
  dropout: 0.05        # Dropout æ¯”ä¾‹
  target_modules:      # è¦å¥— LoRA çš„å±¤
    - "query"
    - "key"
    - "value"
    - "dense"

# è¨“ç·´è¶…åƒæ•¸
training:
  num_epochs: 3
  batch_size: 16       # GPU è¨˜æ†¶é«”ä¸è¶³å¯é™åˆ° 8
  learning_rate: 3.0e-4
  max_length: 128
  warmup_ratio: 0.1

# æ•¸æ“šé›†
data:
  dataset: "glue"
  subset: "sst2"
```

---

## ğŸ’» ç’°å¢ƒéœ€æ±‚

**ç¡¬é«”**
- CPU: 4 æ ¸å¿ƒä»¥ä¸Šï¼ˆå»ºè­°ï¼‰
- RAM: 8GB ä»¥ä¸Š
- GPU: 2GB VRAM ä»¥ä¸Šï¼ˆå»ºè­° NVIDIA GPU with CUDAï¼‰
- ç¡¬ç¢Ÿ: 5GB å¯ç”¨ç©ºé–“

**è»Ÿé«”**
- Python: 3.11+
- CUDA: 11.8+ (GPU è¨“ç·´éœ€è¦)
- Git: ç”¨æ–¼ç‰ˆæœ¬æ§åˆ¶
- Hugging Face å¸³è™Ÿ: ç”¨æ–¼ä¸Šå‚³æ¨¡å‹ï¼ˆå¯é¸ï¼‰

**ç¶²è·¯**
- éœ€è¦ç¶²è·¯é€£ç·šä¸‹è¼‰æ¨¡å‹å’Œæ•¸æ“šé›†
- é¦–æ¬¡åŸ·è¡Œæœƒä¸‹è¼‰ BERT (~420MB) å’Œ SST-2 æ•¸æ“šé›† (~7MB)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ­¥é©Ÿ 1: å®‰è£ä¾è³´

```bash
pip install -r requirements.txt
```

ç’°å¢ƒéœ€æ±‚ï¼šPython 3.11+ã€å»ºè­°ä½¿ç”¨ GPU

### æ­¥é©Ÿ 2: æ¸¬è©¦å®‰è£

```bash
python test_installation.py
```

çœ‹åˆ° `ğŸ‰ All tests passed!` å°±å¯ä»¥é–‹å§‹äº†ã€‚

### æ­¥é©Ÿ 3: é–‹å§‹è¨“ç·´

```bash
python train_lora_basic.py
```

è¨“ç·´æ™‚é–“ï¼š
- GPU (RTX 3060): ç´„ 10-15 åˆ†é˜
- CPU: ç´„ 2-3 å°æ™‚

### æ­¥é©Ÿ 4: æ¸¬è©¦æ¨¡å‹

```bash
python inference_example.py
```

è¼¸å…¥å¥å­æ¸¬è©¦æƒ…æ„Ÿåˆ†ææ•ˆæœã€‚

### æ­¥é©Ÿ 5: ä¸Šå‚³åˆ° Hugging Face (å¯é¸)

è¨“ç·´å®Œæˆå¾Œå¯ä»¥å°‡æ¨¡å‹ä¸Šå‚³åˆ° Hugging Face Hub åˆ†äº«ã€‚

**å‰ç½®ä½œæ¥­**ï¼š
```bash
# ç™»å…¥ Hugging Face
huggingface-cli login
```

**ä¸Šå‚³æ¨¡å‹**ï¼š
```bash
python train_lora_basic.py \
  --push_to_hub \
  --hub_model_id "your-username/bert-lora-sst2"
```

**åƒæ•¸èªªæ˜**ï¼š
- `--push_to_hub`: å•Ÿç”¨ä¸Šå‚³åŠŸèƒ½
- `--hub_model_id`: ä½ çš„ HF æ¨¡å‹ IDï¼ˆæ ¼å¼ï¼šusername/model-nameï¼‰
- `--hub_private`: è¨­ç‚ºç§æœ‰æ¨¡å‹ï¼ˆå¯é¸ï¼‰

ä¸Šå‚³å¾Œå¯åœ¨ `https://huggingface.co/your-username/bert-lora-sst2` æŸ¥çœ‹ã€‚

---

## ğŸ“Š ç†è§£è¨“ç·´éç¨‹

### è¨“ç·´æ™‚ä½ æœƒçœ‹åˆ°é€™äº›è¼¸å‡º

```
ğŸ“Š Parameter Statistics:
  Total parameters:           109,483,778    â† BERT æ‰€æœ‰åƒæ•¸
  Trainable parameters:           294,912    â† åªè¨“ç·´é€™äº›ï¼ˆLoRAï¼‰
  Frozen parameters:          109,188,866    â† å‡çµä¸å‹•
  Trainable percentage:            0.2694%   â† ä¸åˆ° 0.3%ï¼
```

**é€™ä»£è¡¨ä»€éº¼ï¼Ÿ**
- BERT æœ‰ 1 å„„å€‹åƒæ•¸ï¼Œä½†æˆ‘å€‘åªè¨“ç·´ 29 è¬å€‹
- ç¯€çœè¨˜æ†¶é«”ã€åŠ å¿«è¨“ç·´ã€é™ä½éæ“¬åˆé¢¨éšª

```
ğŸ“ Epoch 1/3
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| loss: 0.3245, acc: 0.8567
  Train Loss: 0.3421 | Train Acc: 0.8523
  Eval Loss:  0.2987 | Eval Acc:  0.8761
  ğŸ’¾ Best model saved! (eval_acc: 0.8761)
```

**é€™äº›æ•¸å­—ä»£è¡¨ä»€éº¼ï¼Ÿ**
- **Loss ä¸‹é™**ï¼šæ¨¡å‹åœ¨å­¸ç¿’ï¼Œé æ¸¬è¶Šä¾†è¶Šæº–
- **Accuracy ä¸Šå‡**ï¼šåˆ†é¡æ­£ç¢ºç‡æé«˜
- **Train vs Eval**ï¼š
  - Train é«˜ã€Eval ä½ â†’ å¯èƒ½éæ“¬åˆ
  - å…©è€…æ¥è¿‘ â†’ è¨“ç·´è‰¯å¥½
- **Best model saved**ï¼šé©—è­‰æº–ç¢ºç‡å‰µæ–°é«˜æ™‚è‡ªå‹•å„²å­˜

### è¨“ç·´å®Œæˆå¾Œçš„è¼¸å‡ºæª”æ¡ˆ

```
output/
â”œâ”€â”€ best_lora_model.pt       # æœ€ä½³æ¨¡å‹ï¼ˆæº–ç¢ºç‡æœ€é«˜çš„ epochï¼‰
â”œâ”€â”€ final_lora_model.pt      # æœ€çµ‚æ¨¡å‹ï¼ˆç¬¬ 3 epochï¼‰
â”œâ”€â”€ lora_adapter.pt          # ç´” LoRA æ¬Šé‡ï¼ˆåƒ… 2MBï¼‰
â”œâ”€â”€ training_curves.png      # Loss/Accuracy æ›²ç·šåœ–
â””â”€â”€ training_log.txt         # è©³ç´°è¨“ç·´æ—¥èªŒ
```

**æŸ¥çœ‹ `training_curves.png`** å¯ä»¥çœ‹åˆ°ï¼š
- å·¦åœ–ï¼šLoss æ‡‰è©²é€æ¼¸ä¸‹é™
- å³åœ–ï¼šAccuracy æ‡‰è©²é€æ¼¸ä¸Šå‡
- å¦‚æœ eval æ›²ç·šæ³¢å‹•å¤§ â†’ å¯èƒ½éœ€è¦é™ä½ learning rate

---

## ğŸ” é—œéµç¨‹å¼ç¢¼è§£é‡‹

### LoRA çš„æ•¸å­¸å¯¦ä½œ

**ä½ç½®**: `lora_linear.py` ç¬¬ 73-79 è¡Œ

```python
def forward(self, x: torch.Tensor) -> torch.Tensor:
    x = self.dropout(x)
    lx = F.linear(x, self.lora_A)      # ç¬¬ä¸€æ­¥ï¼šé™ç¶­åˆ° rank
    lx = F.linear(lx, self.lora_B)     # ç¬¬äºŒæ­¥ï¼šå‡ç¶­å› out_features
    return lx * self.scaling           # ç¬¬ä¸‰æ­¥ï¼šç¸®æ”¾ (Î±/r)
```

**ç‚ºä»€éº¼è¦é€™æ¨£ï¼Ÿ**
- åŸæœ¬æ›´æ–°æ•´å€‹å¤§çŸ©é™£ W (dÃ—k) éœ€è¦ dÃ—k å€‹åƒæ•¸
- LoRA ç”¨å…©å€‹å°çŸ©é™£ A (rÃ—k) å’Œ B (dÃ—r) åªéœ€è¦ rÃ—(d+k) å€‹åƒæ•¸
- ç•¶ r << d, k æ™‚ï¼Œåƒæ•¸é‡å¤§å¹…æ¸›å°‘

**ä¾‹å¦‚**ï¼š
- d = k = 768 (BERT hidden size)
- å®Œæ•´å¾®èª¿ï¼š768 Ã— 768 = 589,824 åƒæ•¸
- LoRA (r=8)ï¼š8 Ã— (768+768) = 12,288 åƒæ•¸
- æ¸›å°‘ **48 å€**ï¼

### å¦‚ä½•å¥—ç”¨åˆ°æ¨¡å‹

**ä½ç½®**: `train_lora_basic.py` ç¬¬ 283-292 è¡Œ

```python
apply_lora_to_model(
    model,
    target_modules=config['lora']['target_modules'],
    rank=config['lora']['rank'],
    alpha=config['lora']['alpha'],
    dropout=config['lora']['dropout'],
)

mark_only_lora_as_trainable(model)  # åªè¨“ç·´ LoRA åƒæ•¸
```

**åšäº†ä»€éº¼ï¼Ÿ**
1. éæ­·æ¨¡å‹æ‰€æœ‰å±¤ï¼Œæ‰¾åˆ°åç¨±åŒ…å« `query`ã€`key`ã€`value`ã€`dense` çš„ Linear å±¤
2. ç”¨ `LinearWithLoRA` æ›¿æ›é€™äº›å±¤ï¼ˆä¿ç•™åŸå§‹æ¬Šé‡ï¼‰
3. å‡çµåŸå§‹ BERT æ¬Šé‡ï¼Œåªé–‹å•Ÿ LoRA åƒæ•¸çš„ `requires_grad`

**ç‚ºä»€éº¼é¸é€™äº›å±¤ï¼Ÿ**
- `query`, `key`, `value`: Self-Attention çš„æ ¸å¿ƒ
- `dense`: Attention è¼¸å‡ºçš„æŠ•å½±å±¤
- é€™äº›å±¤å°ä»»å‹™é©æ‡‰æœ€é‡è¦ï¼Œæ•ˆæœæœ€å¥½

### è¨“ç·´å¾ªç’°çš„æ ¸å¿ƒ

**ä½ç½®**: `train_lora_basic.py` ç¬¬ 115-157 è¡Œ

```python
for batch in dataloader:
    # 1. å‰å‘å‚³æ’­
    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss

    # 2. åå‘å‚³æ’­
    optimizer.zero_grad()
    loss.backward()

    # 3. æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

    # 4. æ›´æ–°åƒæ•¸
    optimizer.step()
    scheduler.step()
```

**æ³¨æ„**ï¼šé›–ç„¶å‘¼å« `model.parameters()`ï¼Œä½†å¯¦éš›åªæœ‰ LoRA åƒæ•¸çš„æ¢¯åº¦éé›¶ï¼ˆå› ç‚ºå…¶ä»–åƒæ•¸è¢«å‡çµäº†ï¼‰ã€‚

---

## â“ å¸¸è¦‹å•é¡Œèˆ‡èª¿æ•´

### Q: è¨“ç·´å¤ªæ…¢æ€éº¼è¾¦ï¼Ÿ

**GPU ä¸Šè¨“ç·´æ…¢**ï¼š
```yaml
# config.yaml
training:
  batch_size: 8  # å¾ 16 é™åˆ° 8
```

**CPU ä¸Šè¨“ç·´æ…¢**ï¼š
```bash
# å…ˆè·‘ 1 å€‹ epoch æ¸¬è©¦
python train_lora_basic.py --num_epochs 1
```

### Q: CUDA Out of Memory

**æ–¹æ¡ˆ 1ï¼šé™ä½ batch size**
```yaml
training:
  batch_size: 8  # æˆ–æ›´å° (4)
```

**æ–¹æ¡ˆ 2ï¼šç¸®çŸ­åºåˆ—é•·åº¦**
```yaml
training:
  max_length: 64  # å¾ 128 é™åˆ° 64
```

### Q: æº–ç¢ºç‡ä¸ç†æƒ³æ€éº¼è¾¦ï¼Ÿ

**ç­–ç•¥ 1ï¼šå¢åŠ  LoRA å®¹é‡**
```yaml
lora:
  rank: 16       # å¾ 8 å¢åŠ åˆ° 16
  alpha: 32.0    # å°æ‡‰èª¿æ•´ï¼ˆ= 2 Ã— rankï¼‰
```

**ç­–ç•¥ 2ï¼šèª¿æ•´å­¸ç¿’ç‡**
```bash
python train_lora_basic.py --alpha 32  # å‘½ä»¤åˆ—è¦†å¯«
```
å˜—è©¦ 2e-4, 5e-4 çœ‹å“ªå€‹æ•ˆæœå¥½ã€‚

**ç­–ç•¥ 3ï¼šè¨“ç·´æ›´ä¹…**
```bash
python train_lora_basic.py --num_epochs 5
```

### Q: å¦‚ä½•æ›æˆå…¶ä»–æ•¸æ“šé›†ï¼Ÿ

**ä¿®æ”¹ config.yaml**ï¼š
```yaml
data:
  dataset: "glue"
  subset: "mrpc"      # æ”¹æˆå…¶ä»– GLUE ä»»å‹™
  train_split: "train"
  eval_split: "validation"
```

**èª¿æ•´è³‡æ–™è™•ç†**ï¼ˆå¦‚æœæ ¼å¼ä¸åŒï¼‰ï¼š
ä¿®æ”¹ `train_lora_basic.py` çš„ `prepare_dataset` å‡½æ•¸ã€‚

### Q: å¦‚ä½•åœ¨è‡ªå·±çš„æ•¸æ“šä¸Šè¨“ç·´ï¼Ÿ

éœ€è¦ä¿®æ”¹ï¼š
1. `prepare_dataset`: è¼‰å…¥ä½ çš„æ•¸æ“š
2. `tokenize_function`: æ ¹æ“šä½ çš„æ ¼å¼èª¿æ•´
3. `num_labels`: æ ¹æ“šä½ çš„åˆ†é¡æ•¸é‡

**ç¯„ä¾‹**ï¼š
```python
# è¼‰å…¥è‡ªå·±çš„ CSV æª”æ¡ˆ
import pandas as pd
df = pd.read_csv("my_data.csv")
dataset = Dataset.from_pandas(df)
```

---

## ğŸ§ª å¯¦é©—å»ºè­°

### åŸºæœ¬å¯¦é©—ï¼ˆç¢ºä¿ç†è§£ï¼‰

1. **å®Œæˆä¸€æ¬¡å®Œæ•´è¨“ç·´**
   - è§€å¯Ÿåƒæ•¸çµ±è¨ˆè¼¸å‡º
   - ç†è§£ç‚ºä½•åªè¨“ç·´ 0.27% åƒæ•¸

2. **ä½¿ç”¨äº’å‹•æ¨¡å¼æ¸¬è©¦**
   - è¼¸å…¥æ­£é¢/è² é¢å¥å­
   - è§€å¯Ÿä¿¡å¿ƒåˆ†æ•¸è®ŠåŒ–

3. **åˆ†æè¨“ç·´æ›²ç·š**
   - æ‰“é–‹ `output/training_curves.png`
   - ç¢ºèª Loss ä¸‹é™ã€Accuracy ä¸Šå‡

### é€²éšå¯¦é©—ï¼ˆæ·±å…¥ç†è§£ï¼‰

1. **æ¯”è¼ƒä¸åŒ rank çš„æ•ˆæœ**
   ```bash
   python train_lora_basic.py --rank 4
   python train_lora_basic.py --rank 8
   python train_lora_basic.py --rank 16
   ```
   è§€å¯Ÿåƒæ•¸é‡ã€è¨“ç·´æ™‚é–“ã€æº–ç¢ºç‡çš„è®ŠåŒ–ã€‚

2. **ä¿®æ”¹ target_modules**
   ```yaml
   # config.yaml
   lora:
     target_modules:
       - "query"
       - "value"  # åªç”¨ Q å’Œ Vï¼Œä¸ç”¨ K
   ```
   çœ‹çœ‹æº–ç¢ºç‡æœƒä¸‹é™å¤šå°‘ã€‚

3. **å¯¦ä½œæ¬Šé‡åˆä½µ**
   ```python
   from lora_linear import merge_lora_weights
   merge_lora_weights(model)
   model.save_pretrained("./merged_bert")
   ```
   åˆä½µå¾Œçš„æ¨¡å‹å¯ä»¥ç•¶æ¨™æº– BERT ä½¿ç”¨ï¼ˆä½†é«”ç©è®Šå¤§ï¼‰ã€‚

---

## ğŸ“– æ¥ä¸‹ä¾†å¯ä»¥å­¸ä»€éº¼

### ç†è§£æ›´æ·±

**LoRA åŸç†**
- ğŸ“„ è«–æ–‡ï¼š[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- ğŸ§® ç‚ºä»€éº¼ä½ç§©çŸ©é™£èƒ½æœ‰æ•ˆè¿‘ä¼¼ï¼Ÿï¼ˆçŸ©é™£åˆ†è§£ç†è«–ï¼‰
- ğŸ’¡ ç‚ºä»€éº¼ Attention å±¤ç‰¹åˆ¥é©åˆ LoRAï¼Ÿ

**BERT æ¶æ§‹**
- ğŸ—ï¸ Transformer çš„ Self-Attention æ©Ÿåˆ¶
- ğŸ”„ BERT çš„é è¨“ç·´ä»»å‹™ï¼ˆMLM, NSPï¼‰
- ğŸ“Š ç‚ºä»€éº¼å¾®èª¿æ•ˆæœå¥½ï¼Ÿ

**è¨“ç·´æŠ€å·§**
- ğŸ“ˆ Learning Rate Schedulingï¼ˆç‚ºä½•éœ€è¦ warmupï¼Ÿï¼‰
- ğŸ¯ Gradient Clippingï¼ˆå¦‚ä½•é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Ÿï¼‰
- âš–ï¸ å¦‚ä½•åˆ¤æ–·éæ“¬åˆï¼Ÿ

### æŠ€èƒ½æ“´å±•

**ä¸‹ä¸€å€‹ Task**
- ğŸš€ **Task 02: QLoRA** - 4-bit é‡åŒ– + LoRAï¼Œè¨˜æ†¶é«”å†æ¸›ä¸€åŠ
- ğŸš€ AdaLoRA - è‡ªé©æ‡‰åˆ†é… rank
- ğŸš€ IAÂ³ - æ›´æ¿€é€²çš„åƒæ•¸æ•ˆç‡æ–¹æ³•

**å¯¦éš›æ‡‰ç”¨**
- ğŸŒ éƒ¨ç½²åˆ° API æœå‹™
- ğŸ”„ å¤šä»»å‹™å­¸ç¿’ï¼ˆä¸€å€‹ BERT + å¤šå€‹ LoRA adapterï¼‰
- ğŸ’¾ LoRA adapter ç®¡ç†èˆ‡åˆ‡æ›

**å·¥ç¨‹å„ªåŒ–**
- âš¡ Mixed Precision Training (FP16/BF16)
- ğŸ”§ Gradient Checkpointingï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰
- ğŸ“¦ ONNX å°å‡ºèˆ‡æ¨è«–åŠ é€Ÿ

### ç¨‹å¼èƒ½åŠ›

**PyTorch é€²éš**
- ğŸ”¨ è‡ªå®šç¾© `nn.Module`
- ğŸ¨ Hook æ©Ÿåˆ¶ï¼ˆç›£æ§ä¸­é–“å±¤è¼¸å‡ºï¼‰
- ğŸ’¾ Checkpoint ç®¡ç†èˆ‡çºŒè¨“

**Transformers åº«**
- ğŸ“š AutoModel ç³»åˆ—çš„ä½¿ç”¨
- ğŸ’¿ æ¨¡å‹å„²å­˜/è¼‰å…¥çš„æœ€ä½³å¯¦è¸
- ğŸ”§ Tokenizer çš„ç´°ç¯€

**è³‡æ–™è™•ç†**
- ğŸ“Š Hugging Face `datasets` åº«
- ğŸ”„ è³‡æ–™å¢å¼·ï¼ˆData Augmentationï¼‰
- âš–ï¸ è™•ç†é¡åˆ¥ä¸å¹³è¡¡

---

## ğŸ“š å»¶ä¼¸é–±è®€

### è«–æ–‡
- [LoRA åŸå§‹è«–æ–‡](https://arxiv.org/abs/2106.09685)
- [BERT åŸå§‹è«–æ–‡](https://arxiv.org/abs/1810.04805)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

### å¯¦ä½œåƒè€ƒ
- [Hugging Face PEFT åº«](https://github.com/huggingface/peft) - å®˜æ–¹ LoRA å¯¦ä½œ
- [Microsoft LoRA å¯¦ä½œ](https://github.com/microsoft/LoRA)

### æ•™å­¸è³‡æº
- [Hugging Face Course](https://huggingface.co/course) - Transformers å®Œæ•´æ•™å­¸
- [PyTorch å®˜æ–¹æ•™å­¸](https://pytorch.org/tutorials/)

---

## ğŸ¤ å…±åŒå­¸ç¿’

é€™æ˜¯ä¸€ä»½å…±åŒå­¸ç¿’çš„ç­†è¨˜ï¼Œæ­¡è¿ï¼š
- ğŸ› ç™¼ç¾å•é¡Œï¼Ÿæ Issue
- ğŸ’¡ æœ‰æ›´å¥½çš„è§£é‡‹ï¼Ÿæ PR
- ğŸ¤” æœ‰ç–‘å•ï¼Ÿåœ¨è¨è«–å€ç™¼å•

---

## ğŸ“„ æª”æ¡ˆçµæ§‹ç¸½è¦½

```
task01_lora_basic/
â”œâ”€â”€ lora_linear.py          # âœ¨ æ ¸å¿ƒï¼šLoRA æ•¸å­¸å¯¦ä½œ
â”œâ”€â”€ train_lora_basic.py     # ğŸš€ ä¸»è¦ï¼šå®Œæ•´è¨“ç·´æµç¨‹
â”œâ”€â”€ inference_example.py    # ğŸ¯ æ‡‰ç”¨ï¼šæ¨è«–æ¸¬è©¦
â”œâ”€â”€ config.yaml             # âš™ï¸  é…ç½®ï¼šæ‰€æœ‰è¶…åƒæ•¸
â”œâ”€â”€ requirements.txt        # ğŸ“¦ ç’°å¢ƒï¼šå¥—ä»¶æ¸…å–®
â”œâ”€â”€ test_installation.py    # âœ… æª¢æŸ¥ï¼šå®‰è£é©—è­‰
â”œâ”€â”€ GUIDE.md                # ğŸ“– æ•™å­¸ï¼šè©³ç´°æŒ‡å¼•
â””â”€â”€ README.md               # ğŸ“ æœ¬æª”æ¡ˆï¼šå­¸ç¿’ç­†è¨˜
```

---

<div align="center">

**æº–å‚™å¥½äº†å—ï¼Ÿé–‹å§‹ä½ çš„ç¬¬ä¸€æ¬¡ LoRA å¾®èª¿ï¼** ğŸš€

```bash
python train_lora_basic.py
```

æœ‰å•é¡Œéš¨æ™‚å›ä¾†æŸ¥é€™ä»½ç­†è¨˜ ğŸ“–

</div>
