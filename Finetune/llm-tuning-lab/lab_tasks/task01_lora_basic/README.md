# Task 01: LoRA åŸºç¤å¯¦ä½œ

> å¾é›¶é–‹å§‹æ‰‹å¯« LoRA æ¨¡çµ„ï¼Œç†è§£åƒæ•¸é«˜æ•ˆå¾®èª¿æ ¸å¿ƒåŸç†

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ä»»å‹™å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š

- âœ… **ç†è§£ LoRA æ•¸å­¸åŸç†**ï¼šä½ç§©åˆ†è§£èˆ‡çŸ©é™£åˆ†è§£
- âœ… **æ‰‹å¯« LoRA æ¨¡çµ„**ï¼šå¯¦ä½œå®Œæ•´çš„ LoRALayer
- âœ… **æ‡‰ç”¨åˆ° Transformer**ï¼šå°‡ LoRA æ•´åˆåˆ°èªè¨€æ¨¡å‹
- âœ… **è¨“ç·´èˆ‡è©•ä¼°**ï¼šå®Œæˆä¸€æ¬¡å®Œæ•´çš„å¾®èª¿æµç¨‹
- âœ… **åˆ†æåƒæ•¸èˆ‡è¨˜æ†¶é«”**ï¼šé‡åŒ– LoRA çš„æ•ˆç‡å„ªå‹¢

---

## ğŸ“‹ å‰ç½®çŸ¥è­˜

### å¿…å‚™çŸ¥è­˜
- Python ç¨‹å¼è¨­è¨ˆï¼ˆä¸­ç´šï¼‰
- PyTorch åŸºç¤ï¼ˆ`nn.Module`, `nn.Linear`ï¼‰
- Transformer åŸºæœ¬æ¦‚å¿µï¼ˆAttention æ©Ÿåˆ¶ï¼‰

### å»ºè­°é ç¿’
- ğŸ“– [LoRA ç†è«–](../../docs/01_lora_theory.md)
- ğŸ“„ [LoRA è«–æ–‡](https://arxiv.org/abs/2106.09685)

---

## ğŸ› ï¸ ç’°å¢ƒè¨­å®š

### 1. ä¾è³´å¥—ä»¶

```bash
pip install torch>=2.0.0
pip install transformers>=4.30.0
pip install datasets
pip install matplotlib
pip install tensorboard
```

### 2. ç¡¬é«”éœ€æ±‚

- **æœ€ä½é…ç½®**ï¼šGTX 1080 Ti (11GB)
- **å»ºè­°é…ç½®**ï¼šRTX 3090 (24GB)
- **CPU Only**ï¼šå¯åŸ·è¡Œä½†é€Ÿåº¦è¼ƒæ…¢

---

## ğŸ“‚ ä»»å‹™æª”æ¡ˆçµæ§‹

```
task01_lora_basic/
â”œâ”€â”€ README.md              # æœ¬æ–‡ä»¶
â”œâ”€â”€ GUIDE.md               # è©³ç´°æ•™å­¸æŒ‡å¼•
â”œâ”€â”€ lora_linear.py         # æ‰‹å¯« LoRA æ¨¡çµ„
â”œâ”€â”€ train_lora_basic.py    # è¨“ç·´è…³æœ¬
â”œâ”€â”€ config.yaml            # è¨“ç·´é…ç½®
â”œâ”€â”€ utils.py               # è¼”åŠ©å‡½æ•¸
â”œâ”€â”€ checklist.md           # é©—æ”¶æ¸…å–®
â”œâ”€â”€ discussion.md          # å»¶ä¼¸å•é¡Œ
â””â”€â”€ expected_output/       # é æœŸè¼¸å‡ºç¯„ä¾‹
    â”œâ”€â”€ loss_curve.png
    â”œâ”€â”€ adapter_model.bin
    â””â”€â”€ training_log.txt
```

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### Step 1: é–±è®€æŒ‡å¼•

```bash
cat GUIDE.md
```

### Step 2: æª¢æŸ¥ LoRA å¯¦ä½œ

```bash
# æª¢è¦–æ‰‹å¯« LoRA æ¨¡çµ„
cat lora_linear.py
```

### Step 3: åŸ·è¡Œè¨“ç·´

```bash
# ä½¿ç”¨é è¨­é…ç½®è¨“ç·´
python train_lora_basic.py --config config.yaml

# è‡ªè¨‚åƒæ•¸
python train_lora_basic.py \
    --model gpt2 \
    --rank 8 \
    --alpha 16 \
    --epochs 3 \
    --lr 2e-4
```

### Step 4: æŸ¥çœ‹çµæœ

```bash
# æŸ¥çœ‹è¨“ç·´æ›²ç·š
tensorboard --logdir ./logs

# æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
ls -lh output/
```

---

## ğŸ“Š é æœŸæˆæœ

å®Œæˆè¨“ç·´å¾Œï¼Œä½ æ‡‰è©²ç²å¾—ï¼š

### 1. è¼¸å‡ºæª”æ¡ˆ

```
output/
â”œâ”€â”€ adapter_model.bin           # LoRA æ¬Šé‡ (~2MB)
â”œâ”€â”€ adapter_config.json         # LoRA é…ç½®
â”œâ”€â”€ training_loss_curve.png     # Loss æ›²ç·šåœ–
â”œâ”€â”€ training_metrics.json       # è¨“ç·´æŒ‡æ¨™
â””â”€â”€ model_comparison.txt        # åƒæ•¸é‡å°æ¯”
```

### 2. è¨“ç·´æŒ‡æ¨™

- **è¨“ç·´ Loss**ï¼šå¾ ~3.5 é™åˆ° <2.0
- **é©—è­‰ Loss**ï¼š<2.5
- **è¨“ç·´æ™‚é–“**ï¼š~15-30 åˆ†é˜ (RTX 3090)

### 3. åƒæ•¸æ•ˆç‡

```
åŸå§‹æ¨¡å‹åƒæ•¸ï¼š124M (GPT-2)
LoRA å¯è¨“ç·´åƒæ•¸ï¼š~294K (rank=8)
åƒæ•¸æ¸›å°‘ï¼š~422Ã— ğŸ‰
è¨˜æ†¶é«”ç¯€çœï¼š~3.5Ã— ğŸš€
```

---

## ğŸ“ å¯¦ä½œé‡é»

### æ ¸å¿ƒæ¦‚å¿µ

#### 1. LoRA çŸ©é™£åˆ†è§£

```python
# åŸå§‹æ¬Šé‡æ›´æ–°
W' = Wâ‚€ + Î”W  # Î”W æ˜¯ dÃ—k çŸ©é™£

# LoRA ä½ç§©è¿‘ä¼¼
W' = Wâ‚€ + BA  # B: dÃ—r, A: rÃ—k, r << min(d,k)
```

#### 2. å‰å‘å‚³æ’­

```python
class LoRALayer:
    def forward(self, x):
        # åŸºç¤è¼¸å‡ºï¼ˆå‡çµï¼‰
        base_out = self.linear(x)

        # LoRA è·¯å¾‘
        lora_out = self.lora_B(self.lora_A(x))

        # åˆä½µè¼¸å‡º
        return base_out + lora_out * self.scaling
```

#### 3. åƒæ•¸åˆå§‹åŒ–

```python
# A: Kaiming uniform (æœ‰æ¢¯åº¦)
nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))

# B: é›¶åˆå§‹åŒ–ï¼ˆç¢ºä¿åˆå§‹æ™‚ LoRA è¼¸å‡ºç‚º 0ï¼‰
nn.init.zeros_(self.lora_B)
```

---

## ğŸ” é—œéµå¯¦é©—

### å¯¦é©— 1ï¼šä¸åŒ Rank çš„å½±éŸ¿

```bash
# æ¸¬è©¦å¤šå€‹ rank å€¼
for rank in 2 4 8 16 32; do
    python train_lora_basic.py --rank $rank --output rank_${rank}
done

# æ¯”è¼ƒçµæœ
python utils.py compare_ranks --results_dir ./results
```

**é æœŸè§€å¯Ÿï¼š**
- rank=2ï¼šå¯è¨“ç·´ä½†æ•ˆæœè¼ƒå·®
- rank=8ï¼šå¹³è¡¡é»ï¼Œæ¨è–¦å€¼
- rank=32ï¼šæ•ˆæœæ¥è¿‘å…¨åƒæ•¸å¾®èª¿

### å¯¦é©— 2ï¼šAlpha ç¸®æ”¾ä¿‚æ•¸

```bash
# å›ºå®š rank=8ï¼Œæ¸¬è©¦ä¸åŒ alpha
for alpha in 4 8 16 32; do
    python train_lora_basic.py --rank 8 --alpha $alpha
done
```

**é æœŸè§€å¯Ÿï¼š**
- alpha éå°ï¼šLoRA å½±éŸ¿ä¸è¶³
- alpha éå¤§ï¼šå¯èƒ½ç ´å£é è¨“ç·´çŸ¥è­˜
- alpha = rank * 2ï¼šç¶“é©—æœ€ä½³å€¼

### å¯¦é©— 3ï¼šæ‡‰ç”¨å±¤é¸æ“‡

```bash
# åªå° Q, V ä½¿ç”¨ LoRA
python train_lora_basic.py --target_modules q_proj v_proj

# å°æ‰€æœ‰ Linear å±¤ä½¿ç”¨ LoRA
python train_lora_basic.py --target_modules all
```

---

## âœ… é©—æ”¶æ¨™æº–

å®Œæˆå¾Œï¼Œè«‹æª¢æŸ¥ [checklist.md](checklist.md) ç¢ºèªï¼š

- [ ] `lora_linear.py` å¯¦ä½œæ­£ç¢º
- [ ] è¨“ç·´æˆåŠŸå®Œæˆï¼ŒLoss æ”¶æ–‚
- [ ] ç”¢ç”Ÿ `adapter_model.bin` æª”æ¡ˆ
- [ ] ç¹ªè£½ Loss æ›²ç·šåœ–
- [ ] è¨ˆç®—åƒæ•¸é‡ä¸¦é©—è­‰ç¯€çœæ•ˆæœ
- [ ] å›ç­” `discussion.md` ä¸­çš„å»¶ä¼¸å•é¡Œ

---

## ğŸ› å¸¸è¦‹å•é¡Œ

### Q1: è¨“ç·´æ™‚å‡ºç¾ CUDA Out of Memory

**A**: å˜—è©¦ä»¥ä¸‹æ–¹æ³•ï¼š
```bash
# æ¸›å°‘ batch size
python train_lora_basic.py --batch_size 4

# ä½¿ç”¨æ¢¯åº¦ç´¯ç©
python train_lora_basic.py --gradient_accumulation_steps 4

# ä½¿ç”¨æ›´å°çš„æ¨¡å‹
python train_lora_basic.py --model distilgpt2
```

### Q2: Loss æ²’æœ‰ä¸‹é™

**A**: æª¢æŸ¥ä»¥ä¸‹é …ç›®ï¼š
- LoRA åƒæ•¸æ˜¯å¦æ­£ç¢ºè¨­ç½® `requires_grad=True`
- åŸå§‹æ¬Šé‡æ˜¯å¦æ­£ç¢ºå‡çµ
- å­¸ç¿’ç‡æ˜¯å¦åˆé©ï¼ˆå»ºè­° 1e-4 ~ 5e-4ï¼‰

### Q3: è¨“ç·´é€Ÿåº¦å¾ˆæ…¢

**A**:
- æª¢æŸ¥æ˜¯å¦åœ¨ä½¿ç”¨ GPUï¼š`torch.cuda.is_available()`
- æ¸›å°‘åºåˆ—é•·åº¦ï¼š`--max_length 256`
- ä½¿ç”¨æ··åˆç²¾åº¦è¨“ç·´ï¼š`--fp16`

---

## ğŸ“š å»¶ä¼¸è³‡æº

### é€²éšé–±è®€
- [AdaLoRA è«–æ–‡](https://arxiv.org/abs/2303.10512) - å‹•æ…‹èª¿æ•´ rank
- [IAÂ³ è«–æ–‡](https://arxiv.org/abs/2205.05638) - å¦ä¸€ç¨®é«˜æ•ˆå¾®èª¿æ–¹æ³•
- [Hugging Face PEFT æ–‡æª”](https://huggingface.co/docs/peft)

### ç›¸é—œä»»å‹™
- â­ï¸ [Task 02: QLoRA å¯¦æˆ°](../task02_qlora/) - åŠ å…¥é‡åŒ–æŠ€è¡“
- â­ï¸ [Task 03: SDK èˆ‡ API](../task03_sdk_api/) - å»ºç«‹è¨“ç·´æœå‹™

---

## ğŸ¤ éœ€è¦å¹«åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°å•é¡Œï¼š

1. æŸ¥çœ‹ [GUIDE.md](GUIDE.md) è©³ç´°æ•™å­¸
2. é–±è®€ [discussion.md](discussion.md) å¸¸è¦‹å•é¡Œ
3. åƒè€ƒ [expected_output/](expected_output/) ç¯„ä¾‹è¼¸å‡º
4. åœ¨ GitHub Issues æå•

---

<div align="center">

**æº–å‚™å¥½é–‹å§‹äº†å—ï¼ŸğŸš€**

[é–‹å§‹å­¸ç¿’ GUIDE.md](GUIDE.md) | [æŸ¥çœ‹åƒè€ƒç­”æ¡ˆ](lora_linear.py)

**ç¥å­¸ç¿’é †åˆ©ï¼ğŸ’ª**

</div>
