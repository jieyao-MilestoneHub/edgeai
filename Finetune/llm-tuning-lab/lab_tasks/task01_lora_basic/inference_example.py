"""
LoRA Inference Example - SST-2 Sentiment Classification

ç¤ºç¯„å¦‚ä½•ä½¿ç”¨è¨“ç·´å¥½çš„ LoRA æ¨¡å‹é€²è¡Œæƒ…æ„Ÿåˆ†ææ¨è«–

ä½œè€…: Jiao
æˆæ¬Š: MIT
"""

import argparse
from pathlib import Path
import torch
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
)
from lora_linear import (
    apply_lora_to_model,
    load_lora_state_dict,
)


def load_model_with_lora(
    checkpoint_path: str,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
):
    """è¼‰å…¥å¸¶æœ‰ LoRA æ¬Šé‡çš„æ¨¡å‹"""
    print(f"Loading checkpoint from: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device)
    config = checkpoint['config']

    # 1. è¼‰å…¥åŸºç¤æ¨¡å‹
    print(f"Loading base model: {config['model_name']}")
    model = AutoModelForSequenceClassification.from_pretrained(
        config['model_name'],
        num_labels=config['num_labels']
    )
    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])

    # 2. å¥—ç”¨ LoRA çµæ§‹
    print(f"Applying LoRA structure (rank={config['lora']['rank']})")
    apply_lora_to_model(
        model,
        target_modules=config['lora']['target_modules'],
        rank=config['lora']['rank'],
        alpha=config['lora']['alpha'],
        dropout=0.0,  # æ¨è«–æ™‚ä¸éœ€è¦ dropout
    )

    # 3. è¼‰å…¥ LoRA æ¬Šé‡
    print(f"Loading LoRA weights...")
    load_lora_state_dict(model, checkpoint['lora_state_dict'])

    model = model.to(device)
    model.eval()

    print(f"âœ… Model loaded successfully!")
    print(f"   Device: {device}")
    if 'eval_acc' in checkpoint:
        print(f"   Validation Accuracy: {checkpoint['eval_acc']:.4f}")

    return model, tokenizer, device


def batch_predict(
    texts: list[str],
    model,
    tokenizer,
    device: str,
    batch_size: int = 16
) -> list[tuple[str, float]]:
    """æ‰¹æ¬¡é æ¸¬å¤šå€‹æ–‡æœ¬çš„æƒ…æ„Ÿ

    Args:
        texts: è¦é æ¸¬çš„æ–‡æœ¬åˆ—è¡¨
        model: æ¨¡å‹
        tokenizer: Tokenizer
        device: è¨­å‚™ (cpu/cuda)
        batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆé»˜èª 16ï¼‰

    Returns:
        list[tuple[str, float]]: æ¯å€‹æ–‡æœ¬çš„ (æƒ…æ„Ÿæ¨™ç±¤, ä¿¡å¿ƒåˆ†æ•¸)
    """
    label_map = {0: "Negative", 1: "Positive"}
    results = []

    # åˆ†æ‰¹è™•ç†
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]

        # æ‰¹æ¬¡ tokenize
        inputs = tokenizer(
            batch_texts,
            return_tensors="pt",
            truncation=True,
            max_length=128,
            padding=True
        ).to(device)

        # æ‰¹æ¬¡æ¨è«–
        with torch.no_grad():
            outputs = model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=-1)
            pred_labels = torch.argmax(probs, dim=-1)
            confidences = torch.gather(probs, 1, pred_labels.unsqueeze(1)).squeeze(1)

        # æ”¶é›†çµæœ
        for pred_label, confidence in zip(pred_labels.cpu().tolist(), confidences.cpu().tolist()):
            sentiment = label_map[pred_label]
            results.append((sentiment, confidence))

    return results


def interactive_mode(model, tokenizer, device):
    """äº’å‹•æ¨¡å¼ï¼šè®“ä½¿ç”¨è€…è¼¸å…¥å¥å­é€²è¡Œå³æ™‚é æ¸¬"""
    print("\n" + "="*60)
    print("ğŸ­ Interactive Sentiment Analysis")
    print("="*60)
    print("è¼¸å…¥å¥å­é€²è¡Œæƒ…æ„Ÿåˆ†æï¼Œè¼¸å…¥ 'quit' æˆ– 'exit' çµæŸ")
    print("-"*60)

    while True:
        try:
            text = input("\nè«‹è¼¸å…¥å¥å­: ").strip()

            if text.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ Goodbye!")
                break

            if not text:
                print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„å¥å­")
                continue

            # é æ¸¬ï¼ˆä½¿ç”¨ batch_predict è™•ç†å–®ä¸€æ–‡æœ¬ï¼‰
            results = batch_predict([text], model, tokenizer, device)
            sentiment, confidence = results[0]

            # é¡¯ç¤ºçµæœ
            emoji = "ğŸ˜Š" if sentiment == "Positive" else "ğŸ˜"
            print(f"\n{emoji} æƒ…æ„Ÿ: {sentiment}")
            print(f"ğŸ“Š ä¿¡å¿ƒåˆ†æ•¸: {confidence:.2%}")

            # é¡¯ç¤ºä¿¡å¿ƒç­‰ç´š
            if confidence > 0.9:
                level = "éå¸¸ç¢ºå®š"
            elif confidence > 0.7:
                level = "ç¢ºå®š"
            elif confidence > 0.5:
                level = "è¼ƒç‚ºç¢ºå®š"
            else:
                level = "ä¸å¤ªç¢ºå®š"
            print(f"ğŸ¯ ç­‰ç´š: {level}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")


def demo_examples(model, tokenizer, device):
    """ç¤ºç¯„ç¯„ä¾‹ï¼ˆä½¿ç”¨æ‰¹æ¬¡è™•ç†æå‡æ•ˆç‡ï¼‰"""
    print("\n" + "="*60)
    print("ğŸ“ Demo Examples")
    print("="*60)

    examples = [
        "This movie was absolutely fantastic! I loved every minute of it.",
        "The worst film I've ever seen. Totally disappointing.",
        "It was okay, nothing special but not terrible either.",
        "Brilliant performance by the lead actor!",
        "I fell asleep halfway through. So boring.",
        "A masterpiece! Highly recommended.",
        "Waste of time and money.",
        "Pretty good, would watch again.",
    ]

    # æ‰¹æ¬¡é æ¸¬æ‰€æœ‰ç¯„ä¾‹ï¼ˆæ›´é«˜æ•ˆï¼‰
    results = batch_predict(examples, model, tokenizer, device)

    # é¡¯ç¤ºçµæœ
    for i, (text, (sentiment, confidence)) in enumerate(zip(examples, results), 1):
        emoji = "ğŸ˜Š" if sentiment == "Positive" else "ğŸ˜"
        print(f"\n{i}. \"{text}\"")
        print(f"   {emoji} {sentiment} ({confidence:.2%})")


def main(args):
    # è¼‰å…¥æ¨¡å‹
    checkpoint_path = args.checkpoint if args.checkpoint else "output/best_lora_model.pt"

    if not Path(checkpoint_path).exists():
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        print("è«‹å…ˆè¨“ç·´æ¨¡å‹: python train_lora_basic.py")
        return

    model, tokenizer, device = load_model_with_lora(checkpoint_path)

    # æ ¹æ“šæ¨¡å¼åŸ·è¡Œ
    if args.mode == "interactive":
        interactive_mode(model, tokenizer, device)

    elif args.mode == "demo":
        demo_examples(model, tokenizer, device)

    elif args.mode == "text":
        if not args.text:
            print("âŒ è«‹ä½¿ç”¨ --text åƒæ•¸æä¾›è¼¸å…¥æ–‡æœ¬")
            return

        # ä½¿ç”¨ batch_predict è™•ç†å–®ä¸€æ–‡æœ¬
        results = batch_predict([args.text], model, tokenizer, device)
        sentiment, confidence = results[0]
        emoji = "ğŸ˜Š" if sentiment == "Positive" else "ğŸ˜"

        print(f"\nè¼¸å…¥: \"{args.text}\"")
        print(f"{emoji} æƒ…æ„Ÿ: {sentiment}")
        print(f"ğŸ“Š ä¿¡å¿ƒåˆ†æ•¸: {confidence:.2%}")

    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {args.mode}")
        print("å¯ç”¨æ¨¡å¼: interactive, demo, text")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="LoRA Sentiment Classification Inference"
    )
    parser.add_argument(
        "--checkpoint",
        type=str,
        default="output/best_lora_model.pt",
        help="Path to checkpoint file (default: output/best_lora_model.pt)"
    )
    parser.add_argument(
        "--mode",
        type=str,
        default="interactive",
        choices=["interactive", "demo", "text"],
        help="Inference mode: interactive (äº’å‹•æ¨¡å¼), demo (ç¯„ä¾‹å±•ç¤º), text (å–®æ¬¡é æ¸¬)"
    )
    parser.add_argument(
        "--text",
        type=str,
        default=None,
        help="Input text for sentiment analysis (when mode=text)"
    )

    args = parser.parse_args()
    main(args)
