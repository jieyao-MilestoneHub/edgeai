"""
LoRA åŸºç¤è¨“ç·´è…³æœ¬

é€™å€‹è…³æœ¬å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ‰‹å¯«çš„ LoRA å¯¦ä½œä¾†å¾®èª¿èªè¨€æ¨¡å‹ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    python train_lora_basic.py --rank 8 --alpha 16 --epochs 3

ä½œè€…ï¼šLLM Tuning Lab
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import (
    GPT2LMHeadModel,
    GPT2Tokenizer,
    GPT2Config,
    get_linear_schedule_with_warmup,
)
from datasets import load_dataset
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import argparse
import json
import os
from datetime import datetime

# å°å…¥æˆ‘å€‘æ‰‹å¯«çš„ LoRA æ¨¡çµ„
from lora_linear import (
    apply_lora_to_model,
    count_lora_parameters,
    get_lora_parameters,
)


def train_one_epoch(model, dataloader, optimizer, scheduler, device, epoch):
    """
    è¨“ç·´ä¸€å€‹ epoch

    Args:
        model: æ¨¡å‹
        dataloader: è¨“ç·´è³‡æ–™
        optimizer: å„ªåŒ–å™¨
        scheduler: å­¸ç¿’ç‡èª¿åº¦å™¨
        device: è¨ˆç®—è¨­å‚™
        epoch: ç•¶å‰ epoch ç·¨è™Ÿ

    Returns:
        å¹³å‡ loss
    """
    model.train()
    total_loss = 0
    num_batches = len(dataloader)

    progress_bar = tqdm(
        dataloader,
        desc=f"Epoch {epoch}",
        leave=True,
    )

    for batch_idx, batch in enumerate(progress_bar):
        # ç§»å‹•è³‡æ–™åˆ° device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        # å‰å‘å‚³æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=input_ids,  # èªè¨€æ¨¡å‹ï¼šè¼¸å…¥å³æ¨™ç±¤
        )
        loss = outputs.loss

        # åå‘å‚³æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼‰
        torch.nn.utils.clip_grad_norm_(
            model.parameters(),
            max_norm=1.0
        )

        # æ›´æ–°åƒæ•¸
        optimizer.step()
        scheduler.step()

        # è¨˜éŒ„
        total_loss += loss.item()
        current_lr = scheduler.get_last_lr()[0]

        # æ›´æ–°é€²åº¦æ¢
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
            'avg_loss': f'{total_loss/(batch_idx+1):.4f}',
            'lr': f'{current_lr:.2e}',
        })

    avg_loss = total_loss / num_batches
    return avg_loss


@torch.no_grad()
def evaluate(model, dataloader, device):
    """
    è©•ä¼°æ¨¡å‹

    Args:
        model: æ¨¡å‹
        dataloader: è©•ä¼°è³‡æ–™
        device: è¨ˆç®—è¨­å‚™

    Returns:
        å¹³å‡ loss
    """
    model.eval()
    total_loss = 0
    num_batches = len(dataloader)

    progress_bar = tqdm(
        dataloader,
        desc="Evaluating",
        leave=False,
    )

    for batch in progress_bar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=input_ids,
        )

        total_loss += outputs.loss.item()
        progress_bar.set_postfix({
            'loss': f'{outputs.loss.item():.4f}'
        })

    avg_loss = total_loss / num_batches
    return avg_loss


def plot_training_curve(train_losses, eval_losses, save_path):
    """ç¹ªè£½è¨“ç·´æ›²ç·š"""
    plt.figure(figsize=(10, 6))

    epochs = range(1, len(train_losses) + 1)

    plt.plot(epochs, train_losses, 'b-o', label='Train Loss', linewidth=2)
    plt.plot(epochs, eval_losses, 'r-s', label='Eval Loss', linewidth=2)

    plt.xlabel('Epoch', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('LoRA Training Progress', fontsize=14, fontweight='bold')
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)

    # æ·»åŠ æ•¸å€¼æ¨™è¨»
    for i, (train_loss, eval_loss) in enumerate(zip(train_losses, eval_losses)):
        plt.text(i+1, train_loss, f'{train_loss:.3f}', ha='center', va='bottom', fontsize=9)
        plt.text(i+1, eval_loss, f'{eval_loss:.3f}', ha='center', va='top', fontsize=9)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Loss curve saved to: {save_path}")


def main(args):
    """ä¸»è¨“ç·´å‡½æ•¸"""

    # ========== 1. ç’°å¢ƒè¨­å®š ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*60}")
    print(f"ğŸš€ LoRA Training Script")
    print(f"{'='*60}")
    print(f"Device: {device}")
    print(f"PyTorch version: {torch.__version__}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
    print(f"{'='*60}\n")

    # ========== 2. è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer ==========
    print("ğŸ“¥ Loading model and tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained(args.model_name)
    model = GPT2LMHeadModel.from_pretrained(args.model_name)

    # è¨­å®š pad token
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.eos_token_id

    # ========== 3. æ‡‰ç”¨ LoRA ==========
    print(f"\nğŸ”§ Applying LoRA (rank={args.rank}, alpha={args.alpha})...")

    # å° GPT-2 çš„ attention å±¤æ‡‰ç”¨ LoRA
    # GPT-2 ä½¿ç”¨ 'c_attn' (Q,K,V åˆä½µ) å’Œ 'c_proj' (O)
    model = apply_lora_to_model(
        model,
        target_modules=['c_attn', 'c_proj'],  # GPT-2 ç‰¹å®š
        rank=args.rank,
        alpha=args.alpha,
        dropout=args.lora_dropout,
    )

    model = model.to(device)

    # ========== 4. åƒæ•¸çµ±è¨ˆ ==========
    print("\nğŸ“Š Parameter Statistics:")
    print("â”€" * 60)
    param_stats = count_lora_parameters(model)
    print(f"Total parameters:     {param_stats['total']:>15,}")
    print(f"Trainable parameters: {param_stats['trainable']:>15,}")
    print(f"Frozen parameters:    {param_stats['frozen']:>15,}")
    print(f"Trainable percentage: {param_stats['percentage']:>14.4f}%")
    print(f"Parameter reduction:  {param_stats['total']/param_stats['trainable']:>14.1f}Ã—")
    print("â”€" * 60)

    # ========== 5. è¼‰å…¥æ•¸æ“šé›† ==========
    print("\nğŸ“š Loading dataset...")

    # ä½¿ç”¨ WikiText-2 ä½œç‚ºç¤ºç¯„
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

    def tokenize_function(examples):
        """Tokenize æ–‡æœ¬"""
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.max_length,
            padding="max_length",
            return_tensors="pt",
        )

    # Tokenize è³‡æ–™é›†
    print("Tokenizing dataset...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
        desc="Tokenizing",
    )

    # éæ¿¾æ‰ç©ºåºåˆ—
    def filter_empty(example):
        return len(example['input_ids']) > 0

    tokenized_datasets = tokenized_datasets.filter(filter_empty)

    print(f"Train samples: {len(tokenized_datasets['train'])}")
    print(f"Validation samples: {len(tokenized_datasets['validation'])}")

    # å‰µå»º DataLoader
    train_dataloader = DataLoader(
        tokenized_datasets["train"],
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,  # Windows ç›¸å®¹æ€§
    )

    eval_dataloader = DataLoader(
        tokenized_datasets["validation"],
        batch_size=args.batch_size,
        num_workers=0,
    )

    # ========== 6. å„ªåŒ–å™¨èˆ‡èª¿åº¦å™¨ ==========
    print("\nâš™ï¸ Setting up optimizer and scheduler...")

    # åªå„ªåŒ– LoRA åƒæ•¸
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.learning_rate,
        weight_decay=args.weight_decay,
        betas=(0.9, 0.999),
        eps=1e-8,
    )

    # å­¸ç¿’ç‡èª¿åº¦å™¨
    num_training_steps = len(train_dataloader) * args.num_epochs
    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warmup

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps,
    )

    print(f"Total training steps: {num_training_steps}")
    print(f"Warmup steps: {num_warmup_steps}")

    # ========== 7. è¨“ç·´å¾ªç’° ==========
    print(f"\nğŸ‹ï¸ Starting training for {args.num_epochs} epochs...\n")

    train_losses = []
    eval_losses = []
    best_eval_loss = float('inf')

    for epoch in range(1, args.num_epochs + 1):
        print(f"\n{'='*60}")
        print(f"Epoch {epoch}/{args.num_epochs}")
        print(f"{'='*60}")

        # è¨“ç·´
        train_loss = train_one_epoch(
            model, train_dataloader, optimizer, scheduler, device, epoch
        )
        train_losses.append(train_loss)

        # è©•ä¼°
        eval_loss = evaluate(model, eval_dataloader, device)
        eval_losses.append(eval_loss)

        # æ‰“å°çµ±è¨ˆ
        print(f"\nğŸ“ˆ Epoch {epoch} Results:")
        print(f"  Train Loss: {train_loss:.4f}")
        print(f"  Eval Loss:  {eval_loss:.4f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if eval_loss < best_eval_loss:
            best_eval_loss = eval_loss
            print(f"  ğŸŒŸ New best model! (Eval Loss: {eval_loss:.4f})")

            # ä¿å­˜ LoRA æ¬Šé‡
            lora_params = get_lora_parameters(model)
            torch.save({
                'lora_state_dict': lora_params,
                'config': {
                    'rank': args.rank,
                    'alpha': args.alpha,
                    'model_name': args.model_name,
                    'target_modules': ['c_attn', 'c_proj'],
                },
                'training': {
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'eval_loss': eval_loss,
                },
            }, os.path.join(args.output_dir, 'best_adapter_model.bin'))

    # ========== 8. ä¿å­˜æœ€çµ‚çµæœ ==========
    print(f"\n{'='*60}")
    print("ğŸ’¾ Saving results...")
    print(f"{'='*60}")

    # å‰µå»ºè¼¸å‡ºç›®éŒ„
    os.makedirs(args.output_dir, exist_ok=True)

    # ä¿å­˜æœ€çµ‚ LoRA æ¬Šé‡
    lora_params = get_lora_parameters(model)
    torch.save({
        'lora_state_dict': lora_params,
        'config': {
            'rank': args.rank,
            'alpha': args.alpha,
            'model_name': args.model_name,
            'target_modules': ['c_attn', 'c_proj'],
        },
        'training': {
            'final_train_loss': train_losses[-1],
            'final_eval_loss': eval_losses[-1],
            'best_eval_loss': best_eval_loss,
        },
    }, os.path.join(args.output_dir, 'final_adapter_model.bin'))
    print(f"âœ… Saved: {os.path.join(args.output_dir, 'final_adapter_model.bin')}")

    # ä¿å­˜è¨“ç·´ metrics
    metrics = {
        'train_losses': train_losses,
        'eval_losses': eval_losses,
        'best_eval_loss': best_eval_loss,
        'hyperparameters': vars(args),
        'param_stats': param_stats,
    }

    with open(os.path.join(args.output_dir, 'training_metrics.json'), 'w') as f:
        json.dump(metrics, f, indent=2)
    print(f"âœ… Saved: {os.path.join(args.output_dir, 'training_metrics.json')}")

    # ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_training_curve(
        train_losses,
        eval_losses,
        os.path.join(args.output_dir, 'training_loss_curve.png')
    )

    # ========== 9. ç¸½çµ ==========
    print(f"\n{'='*60}")
    print("ğŸ‰ Training Completed!")
    print(f"{'='*60}")
    print(f"Final Train Loss: {train_losses[-1]:.4f}")
    print(f"Final Eval Loss:  {eval_losses[-1]:.4f}")
    print(f"Best Eval Loss:   {best_eval_loss:.4f}")
    print(f"\nResults saved to: {args.output_dir}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LoRA Fine-tuning Script")

    # æ¨¡å‹åƒæ•¸
    parser.add_argument("--model_name", type=str, default="gpt2",
                       help="Pretrained model name")

    # LoRA åƒæ•¸
    parser.add_argument("--rank", type=int, default=8,
                       help="LoRA rank")
    parser.add_argument("--alpha", type=float, default=16.0,
                       help="LoRA alpha (scaling factor)")
    parser.add_argument("--lora_dropout", type=float, default=0.0,
                       help="LoRA dropout probability")

    # è¨“ç·´åƒæ•¸
    parser.add_argument("--num_epochs", type=int, default=3,
                       help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="Training batch size")
    parser.add_argument("--learning_rate", type=float, default=2e-4,
                       help="Learning rate")
    parser.add_argument("--weight_decay", type=float, default=0.01,
                       help="Weight decay")
    parser.add_argument("--max_length", type=int, default=512,
                       help="Maximum sequence length")

    # è¼¸å‡ºåƒæ•¸
    parser.add_argument("--output_dir", type=str, default="./output",
                       help="Output directory")

    args = parser.parse_args()

    # é‹è¡Œè¨“ç·´
    main(args)
