"""
LoRA Basic Training Script - SST-2 Sentiment Classification

å®Œæ•´çš„ LoRA è¨“ç·´æµç¨‹ï¼Œæ”¯æ´ï¼š
- å¾ config.yaml è®€å–é…ç½®
- è‡ªå‹•å¥—ç”¨ LoRA åˆ° BERT æ¨¡å‹
- GLUE SST-2 æƒ…æ„Ÿåˆ†é¡ä»»å‹™
- å®Œæ•´çš„è¨“ç·´èˆ‡è©•ä¼°å¾ªç’°
- æº–ç¢ºç‡è©•ä¼°æŒ‡æ¨™
- è¨“ç·´é€²åº¦è¦–è¦ºåŒ–
- LoRA æ¬Šé‡å„²å­˜èˆ‡è¼‰å…¥

ä½œè€…: Jiao
æˆæ¬Š: MIT
"""

import yaml
import argparse
from pathlib import Path
from typing import Dict, Any

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.optim import AdamW
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    get_scheduler,
)
from datasets import load_dataset
import matplotlib.pyplot as plt
from tqdm import tqdm
import evaluate

# å¾åŒç›®éŒ„å°å…¥æˆ‘å€‘å¯¦ä½œçš„ LoRA æ¨¡çµ„
from lora_linear import (
    apply_lora_to_model,
    mark_only_lora_as_trainable,
    count_lora_parameters,
    get_lora_state_dict,
    merge_lora_weights,
)


def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """è¼‰å…¥ YAML é…ç½®æª”"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def prepare_dataset(config: Dict[str, Any], tokenizer):
    """æº–å‚™ä¸¦é è™•ç† SST-2 è³‡æ–™é›†"""
    print(f"Loading dataset: {config['data']['dataset']}/{config['data']['subset']}")

    # è¼‰å…¥ GLUE SST-2 è³‡æ–™é›†
    dataset = load_dataset(
        config['data']['dataset'],
        config['data']['subset']
    )

    # Tokenization å‡½æ•¸ (SST-2 æ ¼å¼: {sentence, label})
    def tokenize_function(examples):
        return tokenizer(
            examples["sentence"],
            truncation=True,
            max_length=config['training']['max_length'],
            return_tensors=None,
        )

    # å°è³‡æ–™é›†é€²è¡Œ tokenize
    print("Tokenizing dataset...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        desc="Tokenizing",
    )

    # è¨­å®šæ ¼å¼ç‚º PyTorch tensor
    tokenized_datasets.set_format(
        type="torch",
        columns=["input_ids", "attention_mask", "label"]
    )

    return tokenized_datasets


def train_one_epoch(
    model: nn.Module,
    dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    scheduler: Any,
    device: torch.device,
    gradient_clip: float = 1.0,
) -> tuple[float, float]:
    """è¨“ç·´ä¸€å€‹ epoch"""
    model.train()
    total_loss = 0.0

    # ä½¿ç”¨ evaluate åº«ä¾†è¨ˆç®— accuracy
    metric = evaluate.load("accuracy")

    progress_bar = tqdm(dataloader, desc="Training", leave=False)

    for batch in progress_bar:
        # å°‡è³‡æ–™ç§»åˆ°è£ç½®
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        # å‰å‘å‚³æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
        )
        loss = outputs.loss
        logits = outputs.logits

        # æ”¶é›†é æ¸¬çµæœç”¨æ–¼è¨ˆç®—æº–ç¢ºç‡
        predictions = torch.argmax(logits, dim=-1)
        metric.add_batch(predictions=predictions.cpu(), references=labels.cpu())

        # åå‘å‚³æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        if gradient_clip > 0:
            torch.nn.utils.clip_grad_norm_(
                model.parameters(),
                max_norm=gradient_clip
            )

        # æ›´æ–°åƒæ•¸
        optimizer.step()
        scheduler.step()

        # è¨˜éŒ„
        total_loss += loss.item()

        # æ›´æ–°é€²åº¦æ¢
        progress_bar.set_postfix({
            'loss': f'{loss.item():.4f}',
        })

    avg_loss = total_loss / len(dataloader)
    avg_accuracy = metric.compute()['accuracy']

    return avg_loss, avg_accuracy


def evaluate(
    model: nn.Module,
    dataloader: DataLoader,
    device: torch.device,
) -> tuple[float, float]:
    """è©•ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0.0

    # ä½¿ç”¨ evaluate åº«ä¾†è¨ˆç®— accuracy
    metric = evaluate.load("accuracy")

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Evaluating", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['label'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
            )

            loss = outputs.loss
            logits = outputs.logits

            # æ”¶é›†é æ¸¬çµæœç”¨æ–¼è¨ˆç®—æº–ç¢ºç‡
            predictions = torch.argmax(logits, dim=-1)
            metric.add_batch(predictions=predictions.cpu(), references=labels.cpu())

            total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    avg_accuracy = metric.compute()['accuracy']

    return avg_loss, avg_accuracy


def plot_training_curves(
    train_losses: list,
    eval_losses: list,
    train_accuracies: list,
    eval_accuracies: list,
    save_path: str = "training_curves.png"
):
    """ç¹ªè£½è¨“ç·´æ›²ç·šï¼ˆLoss å’Œ Accuracyï¼‰"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    epochs = range(1, len(train_losses) + 1)

    # Loss æ›²ç·š
    ax1.plot(epochs, train_losses, 'b-o', label='Train Loss', linewidth=2)
    ax1.plot(epochs, eval_losses, 'r-s', label='Eval Loss', linewidth=2)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)

    # Accuracy æ›²ç·š
    ax2.plot(epochs, train_accuracies, 'b-o', label='Train Accuracy', linewidth=2)
    ax2.plot(epochs, eval_accuracies, 'r-s', label='Eval Accuracy', linewidth=2)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy', fontsize=12)
    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')
    ax2.legend(fontsize=11)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Training curves saved to: {save_path}")


def main(args):
    # ========== 1. è¼‰å…¥é…ç½® ==========
    config_path = args.config if args.config else "config.yaml"
    print(f"Loading config from: {config_path}")
    config = load_config(config_path)

    # å…è¨±å‘½ä»¤åˆ—è¦†å¯«é…ç½®
    if args.rank is not None:
        config['lora']['rank'] = args.rank
    if args.alpha is not None:
        config['lora']['alpha'] = args.alpha
    if args.num_epochs is not None:
        config['training']['num_epochs'] = args.num_epochs

    # ========== 2. è¨­å®šè£ç½® ==========
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # ========== 3. è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer ==========
    print(f"\nLoading model: {config['model_name']}")
    print(f"Task: {config['task_type']} with {config['num_labels']} labels")

    model = AutoModelForSequenceClassification.from_pretrained(
        config['model_name'],
        num_labels=config['num_labels']
    )
    tokenizer = AutoTokenizer.from_pretrained(config['model_name'])

    # ========== 4. å¥—ç”¨ LoRA ==========
    print(f"\nApplying LoRA:")
    print(f"  - rank: {config['lora']['rank']}")
    print(f"  - alpha: {config['lora']['alpha']}")
    print(f"  - dropout: {config['lora']['dropout']}")
    print(f"  - target_modules: {config['lora']['target_modules']}")

    apply_lora_to_model(
        model,
        target_modules=config['lora']['target_modules'],
        rank=config['lora']['rank'],
        alpha=config['lora']['alpha'],
        dropout=config['lora']['dropout'],
    )

    # åªè¨“ç·´ LoRA åƒæ•¸
    mark_only_lora_as_trainable(model)

    # é¡¯ç¤ºåƒæ•¸çµ±è¨ˆ
    param_stats = count_lora_parameters(model)
    print("\n" + "="*60)
    print("ğŸ“Š Parameter Statistics:")
    print(f"  Total parameters:     {param_stats['total']:>12,}")
    print(f"  Trainable parameters: {param_stats['trainable']:>12,}")
    print(f"  Frozen parameters:    {param_stats['frozen']:>12,}")
    print(f"  Trainable percentage: {param_stats['percentage']:>11.4f}%")
    print("="*60)

    # å°‡æ¨¡å‹ç§»åˆ°è£ç½®
    model = model.to(device)

    # ========== 5. æº–å‚™è³‡æ–™é›† ==========
    tokenized_datasets = prepare_dataset(config, tokenizer)

    # ä½¿ç”¨å‹•æ…‹ paddingï¼ˆæ›´ç¯€çœè¨˜æ†¶é«”ï¼‰
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    train_dataloader = DataLoader(
        tokenized_datasets[config['data']['train_split']],
        batch_size=config['training']['batch_size'],
        collate_fn=data_collator,
        shuffle=True,
    )

    eval_dataloader = DataLoader(
        tokenized_datasets[config['data']['eval_split']],
        batch_size=config['training']['batch_size'],
        collate_fn=data_collator,
        shuffle=False,
    )

    print(f"\nğŸ“š Dataset Info:")
    print(f"  Train samples: {len(tokenized_datasets[config['data']['train_split']])}")
    print(f"  Eval samples:  {len(tokenized_datasets[config['data']['eval_split']])}")
    print(f"  Batch size:    {config['training']['batch_size']}")
    print(f"  Task:          SST-2 Sentiment Classification (Binary)")

    # ========== 6. è¨­å®šå„ªåŒ–å™¨èˆ‡ Scheduler ==========
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay'],
    )

    num_training_steps = len(train_dataloader) * config['training']['num_epochs']
    num_warmup_steps = int(config['training']['warmup_ratio'] * num_training_steps)

    scheduler = get_scheduler(
        name="linear",
        optimizer=optimizer,
        num_warmup_steps=num_warmup_steps,
        num_training_steps=num_training_steps,
    )

    print(f"\nğŸ“ Training Config:")
    print(f"  Epochs:          {config['training']['num_epochs']}")
    print(f"  Learning rate:   {config['training']['learning_rate']}")
    print(f"  Weight decay:    {config['training']['weight_decay']}")
    print(f"  Gradient clip:   {config['training']['gradient_clip']}")
    print(f"  Warmup ratio:    {config['training']['warmup_ratio']}")
    print(f"  Warmup steps:    {num_warmup_steps}")
    print(f"  Total steps:     {num_training_steps}")

    # ========== 7. è¨“ç·´å¾ªç’° ==========
    print("\n" + "="*60)
    print("ğŸš€ Starting Training...")
    print("="*60 + "\n")

    train_losses = []
    eval_losses = []
    train_accuracies = []
    eval_accuracies = []
    best_eval_accuracy = 0.0

    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_dir = Path(config['output']['dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    for epoch in range(config['training']['num_epochs']):
        print(f"\nğŸ“ Epoch {epoch + 1}/{config['training']['num_epochs']}")
        print("-" * 60)

        # è¨“ç·´
        train_loss, train_acc = train_one_epoch(
            model=model,
            dataloader=train_dataloader,
            optimizer=optimizer,
            scheduler=scheduler,
            device=device,
            gradient_clip=config['training']['gradient_clip'],
        )
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)

        # è©•ä¼°
        eval_loss, eval_acc = evaluate(
            model=model,
            dataloader=eval_dataloader,
            device=device,
        )
        eval_losses.append(eval_loss)
        eval_accuracies.append(eval_acc)

        # é¡¯ç¤ºçµæœ
        print(f"\n  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
        print(f"  Eval Loss:  {eval_loss:.4f} | Eval Acc:  {eval_acc:.4f}")

        # å„²å­˜æœ€ä½³æ¨¡å‹
        if eval_acc > best_eval_accuracy:
            best_eval_accuracy = eval_acc
            best_checkpoint_path = output_dir / "best_lora_model.pt"

            torch.save({
                'epoch': epoch + 1,
                'lora_state_dict': get_lora_state_dict(model),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'eval_loss': eval_loss,
                'train_acc': train_acc,
                'eval_acc': eval_acc,
                'config': config,
            }, best_checkpoint_path)

            print(f"  ğŸ’¾ Best model saved! (eval_acc: {eval_acc:.4f})")

    # ========== 8. å„²å­˜æœ€çµ‚çµæœ ==========
    print("\n" + "="*60)
    print("ğŸ’¾ Saving Results...")
    print("="*60)

    # 8.1 å„²å­˜æœ€çµ‚ LoRA æ¬Šé‡
    final_checkpoint_path = output_dir / "final_lora_model.pt"
    torch.save({
        'epoch': config['training']['num_epochs'],
        'lora_state_dict': get_lora_state_dict(model),
        'optimizer_state_dict': optimizer.state_dict(),
        'config': config,
        'train_losses': train_losses,
        'eval_losses': eval_losses,
        'train_accuracies': train_accuracies,
        'eval_accuracies': eval_accuracies,
    }, final_checkpoint_path)
    print(f"âœ… Final LoRA weights saved to: {final_checkpoint_path}")

    # 8.2 å„²å­˜åªæœ‰ LoRA åƒæ•¸çš„ç²¾ç°¡ç‰ˆæœ¬
    lora_only_path = output_dir / "lora_adapter.pt"
    torch.save({
        'lora_state_dict': get_lora_state_dict(model),
        'config': {
            'rank': config['lora']['rank'],
            'alpha': config['lora']['alpha'],
            'target_modules': config['lora']['target_modules'],
            'model_name': config['model_name'],
            'num_labels': config['num_labels'],
        }
    }, lora_only_path)
    print(f"âœ… LoRA adapter saved to: {lora_only_path}")

    # 8.3 ç¹ªè£½è¨“ç·´æ›²ç·š
    plot_training_curves(
        train_losses,
        eval_losses,
        train_accuracies,
        eval_accuracies,
        save_path=str(output_dir / "training_curves.png")
    )

    # 8.4 å„²å­˜è¨“ç·´æ—¥èªŒ
    log_path = output_dir / "training_log.txt"
    with open(log_path, 'w', encoding='utf-8') as f:
        f.write("LoRA Training Log - SST-2 Sentiment Classification\n")
        f.write("="*60 + "\n\n")
        f.write(f"Model: {config['model_name']}\n")
        f.write(f"Dataset: {config['data']['dataset']}/{config['data']['subset']}\n")
        f.write(f"Task: Binary Sentiment Classification\n\n")
        f.write(f"LoRA Config:\n")
        f.write(f"  rank: {config['lora']['rank']}\n")
        f.write(f"  alpha: {config['lora']['alpha']}\n")
        f.write(f"  dropout: {config['lora']['dropout']}\n\n")
        f.write(f"Parameter Statistics:\n")
        f.write(f"  Total: {param_stats['total']:,}\n")
        f.write(f"  Trainable: {param_stats['trainable']:,}\n")
        f.write(f"  Percentage: {param_stats['percentage']:.4f}%\n\n")
        f.write(f"Training Results:\n")
        for i, (tl, el, ta, ea) in enumerate(zip(train_losses, eval_losses, train_accuracies, eval_accuracies), 1):
            f.write(f"  Epoch {i}:\n")
            f.write(f"    Train Loss: {tl:.4f}, Train Acc: {ta:.4f}\n")
            f.write(f"    Eval Loss:  {el:.4f}, Eval Acc:  {ea:.4f}\n")
        f.write(f"\nBest Eval Accuracy: {best_eval_accuracy:.4f}\n")

    print(f"âœ… Training log saved to: {log_path}")

    # ========== 9. ä¸Šå‚³åˆ° Hugging Face Hub (å¯é¸) ==========
    if args.push_to_hub:
        print("\n" + "="*60)
        print("ğŸ“¤ Uploading to Hugging Face Hub...")
        print("="*60)

        try:
            # åˆä½µ LoRA æ¬Šé‡åˆ°åŸºç¤æ¨¡å‹
            print("Merging LoRA weights...")
            merge_lora_weights(model)

            # æº–å‚™ model card
            model_card = f"""---
language: en
license: mit
tags:
- text-classification
- sentiment-analysis
- lora
- bert
- sst2
datasets:
- glue
metrics:
- accuracy
---

# BERT-base LoRA Fine-tuned on SST-2

LoRA fine-tuned BERT model for binary sentiment classification on the SST-2 dataset.

## Model Details

- **Base Model**: `{config['model_name']}`
- **Task**: Binary Sentiment Classification (SST-2)
- **Method**: LoRA (Low-Rank Adaptation)
- **Language**: English

## Training Configuration

- **LoRA rank**: {config['lora']['rank']}
- **LoRA alpha**: {config['lora']['alpha']}
- **LoRA dropout**: {config['lora']['dropout']}
- **Target modules**: {', '.join(config['lora']['target_modules'])}
- **Training epochs**: {config['training']['num_epochs']}
- **Learning rate**: {config['training']['learning_rate']}
- **Batch size**: {config['training']['batch_size']}

## Performance

- **Best Validation Accuracy**: {best_eval_accuracy:.4f}
- **Trainable Parameters**: {param_stats['trainable']:,} ({param_stats['percentage']:.4f}% of total)
- **Total Parameters**: {param_stats['total']:,}

## Training Results

| Epoch | Train Loss | Train Acc | Eval Loss | Eval Acc |
|-------|-----------|-----------|-----------|----------|
"""
            for i, (tl, el, ta, ea) in enumerate(zip(train_losses, eval_losses, train_accuracies, eval_accuracies), 1):
                model_card += f"| {i} | {tl:.4f} | {ta:.4f} | {el:.4f} | {ea:.4f} |\n"

            model_card += f"""
## Usage

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer
import torch

# Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained("{args.hub_model_id}")
tokenizer = AutoTokenizer.from_pretrained("{args.hub_model_id}")

# Prepare input
text = "This movie is fantastic!"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=128)

# Predict
model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    sentiment = "Positive" if predictions[0][1] > 0.5 else "Negative"
    confidence = predictions[0][1].item() if predictions[0][1] > 0.5 else predictions[0][0].item()

print(f"Sentiment: {{sentiment}} (Confidence: {{confidence:.2%}})")
```

## Dataset

This model was trained on the [SST-2 (Stanford Sentiment Treebank)](https://huggingface.co/datasets/glue) dataset:
- **Training samples**: 67,349
- **Validation samples**: 872
- **Task**: Binary sentiment classification (positive/negative)

## Training Procedure

The model was fine-tuned using LoRA (Low-Rank Adaptation), which only updates a small number of parameters:
- Only {param_stats['percentage']:.4f}% of the model's parameters were trained
- The base BERT weights remain frozen
- LoRA adapters were applied to: {', '.join(config['lora']['target_modules'])}

## Limitations

- This model is specifically trained for sentiment classification on movie reviews
- Performance may vary on other domains or text types
- The model is based on BERT-base and has the same limitations (max sequence length: 512 tokens)

## Citation

If you use this model, please cite:

```bibtex
@misc{{bert-lora-sst2,
  author = {{LLM Tuning Lab}},
  title = {{BERT-base LoRA Fine-tuned on SST-2}},
  year = {{2025}},
  publisher = {{Hugging Face}},
  howpublished = {{\\url{{https://huggingface.co/{args.hub_model_id}}}}},
}}
```

## References

- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
"""

            # å„²å­˜ model card
            readme_path = output_dir / "README.md"
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(model_card)
            print(f"âœ… Model card created: {readme_path}")

            # ä¸Šå‚³æ¨¡å‹
            print(f"Uploading model to: {args.hub_model_id}")
            model.push_to_hub(
                repo_id=args.hub_model_id,
                commit_message=f"Upload LoRA fine-tuned BERT (Acc: {best_eval_accuracy:.4f})",
                private=args.hub_private
            )

            # ä¸Šå‚³ tokenizer
            print(f"Uploading tokenizer...")
            tokenizer.push_to_hub(
                repo_id=args.hub_model_id,
                commit_message="Upload tokenizer",
                private=args.hub_private
            )

            # ä¸Šå‚³è¨“ç·´æ›²ç·šåœ–
            print(f"Uploading training curves...")
            from huggingface_hub import upload_file
            upload_file(
                path_or_fileobj=str(output_dir / "training_curves.png"),
                path_in_repo="training_curves.png",
                repo_id=args.hub_model_id,
                repo_type="model",
                commit_message="Upload training curves"
            )

            print(f"\nâœ… Model successfully uploaded!")
            print(f"ğŸŒ View at: https://huggingface.co/{args.hub_model_id}")

        except Exception as e:
            print(f"\nâŒ Upload failed: {e}")
            print("Please check:")
            print("  1. Hugging Face token is set (run: huggingface-cli login)")
            print("  2. Model ID format is correct (username/model-name)")
            print("  3. You have write permissions to the repository")

    # ========== 10. å®Œæˆ ==========
    print("\n" + "="*60)
    print("ğŸ‰ Training Completed!")
    print("="*60)
    print(f"\nğŸ“ Output Directory: {output_dir.absolute()}")
    print(f"ğŸ“ˆ Best Eval Accuracy: {best_eval_accuracy:.4f}")
    print(f"ğŸ“‰ Final Train Loss: {train_losses[-1]:.4f}")
    print(f"ğŸ“‰ Final Eval Loss: {eval_losses[-1]:.4f}")
    print(f"ğŸ¯ Final Train Accuracy: {train_accuracies[-1]:.4f}")
    print(f"ğŸ¯ Final Eval Accuracy: {eval_accuracies[-1]:.4f}")
    print("\nâœ¨ Files created:")
    print(f"  - {final_checkpoint_path.name}")
    print(f"  - {lora_only_path.name}")
    print(f"  - best_lora_model.pt")
    print(f"  - training_curves.png")
    print(f"  - training_log.txt")
    print("\nğŸ’¡ Next steps:")
    print(f"  - Run inference: python inference_example.py")
    if args.push_to_hub:
        print(f"  - View model: https://huggingface.co/{args.hub_model_id}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Train BERT with LoRA on SST-2 Sentiment Classification"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Path to config file (default: config.yaml)"
    )
    parser.add_argument(
        "--rank",
        type=int,
        default=None,
        help="LoRA rank (overrides config)"
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=None,
        help="LoRA alpha (overrides config)"
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=None,
        help="Number of epochs (overrides config)"
    )
    parser.add_argument(
        "--push_to_hub",
        action="store_true",
        help="Push model to Hugging Face Hub after training"
    )
    parser.add_argument(
        "--hub_model_id",
        type=str,
        default="your-username/bert-lora-sst2",
        help="Hugging Face Hub model ID (e.g., 'username/model-name')"
    )
    parser.add_argument(
        "--hub_private",
        action="store_true",
        help="Make the uploaded model private on Hugging Face Hub"
    )

    args = parser.parse_args()
    main(args)
