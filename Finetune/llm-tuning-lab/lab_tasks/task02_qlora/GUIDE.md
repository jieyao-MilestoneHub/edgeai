# Task 02 è©³ç´°æ•™å­¸æŒ‡å¼•ï¼šQLoRA å¯¦æˆ°

> ä½¿ç”¨ 4-bit é‡åŒ–æŠ€è¡“ï¼Œç”¨æ¶ˆè²»ç´š GPU è¨“ç·´ 7B/13B æ¨¡å‹

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ä»»å‹™å¾Œï¼Œä½ å°‡èƒ½å¤ ï¼š
- âœ… ç†è§£é‡åŒ–çš„åŸºæœ¬åŸç†ï¼ˆINT8, INT4, NF4ï¼‰
- âœ… ä½¿ç”¨ bitsandbytes é€²è¡Œ 4-bit é‡åŒ–
- âœ… å¯¦ä½œ QLoRA è¨“ç·´æµç¨‹
- âœ… å°æ¯” LoRA vs QLoRA çš„è¨˜æ†¶é«”èˆ‡ç²¾åº¦å·®ç•°

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šé‡åŒ–åŸç†

### 1.1 ç‚ºä»€éº¼éœ€è¦é‡åŒ–ï¼Ÿ

**å•é¡Œï¼šå¤§æ¨¡å‹è¨“ç·´çš„è¨˜æ†¶é«”ç“¶é ¸**

```python
# LLaMA-7B å…¨åƒæ•¸å¾®èª¿
æ¨¡å‹åƒæ•¸ï¼š7B
FP16 ç²¾åº¦ï¼š2 bytes/param
æ¨¡å‹æ¬Šé‡ï¼š7B Ã— 2 = 14GB

è¨“ç·´æ™‚é¡å¤–éœ€æ±‚ï¼š
- Optimizer states (Adam): 14GB Ã— 2 = 28GB
- Gradients: 14GB
- Activations: ~10GB
ç¸½è¨ˆï¼š~66GB  âŒ å–®å¼µ A100 (40GB) ç„¡æ³•è¨“ç·´
```

**LoRA çš„æ”¹å–„ï¼š**
```python
# LoRA (FP16)
å‡çµæ¬Šé‡ï¼š14GB (åªè¼‰å…¥ï¼Œä¸è¨“ç·´)
LoRA åƒæ•¸ï¼š~40MB
Optimizer: 80MB
ç¸½è¨ˆï¼š~14.2GB  âœ… å¯ä»¥è¨“ç·´ï¼Œä½†ä»éœ€å¤§è¨˜æ†¶é«”è¼‰å…¥
```

**QLoRA çš„çªç ´ï¼š**
```python
# QLoRA (4-bit é‡åŒ–)
é‡åŒ–æ¬Šé‡ï¼š7B Ã— 0.5 bytes = 3.5GB  ğŸ‰
LoRA åƒæ•¸ï¼š~40MB
Optimizer: 80MB
ç¸½è¨ˆï¼š~3.7GB  âœ…âœ… æ¶ˆè²»ç´š GPU ä¹Ÿèƒ½è¨“ç·´å¤§æ¨¡å‹ï¼
```

### 1.2 é‡åŒ–åŸºç¤

#### ä»€éº¼æ˜¯é‡åŒ–ï¼Ÿ

```
é‡åŒ– = ç”¨æ›´å°‘çš„ bits è¡¨ç¤ºæ•¸å­—

FP32 (32-bit)ï¼šç²¾åº¦æœ€é«˜ï¼Œè¨˜æ†¶é«”æœ€å¤§
    â†“ 2Ã— å£“ç¸®
FP16 (16-bit)ï¼šç²¾åº¦ç•¥é™ï¼Œè¨˜æ†¶é«”æ¸›åŠ
    â†“ 2Ã— å£“ç¸®
INT8 (8-bit)ï¼šæ•´æ•¸è¡¨ç¤ºï¼Œè¨˜æ†¶é«” 1/4
    â†“ 2Ã— å£“ç¸®
INT4 (4-bit)ï¼šæ›´æ¿€é€²ï¼Œè¨˜æ†¶é«” 1/8  â† QLoRA ä½¿ç”¨
```

#### é‡åŒ–éç¨‹

```python
# åŸå§‹ FP16 æ¬Šé‡
weight_fp16 = torch.tensor([0.5234, -1.2341, 0.8923, ...])

# é‡åŒ–åˆ° INT4ï¼ˆç¯„åœ 0-15ï¼‰
min_val = weight_fp16.min()  # -1.2341
max_val = weight_fp16.max()  #  0.8923

# ç·šæ€§æ˜ å°„åˆ° [0, 15]
scale = (max_val - min_val) / 15
weight_int4 = ((weight_fp16 - min_val) / scale).round().to(torch.uint8)
# [10, 0, 13, ...]

# åé‡åŒ–ï¼ˆè¨ˆç®—æ™‚ï¼‰
weight_dequant = weight_int4 * scale + min_val
# [0.5234, -1.2341, 0.8923, ...] ï¼ˆæœ‰å¾®å°èª¤å·®ï¼‰
```

### 1.3 NF4 (NormalFloat 4-bit)

QLoRA çš„é—œéµå‰µæ–°ï¼š**é‡å°ç¥ç¶“ç¶²çµ¡æ¬Šé‡åˆ†ä½ˆå„ªåŒ–çš„é‡åŒ–æ ¼å¼**

**è§€å¯Ÿï¼š** ç¥ç¶“ç¶²çµ¡æ¬Šé‡é€šå¸¸æœå¾æ­£æ…‹åˆ†ä½ˆï¼ˆå¤§éƒ¨åˆ†å€¼æ¥è¿‘ 0ï¼‰

```
æ™®é€š INT4ï¼šå‡å‹»åˆ†ä½ˆé‡åŒ–ç´šåˆ¥
[-15, -13, -11, ..., -1, 1, ..., 11, 13, 15]
         â†“
å•é¡Œï¼šæµªè²»äº†æ¥è¿‘ 0 çš„ç²¾åº¦

NF4ï¼šæ ¹æ“šæ­£æ…‹åˆ†ä½ˆå„ªåŒ–
æ›´å¤šç´šåˆ¥é›†ä¸­åœ¨ 0 é™„è¿‘ï¼š
[-1.0, -0.6961, -0.5250, -0.3949, -0.2844, -0.1848, -0.0911, 0,
 0.0911, 0.1848, 0.2844, 0.3949, 0.5250, 0.6961, 1.0]
         â†“
å„ªå‹¢ï¼šåœ¨åŒæ¨£ 4-bit ä¸‹ï¼Œç²¾åº¦æ›´é«˜ï¼
```

### 1.4 é›™é‡é‡åŒ– (Double Quantization)

**é€²ä¸€æ­¥å£“ç¸®ï¼šé€£é‡åŒ–å¸¸æ•¸ä¹Ÿé‡åŒ–ï¼**

```python
# ç¬¬ä¸€æ¬¡é‡åŒ–ï¼šæ¬Šé‡
weight_int4, scale_fp32, zero_point_fp32 = quantize_weights(W)

# å•é¡Œï¼šscale å’Œ zero_point ä»æ˜¯ FP32
# å°æ–¼å¤§æ¨¡å‹ï¼Œé€™äº›å¸¸æ•¸ä¹Ÿä½”ç©ºé–“

# ç¬¬äºŒæ¬¡é‡åŒ–ï¼šé‡åŒ–å¸¸æ•¸ä¹Ÿé‡åŒ–åˆ° INT8
scale_int8 = quantize_scale(scale_fp32)
zero_point_int8 = quantize_scale(zero_point_fp32)

# ç¸½è¨˜æ†¶é«”ç¯€çœï¼š
# åŸæœ¬ï¼š4-bit weights + FP32 scales
# ç¾åœ¨ï¼š4-bit weights + INT8 scales
# é¡å¤–ç¯€çœ ~0.4 GB (å° 65B æ¨¡å‹)
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šQLoRA å¯¦ä½œ

### 2.1 ä½¿ç”¨ bitsandbytes

```python
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ========== 4-bit é‡åŒ–é…ç½® ==========
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                      # å•Ÿç”¨ 4-bit é‡åŒ–
    bnb_4bit_quant_type="nf4",              # ä½¿ç”¨ NF4 æ ¼å¼
    bnb_4bit_compute_dtype=torch.bfloat16,  # è¨ˆç®—æ™‚çš„ç²¾åº¦
    bnb_4bit_use_double_quant=True,         # å•Ÿç”¨é›™é‡é‡åŒ–
)

# ========== è¼‰å…¥é‡åŒ–æ¨¡å‹ ==========
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=bnb_config,
    device_map="auto",  # è‡ªå‹•åˆ†é… GPU/CPU
    trust_remote_code=True,
)

# ========== æº–å‚™æ¨¡å‹ï¼ˆå•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ç­‰ï¼‰ ==========
model = prepare_model_for_kbit_training(model)

# ========== æ·»åŠ  LoRA ==========
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
# trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06%
```

### 2.2 å®Œæ•´è¨“ç·´è…³æœ¬

è©³è¦‹ `train_qlora.py`

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¨˜æ†¶é«”èˆ‡ç²¾åº¦å°æ¯”

### 3.1 è¨˜æ†¶é«”ä½¿ç”¨å°æ¯”

```python
import torch
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model

def measure_memory(model_name, use_qlora=False):
    """æ¸¬é‡æ¨¡å‹è¨˜æ†¶é«”ä½¿ç”¨"""
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()

    if use_qlora:
        # QLoRA è¼‰å…¥
        bnb_config = BitsAndBytesConfig(load_in_4bit=True, ...)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
        )
    else:
        # æ­£å¸¸è¼‰å…¥
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
        )

    # æ·»åŠ  LoRA
    model = get_peft_model(model, LoraConfig(...))

    # æ¸¬é‡
    memory_used = torch.cuda.max_memory_allocated() / 1e9
    return memory_used

# å°æ¯”
fp16_memory = measure_memory("meta-llama/Llama-2-7b-hf", use_qlora=False)
qlora_memory = measure_memory("meta-llama/Llama-2-7b-hf", use_qlora=True)

print(f"FP16 LoRA:  {fp16_memory:.2f} GB")
print(f"QLoRA:      {qlora_memory:.2f} GB")
print(f"ç¯€çœ:       {(1 - qlora_memory/fp16_memory)*100:.1f}%")

# é æœŸè¼¸å‡ºï¼š
# FP16 LoRA:  14.5 GB
# QLoRA:      3.8 GB
# ç¯€çœ:       73.8%
```

### 3.2 ç²¾åº¦å°æ¯”å¯¦é©—

```python
# åœ¨ç›¸åŒæ•¸æ“šä¸Šè¨“ç·´å…©å€‹æ¨¡å‹
# 1. LoRA (FP16)
# 2. QLoRA (4-bit)

# æ¯”è¼ƒ metrics
results = {
    "FP16 LoRA": {
        "final_loss": 1.234,
        "perplexity": 3.45,
        "training_time": "45 min",
    },
    "QLoRA": {
        "final_loss": 1.256,  # ç•¥é«˜ï¼Œä½†å¯æ¥å—
        "perplexity": 3.52,
        "training_time": "52 min",  # ç•¥æ…¢ï¼ˆé‡åŒ–/åé‡åŒ–é–‹éŠ·ï¼‰
    },
}

# ç²¾åº¦å·®ç•°ï¼šé€šå¸¸ <2%
```

---

## ç¬¬å››éƒ¨åˆ†ï¼šé€²éšæŠ€å·§

### 4.1 åˆ†é å„ªåŒ–å™¨ (Paged Optimizer)

QLoRA çš„å¦ä¸€å€‹å‰µæ–°ï¼šè™•ç†è¨˜æ†¶é«”å³°å€¼

```python
from transformers import Trainer, TrainingArguments

args = TrainingArguments(
    output_dir="./output",
    optim="paged_adamw_32bit",  # ä½¿ç”¨åˆ†é å„ªåŒ–å™¨
    # ç•¶ GPU è¨˜æ†¶é«”ä¸è¶³æ™‚ï¼Œè‡ªå‹•è½‰ç§»åˆ° CPU
)
```

### 4.2 Gradient Checkpointing

```python
model.gradient_checkpointing_enable()

# åŸç†ï¼š
# ä¸å„²å­˜æ‰€æœ‰ä¸­é–“ activations
# éœ€è¦æ™‚é‡æ–°è¨ˆç®—
# è¨˜æ†¶é«” â†“ 50%ï¼Œé€Ÿåº¦ â†“ 20%
```

### 4.3 æœ€ä½³å¯¦è¸

**æ¨è–¦é…ç½®ï¼ˆ7B æ¨¡å‹ï¼‰ï¼š**
```python
# é‡åŒ–é…ç½®
load_in_4bit=True
bnb_4bit_quant_type="nf4"
bnb_4bit_compute_dtype=torch.bfloat16  # å¦‚æœ GPU æ”¯æ´
bnb_4bit_use_double_quant=True

# LoRA é…ç½®
r=16  # æˆ– 64ï¼ˆè¦–ä»»å‹™è¤‡é›œåº¦ï¼‰
lora_alpha=32  # = 2 Ã— r
target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# è¨“ç·´é…ç½®
per_device_train_batch_size=4
gradient_accumulation_steps=4  # æœ‰æ•ˆ batch size = 16
gradient_checkpointing=True
optim="paged_adamw_32bit"
```

---

## ç¬¬äº”éƒ¨åˆ†ï¼šå¸¸è¦‹å•é¡Œ

### Q1: é‡åŒ–æœƒæå¤±å¤šå°‘ç²¾åº¦ï¼Ÿ

A: å¯¦é©—é¡¯ç¤ºï¼ŒNF4 + QLoRA çš„ç²¾åº¦æå¤±é€šå¸¸ <2%ï¼Œå®Œå…¨å¯æ¥å—ã€‚

### Q2: è¨“ç·´é€Ÿåº¦æœƒè®Šæ…¢å—ï¼Ÿ

A: æ˜¯çš„ï¼Œç´„æ…¢ 20-30%ï¼ˆé‡åŒ–/åé‡åŒ–é–‹éŠ·ï¼‰ï¼Œä½†è¨˜æ†¶é«”ç¯€çœæ˜¯å€¼å¾—çš„ã€‚

### Q3: æ¨è«–æ™‚ä¹Ÿéœ€è¦é‡åŒ–å—ï¼Ÿ

A: å¯é¸ã€‚è¨“ç·´å¾Œå¯ä»¥ï¼š
- æ–¹æ¡ˆ Aï¼šåˆä½µ LoRA æ¬Šé‡ï¼Œåé‡åŒ–ç‚º FP16ï¼ˆæ¨è«–æ›´å¿«ï¼‰
- æ–¹æ¡ˆ Bï¼šä¿æŒ 4-bit + LoRAï¼ˆç¯€çœé¡¯å­˜ï¼Œå¯éƒ¨ç½²æ›´å¤šæ¨¡å‹ï¼‰

### Q4: æ‰€æœ‰æ¨¡å‹éƒ½èƒ½ç”¨ QLoRA å—ï¼Ÿ

A: å¤§éƒ¨åˆ† Transformer æ¶æ§‹éƒ½æ”¯æ´ã€‚æª¢æŸ¥ `bitsandbytes` ç›¸å®¹æ€§ã€‚

---

## ğŸ“ å­¸ç¿’æª¢æŸ¥æ¸…å–®

- [ ] ç†è§£é‡åŒ–çš„åŸºæœ¬åŸç†
- [ ] çŸ¥é“ NF4 èˆ‡æ™®é€š INT4 çš„å·®ç•°
- [ ] èƒ½é…ç½® BitsAndBytesConfig
- [ ] æˆåŠŸè¨“ç·´ QLoRA æ¨¡å‹
- [ ] å°æ¯”æ¸¬é‡è¨˜æ†¶é«”ä½¿ç”¨
- [ ] åˆ†æç²¾åº¦æå¤±

---

**æ­å–œå®Œæˆ Task 02ï¼ä½ ç¾åœ¨å¯ä»¥åœ¨æ¶ˆè²»ç´š GPU ä¸Šè¨“ç·´å¤§æ¨¡å‹äº†ï¼ğŸ‰**

[â† è¿”å› Task 01](../task01_lora_basic/) | [ä¸‹ä¸€ç¯‡ï¼šTask 03 SDK èˆ‡ API â†’](../task03_sdk_api/)
