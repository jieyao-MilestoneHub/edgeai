# Task 02: QLoRA å¯¦ä½œ - Qwen2.5-3B èªè¨€æ¨¡å‹å¾®èª¿

> ä¸€ä»½å¹«åŠ©ä½ åœ¨æ¶ˆè²»ç´š GPU ä¸Šå®Œæˆå¤§æ¨¡å‹å¾®èª¿çš„å¯¦ç”¨ç­†è¨˜

## ğŸ¯ æˆ‘å€‘è¦åšä»€éº¼ï¼Ÿ

ä½¿ç”¨ **QLoRA (Quantized Low-Rank Adaptation)** å¾®èª¿ Qwen2.5-3B æ¨¡å‹ï¼Œåœ¨ **Wikitext-2** èªè¨€æ¨¡å‹ä»»å‹™ä¸Šè¨“ç·´ã€‚

**ç‚ºä»€éº¼ç”¨ QLoRAï¼Ÿ**
- åœ¨ **GTX 4060 (8GB)** ä¸Šè¨“ç·´ 3B åƒæ•¸æ¨¡å‹ âœ…
- è¨˜æ†¶é«”éœ€æ±‚é™ä½ **75%**ï¼ˆ14GB â†’ 3.5GBï¼‰
- åªè¨“ç·´ **0.07%** çš„åƒæ•¸ï¼ˆ2M / 3Bï¼‰
- LoRA adapter åªæœ‰ **8MB**ï¼ˆå®Œæ•´æ¨¡å‹ 6GBï¼‰
- è¨“ç·´é€Ÿåº¦æ¯”å…¨åƒæ•¸å¿« **5 å€**

**ä»»å‹™èªªæ˜ï¼š**
- æ¨¡å‹ï¼šQwen/Qwen2.5-3B-Instructï¼ˆ30å„„åƒæ•¸ï¼‰
- æ•¸æ“šé›†ï¼šWikitext-2ï¼ˆèªè¨€æ¨¡å‹åŸºæº–ï¼‰
- ä»»å‹™ï¼šå› æœèªè¨€æ¨¡å‹ï¼ˆCausal Language Modelingï¼‰
- é‡åŒ–ï¼š4-bit NF4 + Double Quantization

---

## ğŸ“ æ ¸å¿ƒæª”æ¡ˆèªªæ˜

### **quantization_utils.py** - QLoRA æ ¸å¿ƒå·¥å…·

é€™æ˜¯æ•´å€‹ task æœ€é‡è¦çš„æª”æ¡ˆï¼Œå¯¦ä½œäº† QLoRA çš„é‡åŒ–èˆ‡ LoRA é…ç½®ã€‚

#### ğŸ”‘ æ ¸å¿ƒå…ƒä»¶

**1. `create_bnb_config()` - é‡åŒ–é…ç½®**
```python
# å»ºç«‹ 4-bit NF4 é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,                  # å•Ÿç”¨ 4-bit
    bnb_4bit_quant_type="nf4",          # NF4 æ ¼å¼
    bnb_4bit_compute_dtype=torch.bfloat16,  # BF16 è¨ˆç®—
    bnb_4bit_use_double_quant=True,     # é›™é‡é‡åŒ–
)
```

**ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆï¼Ÿ**
- **NF4 vs FP4**: NF4 é‡å°æ­£æ…‹åˆ†å¸ƒå„ªåŒ–ï¼Œç²¾åº¦æå‡ 1.8Ã—
- **BF16 vs FP16**: BF16 ç¯„åœå¤§ï¼ˆ10Â³â¸ vs 10â´ï¼‰ï¼Œè¨“ç·´æ›´ç©©å®š
- **Double Quant**: å°é‡åŒ–å¸¸æ•¸å†é‡åŒ–ï¼Œé¡å¤–ç¯€çœ 8% è¨˜æ†¶é«”

**2. `create_lora_config()` - LoRA é…ç½®**
```python
# å»ºç«‹ LoRA é…ç½®ï¼ˆAttention + MLP å±¤ï¼‰
lora_config = LoraConfig(
    r=16,                    # LoRA rank
    lora_alpha=32.0,         # ç¸®æ”¾å› å­
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",     # Attention
        "gate_proj", "up_proj", "down_proj"          # MLP
    ]
)
```

**Target Modules é¸æ“‡ï¼š**
- **Attention å±¤**ï¼ˆå¿…é¸ï¼‰: Q/K/V/O æŠ•å½±
- **MLP å±¤**ï¼ˆå»ºè­°ï¼‰: Gate/Up/Down æŠ•å½±ï¼Œæ•ˆæœæå‡ 10-20%

**3. `load_model_and_tokenizer()` - è¼‰å…¥æ¨¡å‹**

è‡ªå‹•è™•ç†ï¼š
- 4-bit é‡åŒ–è¼‰å…¥
- Tokenizer é…ç½®
- Padding token è¨­å®š

**4. `prepare_model_for_training()` - æº–å‚™è¨“ç·´**

æ•´åˆï¼š
- k-bit è¨“ç·´æº–å‚™
- Gradient Checkpointingï¼ˆç¯€çœè¨˜æ†¶é«” 30-50%ï¼‰
- LoRA å¥—ç”¨

---

### **train_qlora.py** - è¨“ç·´è…³æœ¬

å®Œæ•´çš„è¨“ç·´æµç¨‹ï¼Œå¾è³‡æ–™è¼‰å…¥åˆ°æ¨¡å‹å„²å­˜ã€‚

#### ğŸ“‹ è¨“ç·´æµç¨‹

```
1. è¼‰å…¥é…ç½® (config.yaml)
   â†“
2. è¼‰å…¥ Qwen2.5-3B + å¥—ç”¨ 4-bit é‡åŒ–
   â†“
3. å¥—ç”¨ LoRA (Attention + MLP å±¤)
   â†“
4. è¼‰å…¥ Wikitext-2 æ•¸æ“šé›†
   â†“
5. è¨“ç·´å¾ªç’° (train â†’ eval â†’ save)
   â†“
6. å„²å­˜çµæœ (adapterã€æ›²ç·šåœ–ã€æ—¥èªŒ)
```

#### ğŸ” é—œéµå‡½æ•¸

**`prepare_dataset` (ç¬¬ 80-143 è¡Œ)**
- è¼‰å…¥ Wikitext-2
- Tokenize: è½‰æˆ input_ids
- è¨­å®š labelsï¼ˆèªè¨€æ¨¡å‹ï¼šlabels = input_idsï¼‰

**`plot_training_curves` (ç¬¬ 177-241 è¡Œ)**
- ç¹ªè£½ Loss å’Œ Learning Rate æ›²ç·š
- å·¦åœ–ï¼šè¨“ç·´/é©—è­‰ Loss
- å³åœ–ï¼šLearning Rate è®ŠåŒ–

**`main` (ç¬¬ 248-527 è¡Œ)**
- å®Œæ•´è¨“ç·´æµç¨‹
- è¨˜æ†¶é«”ç›£æ§
- çµæœå„²å­˜

---

### **inference_example.py** - æ¨è«–æ¸¬è©¦

è¨“ç·´å®Œæˆå¾Œæ¸¬è©¦æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›ã€‚

#### ğŸ® ä¸‰ç¨®æ¨¡å¼

**1. Interactive äº’å‹•æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰**
```bash
python inference_example.py --mode interactive
```
å³æ™‚è¼¸å…¥ prompt ä¸¦ç”Ÿæˆæ–‡æœ¬ï¼Œé©åˆé«”é©—æ¨¡å‹æ•ˆæœã€‚

**2. Demo ç¯„ä¾‹æ¨¡å¼**
```bash
python inference_example.py --mode demo
```
æŸ¥çœ‹é è¨­ç¯„ä¾‹çš„ç”Ÿæˆçµæœï¼Œå¿«é€Ÿé©—è­‰æ¨¡å‹ã€‚

**3. Text å–®æ¬¡ç”Ÿæˆ**
```bash
python inference_example.py --mode text --prompt "Explain quantum computing"
```
å°å–®ä¸€ prompt é€²è¡Œç”Ÿæˆï¼Œé©åˆè…³æœ¬èª¿ç”¨ã€‚

#### ğŸ’¡ è¼‰å…¥æµç¨‹

```python
# 1. è¼‰å…¥é‡åŒ–çš„åŸºç¤æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-3B-Instruct",
    quantization_config=bnb_config  # 4-bit NF4
)

# 2. è¼‰å…¥ LoRA adapter
model = PeftModel.from_pretrained(base_model, "./output_qlora_qwen_3b")

# 3. ç”Ÿæˆæ–‡æœ¬
outputs = model.generate(**inputs, max_new_tokens=128)
```

---

### **config.yaml** - é…ç½®æª”

æ‰€æœ‰è¶…åƒæ•¸éƒ½åœ¨é€™è£¡èª¿æ•´ã€‚

```yaml
# æ¨¡å‹
model:
  name: "Qwen/Qwen2.5-3B-Instruct"

# é‡åŒ–é…ç½®
quantization:
  load_in_4bit: true
  quant_type: "nf4"
  compute_dtype: "bfloat16"
  use_double_quant: true

# LoRA è¶…åƒæ•¸
lora:
  rank: 16                # ä½ç§©ç¶­åº¦
  alpha: 32.0             # ç¸®æ”¾å› å­
  dropout: 0.05           # Dropout æ¯”ä¾‹
  target_modules:         # ç›®æ¨™å±¤
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

# è¨“ç·´è¶…åƒæ•¸
training:
  num_epochs: 3
  batch_size: 1           # GTX 4060 (8GB) å»ºè­° 1
  gradient_accumulation_steps: 16
  learning_rate: 2.0e-4
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"
```

---

## ğŸ’» ç’°å¢ƒéœ€æ±‚

**ç¡¬é«”**
- GPU: **8GB VRAM ä»¥ä¸Š**ï¼ˆGTX 4060 / RTX 3060 / RTX 3070ï¼‰
- RAM: 16GB ä»¥ä¸Šï¼ˆå»ºè­° 32GBï¼‰
- ç¡¬ç¢Ÿ: 10GB å¯ç”¨ç©ºé–“
- CPU: 4 æ ¸å¿ƒä»¥ä¸Š

**è»Ÿé«”**
- Python: 3.10+
- CUDA: 11.8+ (GPU è¨“ç·´éœ€è¦)
- PyTorch: 2.0+
- bitsandbytes: 0.41.0+

**ç¶²è·¯**
- éœ€è¦ç¶²è·¯é€£ç·šä¸‹è¼‰æ¨¡å‹å’Œæ•¸æ“šé›†
- é¦–æ¬¡åŸ·è¡Œæœƒä¸‹è¼‰ Qwen2.5-3B (~6GB) å’Œ Wikitext-2 (~4MB)

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### æ­¥é©Ÿ 1: å®‰è£ä¾è³´

```bash
# å…ˆå®‰è£ PyTorch with CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å†å®‰è£å…¶ä»–ä¾è³´
pip install -r requirements.txt
```

### æ­¥é©Ÿ 2: é©—è­‰å®‰è£

```bash
# æª¢æŸ¥ CUDA
python -c "import torch; print(torch.cuda.is_available())"

# æª¢æŸ¥ bitsandbytes
python -c "import bitsandbytes; print('BitsAndBytes OK')"
```

### æ­¥é©Ÿ 3: é–‹å§‹è¨“ç·´

```bash
python train_qlora.py
```

**è¨“ç·´æ™‚é–“ï¼š**
- GTX 4060 (8GB): ç´„ 1-2 å°æ™‚ï¼ˆ3 epochsï¼‰
- RTX 3090 (24GB): ç´„ 30-45 åˆ†é˜
- A100 (40GB): ç´„ 15-20 åˆ†é˜

### æ­¥é©Ÿ 4: æ¸¬è©¦æ¨¡å‹

```bash
# äº’å‹•æ¨¡å¼ï¼ˆæ¨è–¦ï¼‰
python inference_example.py --mode interactive

# ç¯„ä¾‹æ¨¡å¼
python inference_example.py --mode demo

# å–®æ¬¡ç”Ÿæˆ
python inference_example.py --mode text --prompt "Explain AI in simple terms"
```

---

## ğŸ“Š ç†è§£è¨“ç·´éç¨‹

### è¨“ç·´æ™‚ä½ æœƒçœ‹åˆ°é€™äº›è¼¸å‡º

```
ğŸ“Š Trainable Parameters:
trainable params: 2,097,152 || all params: 3,002,097,152 || trainable%: 0.0698
```

**é€™ä»£è¡¨ä»€éº¼ï¼Ÿ**
- Qwen2.5-3B æœ‰ 30 å„„å€‹åƒæ•¸ï¼Œä½†æˆ‘å€‘åªè¨“ç·´ 210 è¬å€‹ï¼ˆLoRAï¼‰
- è¨“ç·´åƒæ•¸ä¸åˆ° **0.07%**ï¼Œè¨˜æ†¶é«”ç¯€çœ **75%**

```
ğŸ–¥ï¸  Device Information:
   Device: cuda
   GPU: NVIDIA GeForce GTX 4060
   Total VRAM: 8.00 GB

ğŸ’¾ GPU Memory: Allocated=1.45 GB | Reserved=2.00 GB | Free=6.00 GB
```

**è¨˜æ†¶é«”åˆ†é…ï¼š**
- é‡åŒ–æ¨¡å‹ï¼š1.5 GBï¼ˆ14GB â†’ 1.5GBï¼Œç¯€çœ 89%ï¼‰
- LoRA åƒæ•¸ï¼š50 MB
- Optimizer statesï¼š100 MBï¼ˆ8-bit pagedï¼‰
- Activationsï¼š1 GBï¼ˆwith gradient checkpointingï¼‰
- **ç¸½è¨ˆï¼š~3 GB**ï¼ˆ8GB é¡¯å¡å®‰å…¨ï¼‰

### è¨“ç·´å®Œæˆå¾Œçš„è¼¸å‡ºæª”æ¡ˆ

```
output_qlora_qwen_3b/
â”œâ”€â”€ adapter_config.json       # LoRA é…ç½®
â”œâ”€â”€ adapter_model.safetensors # LoRA æ¬Šé‡ï¼ˆåƒ… 8MBï¼‰
â”œâ”€â”€ training_curves.png       # Loss/LR æ›²ç·šåœ–
â””â”€â”€ training_log.txt          # è©³ç´°è¨“ç·´æ—¥èªŒ
```

---

## ğŸ” é—œéµç¨‹å¼ç¢¼è§£é‡‹

### QLoRA çš„é‡åŒ–é…ç½®

**ä½ç½®**: `quantization_utils.py` ç¬¬ 33-75 è¡Œ

```python
def create_bnb_config():
    return BitsAndBytesConfig(
        load_in_4bit=True,                  # å•Ÿç”¨ 4-bit
        bnb_4bit_quant_type="nf4",          # NF4 æ ¼å¼
        bnb_4bit_compute_dtype=torch.bfloat16,  # BF16 è¨ˆç®—
        bnb_4bit_use_double_quant=True,     # é›™é‡é‡åŒ–
    )
```

**ç‚ºä»€éº¼é€™æ¨£ï¼Ÿ**
- **NF4 (4-bit NormalFloat)**: é‡å°ç¥ç¶“ç¶²çµ¡æ¬Šé‡çš„æ­£æ…‹åˆ†å¸ƒå„ªåŒ–
  * æ¬Šé‡æœå¾ N(0, ÏƒÂ²)
  * ä½¿ç”¨åˆ†ä½æ•¸é‡åŒ–ï¼Œæ¯å€‹å€é–“åŒ…å«ç›¸åŒæ•¸é‡çš„æ•¸æ“šé»
  * ç²¾åº¦æ¯”ç·šæ€§ INT4 é«˜ 1.8Ã—

- **Double Quantization**: å…©æ¬¡é‡åŒ–
  * ç¬¬ä¸€æ¬¡ï¼šæ¬Šé‡ (FP16) â†’ 4-bit NF4
  * ç¬¬äºŒæ¬¡ï¼šé‡åŒ–å¸¸æ•¸ (FP32) â†’ INT8
  * é¡å¤–ç¯€çœ 8% è¨˜æ†¶é«”

- **BF16 Compute**: è¨ˆç®—æ™‚ä½¿ç”¨ BFloat16
  * ç¯„åœå¤§ï¼ˆèˆ‡ FP32 ç›¸åŒï¼‰
  * è¨“ç·´ç©©å®šï¼ˆä¸æ˜“ NaNï¼‰
  * é€Ÿåº¦å¿«ï¼ˆç¡¬é«”æ”¯æ´ï¼‰

### LoRA çš„ç›®æ¨™æ¨¡çµ„é¸æ“‡

**ä½ç½®**: `quantization_utils.py` ç¬¬ 130-137 è¡Œ

```python
target_modules = [
    # Attention å±¤ï¼ˆæ ¸å¿ƒï¼Œå¿…é ˆåŒ…å«ï¼‰
    "q_proj", "k_proj", "v_proj", "o_proj",
    # MLP å±¤ï¼ˆå¢å¼·æ•ˆæœï¼Œå»ºè­°åŒ…å«ï¼‰
    "gate_proj", "up_proj", "down_proj"
]
```

**ç‚ºä»€éº¼é¸é€™äº›å±¤ï¼Ÿ**
- **Attention å±¤** (Q/K/V/O): Self-Attention çš„æ ¸å¿ƒï¼Œå°ä»»å‹™é©æ‡‰æœ€é‡è¦
- **MLP å±¤** (Gate/Up/Down): Feed-Forward Networkï¼Œå¢å¼·è¡¨é”èƒ½åŠ›
- å¯¦é©—é¡¯ç¤ºï¼šåŠ ä¸Š MLP å±¤æ•ˆæœæå‡ 10-20%

**åƒæ•¸é‡è¨ˆç®—**ï¼ˆQwen2.5-3B, rank=16ï¼‰ï¼š
```
å–®å±¤ Attention (4096Ã—4096):
  Q proj: 16 Ã— (4096+4096) = 131K
  K proj: 16 Ã— (4096+4096) = 131K
  V proj: 16 Ã— (4096+4096) = 131K
  O proj: 16 Ã— (4096+4096) = 131K
  å°è¨ˆ: 524K

å–®å±¤ MLP (4096â†’11008â†’4096):
  Gate proj: 16 Ã— (4096+11008) = 241K
  Up proj:   16 Ã— (4096+11008) = 241K
  Down proj: 16 Ã— (11008+4096) = 241K
  å°è¨ˆ: 723K

å…¨æ¨¡å‹ï¼ˆ28 å±¤ï¼‰:
  Attention: 524K Ã— 28 = 14.7M
  MLP: 723K Ã— 28 = 20.2M
  ç¸½è¨ˆ: ~35M åƒæ•¸ï¼ˆå®Œæ•´æ¨¡å‹çš„ 1.2%ï¼‰
```

### è¨“ç·´å¾ªç’°çš„è¨˜æ†¶é«”å„ªåŒ–

**ä½ç½®**: `train_qlora.py` ç¬¬ 308-312 è¡Œ

```python
model = prepare_model_for_training(
    model,
    lora_config,
    use_gradient_checkpointing=True  # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
)
```

**Gradient Checkpointing åŸç†**ï¼š
- ä¸å„²å­˜æ‰€æœ‰ä¸­é–“æ¿€æ´»å€¼
- åå‘å‚³æ’­æ™‚é‡æ–°è¨ˆç®—
- è¨˜æ†¶é«”ç¯€çœ 30-50%
- è¨“ç·´é€Ÿåº¦ç•¥é™ 10-20%
- æ¬Šè¡¡ï¼šè¨˜æ†¶é«” vs é€Ÿåº¦

**è¨˜æ†¶é«”å°æ¯”**ï¼ˆQwen2.5-3B, batch_size=1, seq_len=512ï¼‰ï¼š
```
ç„¡ Gradient Checkpointing:
  Activations: ~3 GB
  ç¸½è¨˜æ†¶é«”: ~5.5 GB

æœ‰ Gradient Checkpointing:
  Activations: ~1 GB
  ç¸½è¨˜æ†¶é«”: ~3.5 GB ï¼ˆç¯€çœ 36%ï¼‰
```

---

## â“ å¸¸è¦‹å•é¡Œèˆ‡èª¿æ•´

### Q: è¨“ç·´å¤ªæ…¢æ€éº¼è¾¦ï¼Ÿ

**æ–¹æ¡ˆ 1ï¼šå¢åŠ  gradient accumulation**
```yaml
# config.yaml
training:
  batch_size: 1
  gradient_accumulation_steps: 32  # å¾ 16 å¢åŠ åˆ° 32
```

**æ–¹æ¡ˆ 2ï¼šç¸®çŸ­åºåˆ—é•·åº¦**
```yaml
data:
  max_length: 256  # å¾ 512 é™åˆ° 256
```

### Q: CUDA Out of Memory

**æ–¹æ¡ˆ 1ï¼šé™ä½ batch sizeï¼ˆå·²ç¶“æ˜¯ 1 äº†ï¼‰**

**æ–¹æ¡ˆ 2ï¼šé—œé–‰ MLP å±¤çš„ LoRA**
```yaml
lora:
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    # ç§»é™¤ gate_proj, up_proj, down_proj
```

**æ–¹æ¡ˆ 3ï¼šæ¸›å° LoRA rank**
```yaml
lora:
  rank: 8  # å¾ 16 é™åˆ° 8
  alpha: 16.0  # å°æ‡‰èª¿æ•´
```

### Q: ç”Ÿæˆæ•ˆæœä¸ç†æƒ³æ€éº¼è¾¦ï¼Ÿ

**ç­–ç•¥ 1ï¼šå¢åŠ  LoRA rank**
```yaml
lora:
  rank: 32      # å¾ 16 å¢åŠ åˆ° 32
  alpha: 64.0   # å°æ‡‰èª¿æ•´ï¼ˆ= 2 Ã— rankï¼‰
```

**ç­–ç•¥ 2ï¼šè¨“ç·´æ›´ä¹…**
```bash
python train_qlora.py --num_epochs 5
```

**ç­–ç•¥ 3ï¼šèª¿æ•´ç”Ÿæˆåƒæ•¸**
```bash
python inference_example.py \
  --temperature 0.8 \    # å¢åŠ éš¨æ©Ÿæ€§
  --top_p 0.95 \         # æ“´å¤§æ¡æ¨£ç¯„åœ
  --max_new_tokens 256   # ç”Ÿæˆæ›´é•·æ–‡æœ¬
```

### Q: å¦‚ä½•æ›æˆå…¶ä»–æ¨¡å‹ï¼Ÿ

**ä¿®æ”¹ config.yaml**ï¼š
```yaml
model:
  name: "meta-llama/Llama-2-7b-hf"  # æ”¹æˆå…¶ä»–æ¨¡å‹
```

**æ³¨æ„äº‹é …**ï¼š
- ç¢ºèªæ¨¡å‹æ”¯æ´ 4-bit é‡åŒ–
- èª¿æ•´ target_modulesï¼ˆä¸åŒæ¶æ§‹å±¤åç¨±ä¸åŒï¼‰
- èª¿æ•´è¨˜æ†¶é«”é…ç½®ï¼ˆæ ¹æ“šæ¨¡å‹å¤§å°ï¼‰

### Q: å¦‚ä½•åœ¨è‡ªå·±çš„æ•¸æ“šä¸Šè¨“ç·´ï¼Ÿ

ä¿®æ”¹ `train_qlora.py` çš„ `prepare_dataset` å‡½æ•¸ï¼š

```python
def prepare_dataset(config, tokenizer):
    # è¼‰å…¥è‡ªå·±çš„æ•¸æ“š
    dataset = load_dataset("your-dataset-name")
    # æˆ–å¾æœ¬åœ°è¼‰å…¥
    # dataset = load_dataset("csv", data_files="your_data.csv")

    def tokenize_function(examples):
        # æ ¹æ“šä½ çš„æ•¸æ“šæ ¼å¼èª¿æ•´
        return tokenizer(
            examples["text"],  # æ”¹æˆä½ çš„æ–‡æœ¬æ¬„ä½åç¨±
            truncation=True,
            max_length=config['data']['max_length'],
        )

    return dataset.map(tokenize_function, batched=True)
```

---

## ğŸ§ª å¯¦é©—å»ºè­°

### åŸºæœ¬å¯¦é©—ï¼ˆç¢ºä¿ç†è§£ï¼‰

1. **å®Œæˆä¸€æ¬¡å®Œæ•´è¨“ç·´**
   - è§€å¯Ÿè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
   - ç†è§£ç‚ºä½•åªç”¨ 3GB é¡¯å­˜å°±èƒ½è¨“ç·´ 3B æ¨¡å‹

2. **ä½¿ç”¨äº’å‹•æ¨¡å¼æ¸¬è©¦**
   - è¼¸å…¥ä¸åŒé¡å‹çš„ prompt
   - è§€å¯Ÿç”Ÿæˆè³ªé‡å’Œå¤šæ¨£æ€§

3. **åˆ†æè¨“ç·´æ›²ç·š**
   - æ‰“é–‹ `output_qlora_qwen_3b/training_curves.png`
   - ç¢ºèª Loss ä¸‹é™ã€Learning Rate æ­£å¸¸è®ŠåŒ–

### é€²éšå¯¦é©—ï¼ˆæ·±å…¥ç†è§£ï¼‰

1. **æ¯”è¼ƒä¸åŒ rank çš„æ•ˆæœ**
   ```bash
   python train_qlora.py --rank 8 --alpha 16 --output_dir output_r8
   python train_qlora.py --rank 16 --alpha 32 --output_dir output_r16
   python train_qlora.py --rank 32 --alpha 64 --output_dir output_r32
   ```
   è§€å¯Ÿåƒæ•¸é‡ã€è¨“ç·´æ™‚é–“ã€ç”Ÿæˆæ•ˆæœçš„è®ŠåŒ–ã€‚

2. **å°æ¯” Attention vs Attention+MLP**

   ä¿®æ”¹ config.yamlï¼Œåªè¨“ç·´ Attention å±¤ï¼š
   ```yaml
   lora:
     target_modules:
       - "q_proj"
       - "k_proj"
       - "v_proj"
       - "o_proj"
   ```

   å°æ¯”æ•ˆæœå·®ç•°ã€‚

3. **æ¸¬è©¦ä¸åŒé‡åŒ–é…ç½®**

   ä¿®æ”¹ `quantization_utils.py`ï¼š
   ```python
   # æ¸¬è©¦ FP4 vs NF4
   bnb_4bit_quant_type="fp4"  # æ”¹æˆ fp4

   # æ¸¬è©¦é—œé–‰é›™é‡é‡åŒ–
   bnb_4bit_use_double_quant=False
   ```

   è§€å¯Ÿè¨˜æ†¶é«”å’Œç²¾åº¦è®ŠåŒ–ã€‚

---

## ğŸ“– æ¥ä¸‹ä¾†å¯ä»¥å­¸ä»€éº¼

### ç†è§£æ›´æ·±

**QLoRA åŸç†**
- ğŸ“„ è«–æ–‡ï¼š[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- ğŸ§® ç‚ºä»€éº¼ 4-bit é‡åŒ–å¹¾ä¹ç„¡ç²¾åº¦æå¤±ï¼Ÿ
- ğŸ’¡ NF4 å¦‚ä½•é‡å°æ­£æ…‹åˆ†å¸ƒå„ªåŒ–ï¼Ÿ

**é‡åŒ–æŠ€è¡“**
- ğŸ“Š Post-Training Quantization (PTQ) vs Quantization-Aware Training (QAT)
- ğŸ”¢ INT8, INT4, NF4, FP4 çš„å·®ç•°
- âš–ï¸ ç²¾åº¦èˆ‡è¨˜æ†¶é«”çš„æ¬Šè¡¡

**è¨“ç·´æŠ€å·§**
- ğŸ“ˆ Gradient Checkpointing çš„åŸç†
- ğŸ¯ Paged Optimizer å¦‚ä½•é™ä½è¨˜æ†¶é«”å³°å€¼
- âš¡ Mixed Precision Training (BF16/FP16)

### æŠ€èƒ½æ“´å±•

**ä¸‹ä¸€å€‹ Task**
- ğŸš€ **Task 03**: æ›´å¤§æ¨¡å‹ï¼ˆ7B+ï¼‰çš„å¾®èª¿
- ğŸš€ **Task 04**: æŒ‡ä»¤å¾®èª¿èˆ‡å°è©±æ¨¡å‹
- ğŸš€ **Task 05**: å¤šä»»å‹™å­¸ç¿’èˆ‡ adapter ç®¡ç†

**å¯¦éš›æ‡‰ç”¨**
- ğŸŒ éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
- ğŸ”„ æŒçºŒå­¸ç¿’èˆ‡åœ¨ç·šå¾®èª¿
- ğŸ’¾ Adapter ç‰ˆæœ¬ç®¡ç†èˆ‡åˆ‡æ›

**å·¥ç¨‹å„ªåŒ–**
- âš¡ ONNX å°å‡ºèˆ‡æ¨è«–åŠ é€Ÿ
- ğŸ”§ Flash Attention æ•´åˆ
- ğŸ“¦ æ¨¡å‹å£“ç¸®èˆ‡è’¸é¤¾

### ç¨‹å¼èƒ½åŠ›

**PyTorch é€²éš**
- ğŸ”¨ è‡ªå®šç¾© CUDA kernels
- ğŸ¨ åˆ†æ•£å¼è¨“ç·´ï¼ˆDDP/FSDPï¼‰
- ğŸ’¾ Checkpoint ç®¡ç†èˆ‡æ–·é»çºŒè¨“

**Transformers åº«**
- ğŸ“š Generation ç­–ç•¥ï¼ˆBeam Search, Samplingï¼‰
- ğŸ’¿ æ¨¡å‹é‡åŒ–èˆ‡å„ªåŒ–
- ğŸ”§ Custom model æ•´åˆ

**LLM å·¥ç¨‹**
- ğŸ“Š Evaluation metricsï¼ˆBLEU, ROUGE, Perplexityï¼‰
- ğŸ”„ è³‡æ–™ç®¡é“å„ªåŒ–
- âš–ï¸ æ¨¡å‹ç›£æ§èˆ‡ A/B æ¸¬è©¦

---

## ğŸ“š å»¶ä¼¸é–±è®€

### è«–æ–‡

- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)

### å¯¦ä½œåƒè€ƒ

- [Hugging Face PEFT åº«](https://github.com/huggingface/peft) - å®˜æ–¹ LoRA/QLoRA å¯¦ä½œ
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) - é‡åŒ–æ ¸å¿ƒåº«
- [Qwen2.5 æŠ€è¡“å ±å‘Š](https://qwenlm.github.io/blog/qwen2.5/)

### æ•™å­¸è³‡æº

- [Hugging Face Course](https://huggingface.co/course) - Transformers å®Œæ•´æ•™å­¸
- [PyTorch å®˜æ–¹æ•™å­¸](https://pytorch.org/tutorials/)
- [QLoRA å®˜æ–¹ Colab](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k)

---

## ğŸ¤ å…±åŒå­¸ç¿’

é€™æ˜¯ä¸€ä»½å…±åŒå­¸ç¿’çš„ç­†è¨˜ï¼Œæ­¡è¿ï¼š
- ğŸ› ç™¼ç¾å•é¡Œï¼Ÿæ Issue
- ğŸ’¡ æœ‰æ›´å¥½çš„è§£é‡‹ï¼Ÿæ PR
- ğŸ¤” æœ‰ç–‘å•ï¼Ÿåœ¨è¨è«–å€ç™¼å•

---

## ğŸ“„ æª”æ¡ˆçµæ§‹ç¸½è¦½

```
task02_qlora/
â”œâ”€â”€ quantization_utils.py   # âœ¨ æ ¸å¿ƒï¼šé‡åŒ–èˆ‡ LoRA å·¥å…·
â”œâ”€â”€ train_qlora.py          # ğŸš€ ä¸»è¦ï¼šå®Œæ•´è¨“ç·´æµç¨‹
â”œâ”€â”€ inference_example.py    # ğŸ¯ æ‡‰ç”¨ï¼šæ¨è«–æ¸¬è©¦ï¼ˆä¸‰ç¨®æ¨¡å¼ï¼‰
â”œâ”€â”€ config.yaml             # âš™ï¸  é…ç½®ï¼šæ‰€æœ‰è¶…åƒæ•¸
â”œâ”€â”€ requirements.txt        # ğŸ“¦ ç’°å¢ƒï¼šå¥—ä»¶æ¸…å–®
â””â”€â”€ README.md               # ğŸ“ æœ¬æª”æ¡ˆï¼šå­¸ç¿’ç­†è¨˜
```

---

<div align="center">

**æº–å‚™å¥½äº†å—ï¼Ÿé–‹å§‹ä½ çš„ç¬¬ä¸€æ¬¡ QLoRA å¾®èª¿ï¼** ğŸš€

```bash
python train_qlora.py
```

æœ‰å•é¡Œéš¨æ™‚å›ä¾†æŸ¥é€™ä»½ç­†è¨˜ ğŸ“–

</div>
