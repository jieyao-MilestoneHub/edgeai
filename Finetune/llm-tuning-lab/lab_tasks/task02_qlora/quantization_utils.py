"""
QLoRA é‡åŒ–èˆ‡ LoRA å·¥å…·å‡½æ•¸

æœ¬æ¨¡çµ„æä¾› QLoRA è¨“ç·´æ‰€éœ€çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1) BitsAndBytesConfig: 4-bit NF4 é‡åŒ–é…ç½®
2) LoRAConfig: LoRA è¶…åƒæ•¸é…ç½®
3) load_model_and_tokenizer: è¼‰å…¥é‡åŒ–æ¨¡å‹èˆ‡ tokenizer
4) prepare_model_for_training: æº–å‚™ k-bit è¨“ç·´ä¸¦å¥—ç”¨ LoRA
5) print_memory_stats: é¡¯ç¤º GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

é—œéµæŠ€è¡“ï¼š
- NF4 (4-bit NormalFloat): é‡å°æ­£æ…‹åˆ†å¸ƒå„ªåŒ–çš„é‡åŒ–æ ¼å¼
- Double Quantization: å°é‡åŒ–å¸¸æ•¸å†é‡åŒ–ï¼Œé¡å¤–ç¯€çœ 8% è¨˜æ†¶é«”
- BF16 Compute: è¨ˆç®—æ™‚ä½¿ç”¨ BFloat16ï¼Œè¨“ç·´æ›´ç©©å®š
- Gradient Checkpointing: ç¯€çœè¨˜æ†¶é«”çš„åå‘å‚³æ’­æŠ€è¡“
- Paged Optimizer: CPU-GPU åˆ†é ç®¡ç†ï¼Œé™ä½è¨˜æ†¶é«”å³°å€¼

ä½œè€…: LLM Tuning Lab
æˆæ¬Š: MIT
"""

from __future__ import annotations
from typing import Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


# ====================================================================================
# 1) é‡åŒ–é…ç½®ï¼š4-bit NF4 + Double Quantization + BF16 Compute
# ====================================================================================

def create_bnb_config(
    load_in_4bit: bool = True,
    quant_type: str = "nf4",
    compute_dtype: torch.dtype = torch.bfloat16,
    use_double_quant: bool = True,
) -> BitsAndBytesConfig:
    """
    å»ºç«‹ BitsAndBytes é‡åŒ–é…ç½®ï¼ˆ4-bit QLoRA æ¨™æº–è¨­å®šï¼‰

    Args:
        load_in_4bit: æ˜¯å¦ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆé»˜èª Trueï¼‰
        quant_type: é‡åŒ–é¡å‹ï¼Œ"nf4" æˆ– "fp4"ï¼ˆé»˜èª "nf4"ï¼Œé‡å°æ­£æ…‹åˆ†å¸ƒå„ªåŒ–ï¼‰
        compute_dtype: è¨ˆç®—æ™‚çš„æ•¸æ“šé¡å‹ï¼ˆé»˜èª torch.bfloat16ï¼Œè¨“ç·´æ›´ç©©å®šï¼‰
        use_double_quant: æ˜¯å¦å•Ÿç”¨é›™é‡é‡åŒ–ï¼ˆé»˜èª Trueï¼Œé¡å¤–ç¯€çœ 8% è¨˜æ†¶é«”ï¼‰

    Returns:
        BitsAndBytesConfig: é‡åŒ–é…ç½®å°è±¡

    é—œéµè¨­è¨ˆï¼š
        - NF4 vs FP4:
          * NF4 ä½¿ç”¨æ­£æ…‹åˆ†å¸ƒçš„åˆ†ä½æ•¸é€²è¡Œéå‡å‹»é‡åŒ–
          * æ¬Šé‡é€šå¸¸æœå¾æ­£æ…‹åˆ†å¸ƒï¼ŒNF4 ç²¾åº¦æ›´é«˜ï¼ˆç›¸åŒè¨˜æ†¶é«”ä¸‹æå‡ 1.8Ã—ï¼‰
        - BF16 vs FP16:
          * BF16 ç¯„åœå¤§ï¼ˆ10Â³â¸ vs 10â´ï¼‰ï¼Œæ¢¯åº¦ç´¯ç©ä¸æ˜“æº¢ä½
          * FP16 ç²¾åº¦ç¨é«˜ï¼Œä½†å¤§æ¨¡å‹è¨“ç·´å®¹æ˜“å‡ºç¾ NaN
        - Double Quantization:
          * ç¬¬ä¸€æ¬¡ï¼šæ¬Šé‡ â†’ 4-bit
          * ç¬¬äºŒæ¬¡ï¼šé‡åŒ–å¸¸æ•¸ (FP32) â†’ INT8
          * é¡å¤–ç¯€çœç´„ 8% è¨˜æ†¶é«”ï¼Œç²¾åº¦æå¤± < 0.1%

    ç¯„ä¾‹ï¼š
        >>> config = create_bnb_config()
        >>> model = AutoModelForCausalLM.from_pretrained(
        ...     "Qwen/Qwen2.5-3B",
        ...     quantization_config=config
        ... )
    """
    return BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,              # å•Ÿç”¨ 4-bit é‡åŒ–
        bnb_4bit_quant_type=quant_type,         # NF4 æ ¼å¼ï¼ˆæ­£æ…‹åˆ†å¸ƒå„ªåŒ–ï¼‰
        bnb_4bit_compute_dtype=compute_dtype,   # è¨ˆç®—ç”¨ BF16ï¼ˆç©©å®šæ€§ï¼‰
        bnb_4bit_use_double_quant=use_double_quant,  # é›™é‡é‡åŒ–ï¼ˆç¯€çœ 8%ï¼‰
    )


# ====================================================================================
# 2) LoRA é…ç½®ï¼šAttention + MLP å±¤
# ====================================================================================

def create_lora_config(
    r: int = 16,
    alpha: float = 32.0,
    dropout: float = 0.05,
    target_modules: Optional[list[str]] = None,
    task_type: str = "CAUSAL_LM",
) -> LoraConfig:
    """
    å»ºç«‹ LoRA é…ç½®ï¼ˆé‡å° LLaMA/Qwen ç³»åˆ—æ¶æ§‹ï¼‰

    Args:
        r: LoRA rankï¼ˆä½ç§©ç¶­åº¦ï¼Œé»˜èª 16ï¼‰
            - è¶Šå¤§èƒ½åŠ›è¶Šå¼·ï¼Œä½†åƒæ•¸é‡è¶Šå¤š
            - å»ºè­°ï¼šåˆ†é¡ä»»å‹™ 8-16ï¼Œç”Ÿæˆä»»å‹™ 16-64
        alpha: LoRA alphaï¼ˆç¸®æ”¾å› å­ï¼Œé»˜èª 32.0ï¼‰
            - æ§åˆ¶ LoRA æ›´æ–°çš„å½±éŸ¿å¼·åº¦
            - é€šå¸¸è¨­ç‚º 2Ã—rank æˆ– 1Ã—rank
        dropout: LoRA dropoutï¼ˆé»˜èª 0.05ï¼‰
            - é˜²æ­¢éæ“¬åˆ
            - å°è³‡æ–™é›†å»ºè­° 0.1ï¼Œå¤§è³‡æ–™é›† 0.05
        target_modules: è¦å¥—ç”¨ LoRA çš„å±¤ï¼ˆé»˜èª Noneï¼Œè‡ªå‹•é¸æ“‡ï¼‰
            - None: è‡ªå‹•é¸æ“‡ Attention + MLP å±¤
            - å¯æ‰‹å‹•æŒ‡å®šå±¤åç¨±åˆ—è¡¨
        task_type: ä»»å‹™é¡å‹ï¼ˆé»˜èª "CAUSAL_LM"ï¼Œå› æœèªè¨€æ¨¡å‹ï¼‰

    Returns:
        LoraConfig: LoRA é…ç½®å°è±¡

    Target Modules é¸æ“‡ç­–ç•¥ï¼š
        - Attention å±¤ï¼ˆå¿…é¸ï¼‰ï¼š
          * q_proj, k_proj, v_proj: Query/Key/Value æŠ•å½±
          * o_proj: Output æŠ•å½±
        - MLP å±¤ï¼ˆå¯é¸ï¼Œæ•ˆæœæå‡ 10-20%ï¼‰ï¼š
          * gate_proj: Gate æŠ•å½±ï¼ˆLLaMA/Qwen æ¶æ§‹ï¼‰
          * up_proj: Up æŠ•å½±
          * down_proj: Down æŠ•å½±

    åƒæ•¸é‡è¨ˆç®—ï¼š
        - å–®å±¤ LoRA åƒæ•¸é‡ = r Ã— (d_in + d_out)
        - ä¾‹ï¼šd=4096, r=16
          * å–®å€‹ Attention æŠ•å½±ï¼š16 Ã— (4096+4096) = 131K
          * å››å€‹æŠ•å½± (Q,K,V,O)ï¼š131K Ã— 4 = 524K
          * ä¸‰å€‹ MLP æŠ•å½±ï¼šæ ¹æ“šéš±è—å±¤ç¶­åº¦è¨ˆç®—

    ç¯„ä¾‹ï¼š
        >>> config = create_lora_config(r=16, alpha=32.0)
        >>> model = get_peft_model(model, config)
    """
    # é»˜èªä½¿ç”¨ LLaMA/Qwen æ¶æ§‹çš„æ¨™æº–å±¤
    if target_modules is None:
        target_modules = [
            # Attention å±¤ï¼ˆæ ¸å¿ƒï¼Œå¿…é ˆåŒ…å«ï¼‰
            "q_proj", "k_proj", "v_proj", "o_proj",
            # MLP å±¤ï¼ˆå¢å¼·æ•ˆæœï¼Œå»ºè­°åŒ…å«ï¼‰
            "gate_proj", "up_proj", "down_proj"
        ]

    return LoraConfig(
        r=r,                            # LoRA rankï¼ˆä½ç§©ç¶­åº¦ï¼‰
        lora_alpha=alpha,               # ç¸®æ”¾å› å­ Î±
        lora_dropout=dropout,           # Dropout æ¯”ä¾‹
        target_modules=target_modules,  # ç›®æ¨™å±¤
        bias="none",                    # ä¸è¨“ç·´ biasï¼ˆæ¨™æº–åšæ³•ï¼‰
        task_type=task_type,            # ä»»å‹™é¡å‹
    )


# ====================================================================================
# 3) è¼‰å…¥é‡åŒ–æ¨¡å‹èˆ‡ Tokenizer
# ====================================================================================

def load_model_and_tokenizer(
    model_name: str,
    quantization_config: Optional[BitsAndBytesConfig] = None,
    device_map: str = "auto",
    trust_remote_code: bool = True,
) -> tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    è¼‰å…¥é‡åŒ–æ¨¡å‹èˆ‡ tokenizer

    Args:
        model_name: æ¨¡å‹åç¨±ï¼ˆHugging Face model IDï¼‰
            - ä¾‹ï¼š\"Qwen/Qwen2.5-3B-Instruct\"
        quantization_config: é‡åŒ–é…ç½®ï¼ˆé»˜èª Noneï¼Œè‡ªå‹•å»ºç«‹ 4-bit NF4 é…ç½®ï¼‰
        device_map: è¨­å‚™æ˜ å°„ç­–ç•¥ï¼ˆé»˜èª "auto"ï¼Œè‡ªå‹•åˆ†é…ï¼‰
        trust_remote_code: æ˜¯å¦ä¿¡ä»»é ç«¯ç¨‹å¼ç¢¼ï¼ˆQwen ç­‰æ¨¡å‹éœ€è¦ï¼‰

    Returns:
        (model, tokenizer): é‡åŒ–æ¨¡å‹èˆ‡ tokenizer

    è¨˜æ†¶é«”ç¯€çœç¯„ä¾‹ï¼ˆQwen2.5-3Bï¼‰ï¼š
        - FP16: 6 GB
        - 4-bit NF4: 1.5 GBï¼ˆç¯€çœ 75%ï¼‰
        - 4-bit NF4 + Double Quant: 1.4 GBï¼ˆç¯€çœ 77%ï¼‰

    æ³¨æ„äº‹é …ï¼š
        - é¦–æ¬¡åŸ·è¡Œæœƒä¸‹è¼‰æ¨¡å‹ï¼ˆç´„ 1.5 GB for 3B modelï¼‰
        - éœ€è¦ç¶²è·¯é€£ç·š
        - æ¨¡å‹æœƒå¿«å–åˆ° ~/.cache/huggingface/

    ç¯„ä¾‹ï¼š
        >>> model, tokenizer = load_model_and_tokenizer(\"Qwen/Qwen2.5-3B\")
        >>> print(f\"Model loaded: {model.num_parameters()/1e9:.2f}B parameters\")
    """
    # å¦‚æœæœªæä¾›é‡åŒ–é…ç½®ï¼Œä½¿ç”¨é»˜èª 4-bit NF4
    if quantization_config is None:
        quantization_config = create_bnb_config()

    print(f"ğŸ“¥ Loading model: {model_name}")
    print(f"   Quantization: 4-bit NF4 + Double Quant + BF16 Compute")

    # è¼‰å…¥é‡åŒ–æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=quantization_config,
        device_map=device_map,              # è‡ªå‹•åˆ†é…åˆ° GPU
        trust_remote_code=trust_remote_code,  # Qwen ç­‰æ¨¡å‹éœ€è¦
        torch_dtype=torch.bfloat16,         # éé‡åŒ–éƒ¨åˆ†ä½¿ç”¨ BF16
    )

    # è¼‰å…¥ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=trust_remote_code
    )

    # è¨­å®š padding ç›¸é—œåƒæ•¸ï¼ˆé‡è¦ï¼ï¼‰
    tokenizer.padding_side = "right"  # å³å´ paddingï¼ˆæ¨™æº–åšæ³•ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # ä½¿ç”¨ EOS ä½œç‚º PAD

    print(f"âœ… Model loaded successfully!")

    return model, tokenizer


# ====================================================================================
# 4) æº–å‚™ k-bit è¨“ç·´ä¸¦å¥—ç”¨ LoRA
# ====================================================================================

def prepare_model_for_training(
    model: AutoModelForCausalLM,
    lora_config: LoraConfig,
    use_gradient_checkpointing: bool = True,
) -> AutoModelForCausalLM:
    """
    æº–å‚™é‡åŒ–æ¨¡å‹é€²è¡Œè¨“ç·´ä¸¦å¥—ç”¨ LoRA

    Args:
        model: é‡åŒ–å¾Œçš„æ¨¡å‹
        lora_config: LoRA é…ç½®
        use_gradient_checkpointing: æ˜¯å¦å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼ˆé»˜èª Trueï¼Œç¯€çœè¨˜æ†¶é«”ï¼‰

    Returns:
        å¥—ç”¨ LoRA å¾Œçš„ PEFT æ¨¡å‹

    è™•ç†æµç¨‹ï¼š
        1. prepare_model_for_kbit_training: æº–å‚™é‡åŒ–æ¨¡å‹è¨“ç·´
           - å‡çµé‡åŒ–æ¬Šé‡
           - è¨­å®šè¼¸å…¥ requires_grad
           - å•Ÿç”¨æ¢¯åº¦ç´¯ç©
        2. Gradient Checkpointing: å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»
           - ç¯€çœè¨˜æ†¶é«”ç´„ 30-50%
           - è¨“ç·´é€Ÿåº¦ç•¥é™ 10-20%
           - æ¬Šè¡¡ï¼šè¨˜æ†¶é«” vs é€Ÿåº¦
        3. get_peft_model: å¥—ç”¨ LoRA çµæ§‹
           - æ’å…¥ LoRA å±¤åˆ°ç›®æ¨™æ¨¡çµ„
           - åªæœ‰ LoRA åƒæ•¸å¯è¨“ç·´

    è¨˜æ†¶é«”ç¯€çœï¼ˆQwen2.5-3B, batch_size=1, seq_len=512ï¼‰ï¼š
        - ç„¡ Checkpointing: ~6 GB
        - æœ‰ Checkpointing: ~4 GBï¼ˆç¯€çœ 33%ï¼‰

    ç¯„ä¾‹ï¼š
        >>> lora_config = create_lora_config(r=16)
        >>> model = prepare_model_for_training(model, lora_config)
        >>> model.print_trainable_parameters()
        trainable params: 2,097,152 || all params: 3,002,097,152 || trainable%: 0.0698
    """
    print("\nğŸ”§ Preparing model for QLoRA training...")

    # Step 1: æº–å‚™ k-bit è¨“ç·´
    # - å‡çµé‡åŒ–æ¬Šé‡ï¼ˆ4-bit éƒ¨åˆ†ï¼‰
    # - å•Ÿç”¨è¼¸å…¥çš„æ¢¯åº¦è¨ˆç®—ï¼ˆæŸäº›æ¶æ§‹éœ€è¦ï¼‰
    model = prepare_model_for_kbit_training(model)
    print("   âœ… k-bit training preparation done")

    # Step 2: å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰
    if use_gradient_checkpointing:
        model.gradient_checkpointing_enable()
        print("   âœ… Gradient checkpointing enabled")

        # æŸäº›ç’°å¢ƒéœ€è¦é¡å¤–è¨­å®šï¼ˆç©©å®šæ€§ï¼‰
        if hasattr(model, "enable_input_require_grads"):
            model.enable_input_require_grads()

        # è¨­å®šæ¢¯åº¦æª¢æŸ¥é»åƒæ•¸ï¼ˆPyTorch 2.0+ï¼‰
        if hasattr(model, "gradient_checkpointing_kwargs"):
            model.gradient_checkpointing_kwargs = {"use_reentrant": False}

    # Step 3: å¥—ç”¨ LoRA
    # - åœ¨ç›®æ¨™å±¤æ’å…¥ LoRA æ¨¡çµ„
    # - å‡çµæ‰€æœ‰é è¨“ç·´æ¬Šé‡
    # - åªè¨“ç·´ LoRA åƒæ•¸
    model = get_peft_model(model, lora_config)
    print("   âœ… LoRA adapters applied")

    # é¡¯ç¤ºå¯è¨“ç·´åƒæ•¸çµ±è¨ˆ
    print("\nğŸ“Š Trainable Parameters:")
    model.print_trainable_parameters()

    return model


# ====================================================================================
# 5) GPU è¨˜æ†¶é«”çµ±è¨ˆå·¥å…·
# ====================================================================================

def print_memory_stats(prefix: str = "") -> None:
    """
    é¡¯ç¤ºç•¶å‰ GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³

    Args:
        prefix: è¼¸å‡ºå‰ç¶´å­—ä¸²ï¼ˆä¾‹å¦‚ \"After loading model\"ï¼‰

    è¼¸å‡ºæ ¼å¼ï¼š
        GPU Memory: Allocated=1.50 GB | Reserved=2.00 GB | Free=6.00 GB

    è¨˜æ†¶é«”é¡å‹èªªæ˜ï¼š
        - Allocated: å¯¦éš›ä½¿ç”¨çš„è¨˜æ†¶é«”ï¼ˆå¼µé‡ä½”ç”¨ï¼‰
        - Reserved: PyTorch é ç•™çš„è¨˜æ†¶é«”ï¼ˆåŒ…å«å¿«å–ï¼‰
        - Free: å¯ç”¨è¨˜æ†¶é«”ï¼ˆç¸½é¡¯å­˜ - Reservedï¼‰

    ç¯„ä¾‹ï¼š
        >>> print_memory_stats(\"After loading model\")
        [After loading model] GPU Memory: Allocated=1.50 GB | Reserved=2.00 GB | Free=6.00 GB
    """
    if not torch.cuda.is_available():
        print("âš ï¸  CUDA not available, cannot show memory stats")
        return

    # å–å¾—è¨˜æ†¶é«”çµ±è¨ˆï¼ˆå–®ä½ï¼šbytesï¼‰
    allocated = torch.cuda.memory_allocated() / 1e9   # å·²åˆ†é…
    reserved = torch.cuda.memory_reserved() / 1e9     # å·²é ç•™
    total = torch.cuda.get_device_properties(0).total_memory / 1e9  # ç¸½é¡¯å­˜
    free = total - reserved                            # å¯ç”¨

    # æ ¼å¼åŒ–è¼¸å‡º
    prefix_str = f"[{prefix}] " if prefix else ""
    print(f"{prefix_str}ğŸ’¾ GPU Memory: "
          f"Allocated={allocated:.2f} GB | "
          f"Reserved={reserved:.2f} GB | "
          f"Free={free:.2f} GB")


# ====================================================================================
# 6) å¿«æ·å‡½æ•¸ï¼šä¸€éµæº–å‚™ QLoRA è¨“ç·´
# ====================================================================================

def setup_qlora_model(
    model_name: str,
    lora_rank: int = 16,
    lora_alpha: float = 32.0,
    lora_dropout: float = 0.05,
    target_modules: Optional[list[str]] = None,
) -> tuple[AutoModelForCausalLM, AutoTokenizer]:
    """
    ä¸€éµè¨­ç½® QLoRA æ¨¡å‹ï¼ˆæ•´åˆæ‰€æœ‰æ­¥é©Ÿï¼‰

    Args:
        model_name: æ¨¡å‹åç¨±
        lora_rank: LoRA rank
        lora_alpha: LoRA alpha
        lora_dropout: LoRA dropout
        target_modules: ç›®æ¨™æ¨¡çµ„åˆ—è¡¨

    Returns:
        (model, tokenizer): æº–å‚™å¥½è¨“ç·´çš„æ¨¡å‹èˆ‡ tokenizer

    é€™å€‹å‡½æ•¸æ•´åˆäº†ï¼š
        1. å»ºç«‹é‡åŒ–é…ç½®
        2. è¼‰å…¥é‡åŒ–æ¨¡å‹èˆ‡ tokenizer
        3. å»ºç«‹ LoRA é…ç½®
        4. æº–å‚™è¨“ç·´ä¸¦å¥—ç”¨ LoRA
        5. é¡¯ç¤ºè¨˜æ†¶é«”çµ±è¨ˆ

    ç¯„ä¾‹ï¼ˆæœ€ç°¡å–®çš„ä½¿ç”¨æ–¹å¼ï¼‰ï¼š
        >>> model, tokenizer = setup_qlora_model(\"Qwen/Qwen2.5-3B\")
        >>> # ç›´æ¥é–‹å§‹è¨“ç·´ï¼
    """
    print("="*60)
    print("ğŸš€ Setting up QLoRA Model")
    print("="*60)

    # 1. è¼‰å…¥é‡åŒ–æ¨¡å‹èˆ‡ tokenizer
    model, tokenizer = load_model_and_tokenizer(model_name)
    print_memory_stats("After loading model")

    # 2. å»ºç«‹ LoRA é…ç½®ä¸¦æº–å‚™è¨“ç·´
    lora_config = create_lora_config(
        r=lora_rank,
        alpha=lora_alpha,
        dropout=lora_dropout,
        target_modules=target_modules,
    )
    model = prepare_model_for_training(model, lora_config)
    print_memory_stats("After applying LoRA")

    print("\n" + "="*60)
    print("âœ… QLoRA Model Ready for Training!")
    print("="*60)

    return model, tokenizer
