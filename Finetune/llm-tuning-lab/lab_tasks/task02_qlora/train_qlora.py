"""
QLoRA è¨“ç·´è…³æœ¬

ä½¿ç”¨ 4-bit é‡åŒ– + LoRA è¨“ç·´å¤§å‹èªè¨€æ¨¡å‹

ä½¿ç”¨æ–¹å¼ï¼š
    python train_qlora.py --model meta-llama/Llama-2-7b-hf --epochs 3

ä½œè€…ï¼šLLM Tuning Lab
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from datasets import load_dataset
import argparse
import os

def create_bnb_config():
    """å‰µå»º 4-bit é‡åŒ–é…ç½®"""
    return BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

def create_lora_config(args):
    """å‰µå»º LoRA é…ç½®"""
    return LoraConfig(
        r=args.rank,
        lora_alpha=args.alpha,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
    )

def load_model_and_tokenizer(args):
    """è¼‰å…¥é‡åŒ–æ¨¡å‹å’Œ tokenizer"""
    print(f"Loading model: {args.model_name}")

    # é‡åŒ–é…ç½®
    bnb_config = create_bnb_config()

    # è¼‰å…¥æ¨¡å‹ï¼ˆè‡ªå‹•é‡åŒ–ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )

    # è¼‰å…¥ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer

def prepare_model_for_training(model, args):
    """æº–å‚™æ¨¡å‹ä¸¦æ·»åŠ  LoRA"""
    print("Preparing model for k-bit training...")

    # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ç­‰å„ªåŒ–
    model = prepare_model_for_kbit_training(model)

    # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰
    model.gradient_checkpointing_enable()

    # æ·»åŠ  LoRA
    lora_config = create_lora_config(args)
    model = get_peft_model(model, lora_config)

    # é¡¯ç¤ºåƒæ•¸çµ±è¨ˆ
    model.print_trainable_parameters()

    return model

def print_memory_stats():
    """é¡¯ç¤ºè¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        reserved = torch.cuda.memory_reserved() / 1e9
        print(f"\nGPU Memory:")
        print(f"  Allocated: {allocated:.2f} GB")
        print(f"  Reserved:  {reserved:.2f} GB")

def main(args):
    print("=" * 60)
    print("ğŸš€ QLoRA Training Script")
    print("=" * 60)

    # æª¢æŸ¥ CUDA
    if not torch.cuda.is_available():
        print("Warning: CUDA not available. QLoRA requires GPU.")
        return

    # è¼‰å…¥æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(args)
    print_memory_stats()

    # æº–å‚™è¨“ç·´
    model = prepare_model_for_training(model, args)
    print_memory_stats()

    # è¼‰å…¥æ•¸æ“š
    print("\nLoading dataset...")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1")

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.max_length,
            padding="max_length",
        )

    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset["train"].column_names,
    )

    # è¨“ç·´é…ç½®
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        fp16=False,  # QLoRA ä½¿ç”¨ bf16
        bf16=True,
        logging_steps=10,
        save_steps=500,
        save_total_limit=2,
        optim="paged_adamw_32bit",  # QLoRA æ¨è–¦çš„å„ªåŒ–å™¨
        warmup_steps=100,
        report_to="tensorboard",
    )

    # å‰µå»º Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
    )

    # é–‹å§‹è¨“ç·´
    print("\n" + "=" * 60)
    print("Starting training...")
    print("=" * 60 + "\n")

    trainer.train()

    # ä¿å­˜æ¨¡å‹
    print("\nSaving model...")
    model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    print("\n" + "=" * 60)
    print("âœ… Training completed!")
    print(f"Model saved to: {args.output_dir}")
    print("=" * 60)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()

    # æ¨¡å‹åƒæ•¸
    parser.add_argument("--model_name", type=str, default="meta-llama/Llama-2-7b-hf")

    # LoRA åƒæ•¸
    parser.add_argument("--rank", type=int, default=16)
    parser.add_argument("--alpha", type=float, default=32.0)
    parser.add_argument("--lora_dropout", type=float, default=0.05)

    # è¨“ç·´åƒæ•¸
    parser.add_argument("--num_epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=4)
    parser.add_argument("--gradient_accumulation_steps", type=int, default=4)
    parser.add_argument("--learning_rate", type=float, default=2e-4)
    parser.add_argument("--max_length", type=int, default=512)

    # è¼¸å‡º
    parser.add_argument("--output_dir", type=str, default="./output_qlora")

    args = parser.parse_args()
    main(args)
