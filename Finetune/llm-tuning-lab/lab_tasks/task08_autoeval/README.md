# Task 08: è‡ªå‹•åŒ–è©•æ¸¬ (AutoEval)

> å»ºç«‹è‡ªå‹•åŒ–æ¨¡åž‹è©•æ¸¬æ¡†æž¶

## ðŸŽ¯ å­¸ç¿’ç›®æ¨™

- âœ… å¯¦ä½œè©•æ¸¬æ¡†æž¶
- âœ… å¤šç¨®è©•æ¸¬æŒ‡æ¨™ï¼ˆRouge, BLEU, GPT-Evalï¼‰
- âœ… åŸºæº–æ¸¬è©¦é›†ç®¡ç†
- âœ… è©•æ¸¬å ±å‘Šç”Ÿæˆ

## è©•æ¸¬æµç¨‹

```python
# è¨“ç·´å®Œæˆå¾Œè‡ªå‹•è§¸ç™¼
evaluator = AutoEvaluator(
    model=model,
    test_set="benchmarks/test.jsonl"
)

results = evaluator.evaluate()
# => {rouge-1: 0.45, bleu: 0.38, ...}

evaluator.generate_report("report.html")
```

è©³è¦‹ [GUIDE.md](GUIDE.md)
