"""
è¶…ç°¡åŒ–çš„å¾®èª¿è…³æœ¬ - ä½¿ç”¨ PEFT åº«

é€™å€‹è…³æœ¬è®“ä½ ç”¨æœ€å°‘çš„ç¨‹å¼ç¢¼å®Œæˆæ¨¡å‹å¾®èª¿

ä½¿ç”¨æ–¹å¼ï¼š
    python simple_finetune.py --data_file my_data.csv --output_dir ./my_model

ä½œè€…ï¼šLLM Tuning Lab
"""

import argparse
import pandas as pd
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import LoraConfig, get_peft_model, TaskType
import torch

def load_data(file_path):
    """
    è¼‰å…¥æ•¸æ“š - æ”¯æ´ CSV, JSON, TXT

    CSV æ ¼å¼: input,output
    JSON æ ¼å¼: [{"input": "...", "output": "..."}]
    TXT æ ¼å¼: æ¯è¡Œä¸€å€‹å°è©±
    """
    print(f"ğŸ“Š è¼‰å…¥æ•¸æ“š: {file_path}")

    if file_path.endswith('.csv'):
        df = pd.read_csv(file_path)
        texts = [f"{row['input']}\n{row['output']}" for _, row in df.iterrows()]
    elif file_path.endswith('.json'):
        df = pd.read_json(file_path)
        texts = [f"{row['input']}\n{row['output']}" for _, row in df.iterrows()]
    elif file_path.endswith('.txt'):
        with open(file_path, 'r', encoding='utf-8') as f:
            texts = f.readlines()
    else:
        raise ValueError("æ”¯æ´çš„æ ¼å¼: .csv, .json, .txt")

    print(f"âœ… è¼‰å…¥äº† {len(texts)} å€‹è¨“ç·´æ¨£æœ¬")
    return Dataset.from_dict({"text": texts})

def main(args):
    print("=" * 60)
    print("ğŸš€ é–‹å§‹å¾®èª¿å°æ¨¡å‹")
    print("=" * 60)

    # ========== 1. è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer ==========
    print(f"\nğŸ“¥ è¼‰å…¥æ¨¡å‹: {args.model_name}")
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
    )

    # è¨­å®š pad token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.eos_token_id

    # ========== 2. æ‡‰ç”¨ LoRAï¼ˆä½¿ç”¨ PEFTï¼‰==========
    print(f"\nğŸ”§ æ·»åŠ  LoRA (rank={args.lora_rank})")

    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=args.lora_rank,
        lora_alpha=args.lora_alpha,
        lora_dropout=0.05,
        target_modules=["c_attn", "c_proj"] if "gpt2" in args.model_name.lower() else ["q_proj", "v_proj"],
    )

    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # ========== 3. è¼‰å…¥æ•¸æ“š ==========
    dataset = load_data(args.data_file)

    def tokenize_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            max_length=args.max_length,
            padding="max_length",
        )

    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names,
    )

    # ========== 4. è¨“ç·´é…ç½® ==========
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        logging_steps=10,
        save_strategy="epoch",
        fp16=torch.cuda.is_available(),
        report_to="none",  # ä¸ä½¿ç”¨ wandb ç­‰
    )

    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # Causal LM
    )

    # ========== 5. è¨“ç·´ ==========
    print(f"\nğŸ‹ï¸ é–‹å§‹è¨“ç·´ {args.epochs} epochs...")

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
    )

    trainer.train()

    # ========== 6. ä¿å­˜æ¨¡å‹ ==========
    print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹åˆ°: {args.output_dir}")
    model.save_pretrained(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    print("\n" + "=" * 60)
    print("âœ… è¨“ç·´å®Œæˆï¼")
    print("=" * 60)
    print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_dir}")
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print(f"1. æ¸¬è©¦æ¨¡å‹: python test_from_hf.py --model_path {args.output_dir}")
    print(f"2. ä¸Šå‚³åˆ° HF: python upload_to_hf.py --model_path {args.output_dir}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç°¡åŒ–çš„å¾®èª¿è…³æœ¬")

    # æ¨¡å‹åƒæ•¸
    parser.add_argument("--model_name", type=str, default="distilgpt2",
                       help="æ¨¡å‹åç¨± (distilgpt2, gpt2, TinyLlama/TinyLlama-1.1B-Chat-v1.0)")

    # æ•¸æ“šåƒæ•¸
    parser.add_argument("--data_file", type=str, required=True,
                       help="è¨“ç·´æ•¸æ“šæª”æ¡ˆ (.csv, .json, .txt)")

    # LoRA åƒæ•¸
    parser.add_argument("--lora_rank", type=int, default=8,
                       help="LoRA rank")
    parser.add_argument("--lora_alpha", type=float, default=16,
                       help="LoRA alpha")

    # è¨“ç·´åƒæ•¸
    parser.add_argument("--epochs", type=int, default=3,
                       help="è¨“ç·´ epochs")
    parser.add_argument("--batch_size", type=int, default=4,
                       help="Batch size")
    parser.add_argument("--learning_rate", type=float, default=2e-4,
                       help="å­¸ç¿’ç‡")
    parser.add_argument("--max_length", type=int, default=256,
                       help="æœ€å¤§åºåˆ—é•·åº¦")

    # è¼¸å‡ºåƒæ•¸
    parser.add_argument("--output_dir", type=str, default="./my_model",
                       help="è¼¸å‡ºç›®éŒ„")

    args = parser.parse_args()
    main(args)
