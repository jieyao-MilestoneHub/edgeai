"""æ¸¬è©¦æ¨¡å‹ï¼ˆæœ¬åœ°æˆ– HF Hubï¼‰"""
import argparse
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import PeftModel
import torch

def main(args):
    print("=" * 60)
    print("ğŸ§ª æ¸¬è©¦æ¨¡å‹")
    print("=" * 60)

    # è¼‰å…¥æ¨¡å‹
    if args.model_path:
        print(f"\nğŸ“¥ è¼‰å…¥æœ¬åœ°æ¨¡å‹: {args.model_path}")
        model_name = args.model_path
    else:
        print(f"\nğŸ“¥ å¾ HF Hub è¼‰å…¥: {args.model_name}")
        model_name = args.model_name

    # ä½¿ç”¨ pipelineï¼ˆæœ€ç°¡å–®ï¼‰
    try:
        generator = pipeline(
            "text-generation",
            model=model_name,
            device=0 if torch.cuda.is_available() else -1,
        )

        print(f"\nğŸ¤– è¼¸å…¥: {args.prompt}")
        outputs = generator(
            args.prompt,
            max_length=args.max_length,
            num_return_sequences=1,
            temperature=0.7,
        )

        print(f"ğŸ’¬ è¼¸å‡º: {outputs[0]['generated_text']}")

    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        print("\næç¤ºï¼šå¦‚æœæ˜¯ LoRA æ¨¡å‹ï¼Œå¯èƒ½éœ€è¦æ‰‹å‹•è¼‰å…¥")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, help="æœ¬åœ°æ¨¡å‹è·¯å¾‘")
    parser.add_argument("--model_name", type=str, help="HF Hub æ¨¡å‹åç¨±")
    parser.add_argument("--prompt", type=str, default="Hello, how are you?", help="æ¸¬è©¦ prompt")
    parser.add_argument("--max_length", type=int, default=100, help="æœ€å¤§ç”Ÿæˆé•·åº¦")
    args = parser.parse_args()
    
    if not args.model_path and not args.model_name:
        print("âŒ è«‹æä¾› --model_path æˆ– --model_name")
        exit(1)
    
    main(args)
