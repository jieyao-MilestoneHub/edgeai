# Neural Network Pruning Bootcamp

**21-Day Enterprise-Grade Pruning Training Program**

Learn neural network pruning from unstructured to structured techniques, targeting **â‰¥40% FLOPs reduction** with **â‰¤1.5% accuracy drop** on ResNet-18.

---

## Goal

Achieve enterprise-grade pruning skills by mastering:
- **Unstructured pruning** (Week 1)
- **Structured pruning** (Week 2)
- **Advanced techniques** (Week 3)

**Target**: Prune ResNet-18 (CIFAR-10/ImageNet-mini) with â‰¥40% FLOPs reduction and â‰¤1.5% Top-1 accuracy loss, with clear justification for all design choices.

---

## Quick Start

### Installation

This project uses `uv` for package management (isolated from parent repo):

```bash
# Navigate to Pruning directory
cd Pruning

# Install dependencies (CPU-only, recommended for most users)
uv sync

# OR for CUDA 12.1
uv sync --extra cuda121

# OR for CUDA 12.4
uv sync --extra cuda124

# Activate virtual environment
source .venv/bin/activate  # Linux/Mac
# or
.venv\Scripts\activate  # Windows
```

### Basic Usage

```bash
# 1. Train baseline model
python pruning-bootcamp/scripts/train.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --save-dir results/baseline

# 2. Evaluate baseline
python pruning-bootcamp/scripts/eval.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth

# 3. Apply structured pruning (Week 2)
python pruning-bootcamp/scripts/prune_structured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method bn_scale \
  --sparsity 0.5 \
  --save-dir results/structured

# 4. Profile and compare
python pruning-bootcamp/scripts/profile.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --baseline results/baseline/best_model.pth \
  --pruned results/structured/pruned_model.pth \
  --measure-latency
```

---

## ğŸ“˜ æ ¸å¿ƒè…³æœ¬è©³è§£

æ¯å€‹è…³æœ¬éƒ½æœ‰æ˜ç¢ºçš„å­¸ç¿’ç›®æ¨™èˆ‡é æœŸè¼¸å‡ºï¼Œç¢ºä¿æ‚¨çŸ¥é“ã€Œç‚ºä»€éº¼åŸ·è¡Œã€èˆ‡ã€Œæœƒå¾—åˆ°ä»€éº¼ã€ã€‚

### ğŸ¯ train.py - åŸºæº–æ¨¡å‹è¨“ç·´

**å­¸ç¿’ç›®æ¨™:**
- ç†è§£å®Œæ•´è¨“ç·´æµç¨‹ï¼šdata loading â†’ forward â†’ backward â†’ optimization
- æŒæ¡ Metrics è¿½è¹¤ï¼šFLOPs, Params, Accuracy
- å­¸ç¿’ TensorBoard å¯è¦–åŒ–è¨“ç·´æ›²ç·š
- å»ºç«‹ Baseline ä½œç‚ºå¾ŒçºŒå‰ªæå°æ¯”åŸºæº–

**é—œéµæ¦‚å¿µ:**
- Data augmentation (RandomCrop, RandomHorizontalFlip)
- Learning rate scheduling (MultiStepLR)
- Early stopping (ä¿å­˜æœ€ä½³æ¨¡å‹)
- Checkpoint management

**åŸ·è¡Œå‘½ä»¤:**
```bash
python pruning-bootcamp/scripts/train.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --save-dir results/baseline
```

**é æœŸè¼¸å‡º:**
```
=== Baseline Model Metrics ===
FLOPs: 0.56 G
Params: 11.17 M

Epoch 1: Train Loss=1.234, Train Acc=45.23%, Val Loss=1.012, Val Acc=62.45%
...
Epoch 100: Train Loss=0.123, Train Acc=95.67%, Val Loss=0.234, Val Acc=93.12%

âœ“ Saved best model with accuracy: 93.12%

=== Training Complete ===
Best Validation Accuracy: 93.12%
```

**è¼¸å‡ºæª”æ¡ˆ:**
- `results/baseline/best_model.pth` - æœ€ä½³æ¨¡å‹æ¬Šé‡
- `results/baseline/logs/` - TensorBoard è¨“ç·´æ›²ç·š
- `results/baseline/metrics.yaml` - å®Œæ•´æŒ‡æ¨™è¨˜éŒ„

**å¸¸è¦‹å•é¡Œ:**
- **OOM (Out of Memory):** æ¸›å° batch_size (128 â†’ 64)
- **æº–ç¢ºç‡éä½:** æª¢æŸ¥ data augmentation æ˜¯å¦éå¼·
- **è¨“ç·´ä¸ç©©å®š:** é™ä½ learning rate

---

### ğŸ“Š eval.py - æ¨¡å‹è©•ä¼°èˆ‡æŒ‡æ¨™åˆ†æ

**å­¸ç¿’ç›®æ¨™:**
- å»ºç«‹æ¨™æº–åŒ–è©•ä¼°æµç¨‹
- ç†è§£ FLOPs è¨ˆç®—åŸç† (ç†è«–é‹ç®—é‡)
- ç†è§£ Latency æ¸¬é‡æ–¹æ³• (å¯¦éš›æ¨ç†æ™‚é–“)
- **æ ¸å¿ƒèªçŸ¥: FLOPs â‰  Latency**

**é—œéµæ¦‚å¿µ:**
- Inference mode (torch.no_grad(), model.eval())
- GPU warmup (é¿å…é¦–æ¬¡æ¨ç†åå·®)
- CUDA events ç²¾ç¢ºè¨ˆæ™‚
- Metrics normalization (ç›¸å°æ–¼ baseline)

**åŸ·è¡Œå‘½ä»¤:**
```bash
python pruning-bootcamp/scripts/eval.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --measure-latency
```

**é æœŸè¼¸å‡º:**
```
Using device: cuda

âœ“ Loaded checkpoint from results/baseline/best_model.pth

=== Evaluating Model ===
Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 79/79 [00:12<00:00,  6.45it/s]
Accuracy: 93.12%
FLOPs: 0.56 G
Params: 11.17 M

=== Measuring Latency ===
Latency: 2.45 ms

=== Normalized Metrics ===
accuracy: 93.1200
flops_g: 0.56
params_m: 11.17
latency_ms: 2.45
throughput_fps: 408.16

âœ“ Metrics saved to results/baseline/eval_metrics.yaml
```

**è¼¸å‡ºæª”æ¡ˆ:**
- `results/baseline/eval_metrics.yaml` - å®Œæ•´è©•ä¼°æŒ‡æ¨™

**å¸¸è¦‹å•é¡Œ:**
- **Latency æ¸¬é‡ä¸ç©©å®š:** å¢åŠ  num_runs (200 â†’ 500)
- **FLOPs è¨ˆç®—å¤±æ•—:** å®‰è£ `thop` library: `pip install thop`

---

### âœ‚ï¸ prune_unstructured.py - éçµæ§‹åŒ–å‰ªæ

**å­¸ç¿’ç›®æ¨™:**
- æŒæ¡ 4 ç¨®éçµæ§‹åŒ–å‰ªææ–¹æ³•
  1. **L1 Magnitude** (layerwise): æ¯å±¤ç¨ç«‹å‰ªæ
  2. **Global Magnitude**: å…¨å±€æ¬Šé‡æ’åºå‰ªæ
  3. **Taylor Sensitivity**: |weight Ã— gradient| é‡è¦åº¦
  4. **Movement Pruning**: è¿½è¹¤æ¬Šé‡å‘é›¶ç§»å‹•è¶¨å‹¢
- ç†è§£ Layerwise vs Global åˆ†é…ç­–ç•¥
- æŒæ¡ One-shot vs Iterative å‰ªæå·®ç•°

**é—œéµæ¦‚å¿µ:**
- **Sparsity â‰  FLOPs æ¸›å°‘** (éçµæ§‹åŒ–ä¸æ¸›å°‘è¨ˆç®—é‡!)
- Fine-tuning è‡³é—œé‡è¦ (LR = 1/10 original)
- Iterative pruning é™ä½æº–ç¢ºç‡ä¸‹é™
- å‰ªæå¾Œéœ€ `remove_pruning_reparameterization()`

**åŸ·è¡Œå‘½ä»¤:**
```bash
# æ–¹æ³• 1: L1 layerwise
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method l1 --mode layerwise --sparsity 0.5

# æ–¹æ³• 2: Global magnitude
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method global --mode global --sparsity 0.5

# æ–¹æ³• 3: Iterative pruning
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method global --iterative --num-iterations 5 --sparsity 0.5
```

**é æœŸè¼¸å‡º:**
```
=== Baseline Model ===
Accuracy: 93.12%
Params: 11.17 M

=== Pruning: GLOBAL (global) ===
Target Sparsity: 50.0%
Mode: One-shot

=== Fine-tuning ===
Fine-tune Epoch 1: Val Acc = 89.23%
...
Fine-tune Epoch 30: Val Acc = 91.67%

=== Pruned Model Metrics ===
Actual Sparsity: 50.12%
Final Accuracy: 91.67%
Accuracy Drop: 1.45%

âœ“ Results saved to results/unstructured/global_global_sp0.5
```

**è¼¸å‡ºæª”æ¡ˆ:**
- `pruned_model.pth` - å‰ªæå¾Œæ¨¡å‹
- `metrics.yaml` - å‰ªææŒ‡æ¨™å°æ¯”

**å¸¸è¦‹å•é¡Œ:**
- **æº–ç¢ºç‡å¤§å¹…ä¸‹é™ (>3%):** é™ä½ sparsity æˆ–å¢åŠ  fine-tune epochs
- **FLOPs æ²’æ¸›å°‘:** é€™æ˜¯æ­£å¸¸çš„ï¼éçµæ§‹åŒ–ä¸æ¸›å°‘ FLOPsï¼Œéœ€ç”¨ sparse kernel

---

### ğŸ”§ prune_structured.py - çµæ§‹åŒ–å‰ªæ

**å­¸ç¿’ç›®æ¨™:**
- **é”æˆä¼æ¥­ç›®æ¨™: â‰¥40% FLOPs æ¸›å°‘, â‰¤1.5% æº–ç¢ºç‡ä¸‹é™**
- ç†è§£ Channel/Filter pruning åŸç†
- æŒæ¡ BatchNorm Scaling (Network Slimming) æŠ€è¡“
- å­¸ç¿’ä¾è³´åœ–è™•ç† (torch_pruning)

**é—œéµæ¦‚å¿µ:**
- **çµæ§‹åŒ–å‰ªæçœŸæ­£æ¸›å°‘ FLOPs/Latency**
- Dependency graph è‡ªå‹•è™•ç† skip connections
- BatchNorm Î³ (scaling factor) åæ˜  channel é‡è¦åº¦
- å‰ªæå¾Œæ¨¡å‹éœ€ shape verification

**åŸ·è¡Œå‘½ä»¤:**
```bash
# æ–¹æ³• 1: L1-based channel pruning
python pruning-bootcamp/scripts/prune_structured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method l1 --sparsity 0.4

# æ–¹æ³• 2: BatchNorm Scaling (æ¨è–¦)
python pruning-bootcamp/scripts/prune_structured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method bn_scale --sparsity 0.5
```

**é æœŸè¼¸å‡º:**
```
=== Baseline Model ===
Accuracy: 93.12%
FLOPs: 0.56 G
Params: 11.17 M

=== Structured Pruning: BN_SCALE ===
Target Sparsity: 50.0%

=== After Pruning ===
FLOPs: 0.28 G (50.0% reduction)
Params: 5.58 M (50.0% reduction)

=== Fine-tuning ===
Fine-tune Epoch 1: Val Acc = 88.45%
...
Fine-tune Epoch 20: Val Acc = 92.34%

=== Final Results ===
Baseline Accuracy: 93.12%
Pruned Accuracy: 92.34%
Accuracy Drop: 0.78%
FLOPs Reduction: 50.0%
Params Reduction: 50.0%

âœ… Goal MET (â‰¥40% FLOPs, â‰¤1.5% acc drop)

âœ“ Results saved to results/structured/bn_scale_sp0.5
```

**è¼¸å‡ºæª”æ¡ˆ:**
- `pruned_model.pth` - å£“ç¸®å¾Œæ¨¡å‹
- `metrics.yaml` - åŒ…å« `goal_met` åˆ¤å®š

**å¸¸è¦‹å•é¡Œ:**
- **Shape mismatch error:** torch_pruning è‡ªå‹•è™•ç†ï¼Œè‹¥ä»å ±éŒ¯æª¢æŸ¥ skip connections
- **FLOPs æ¸›å°‘ä¸è¶³:** æé«˜ sparsity (0.4 â†’ 0.6)
- **æº–ç¢ºç‡ä¸‹é™éå¤§:** ä½¿ç”¨ bn_scale è€Œé l1

---

### ğŸ“¦ export_onnx.py - æ¨¡å‹éƒ¨ç½²å°å‡º

**å­¸ç¿’ç›®æ¨™:**
- å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚ºéƒ¨ç½²æ ¼å¼
- é©—è­‰ ONNX/TorchScript æ•¸å€¼ä¸€è‡´æ€§
- ç†è§£ opset version èˆ‡å…¼å®¹æ€§

**é—œéµæ¦‚å¿µ:**
- ONNX: è·¨æ¡†æ¶æ¨¡å‹äº¤æ›æ ¼å¼
- TorchScript: PyTorch åŸç”Ÿåºåˆ—åŒ– (æ›´å¿«)
- Numerical verification (max diff < 1e-5)

**åŸ·è¡Œå‘½ä»¤:**
```bash
python pruning-bootcamp/scripts/export_onnx.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/structured/pruned_model.pth \
  --format both \
  --verify
```

**é æœŸè¼¸å‡º:**
```
Using device: cuda

âœ“ Loaded checkpoint from results/structured/pruned_model.pth

=== Exporting Model ===
âœ“ Exported to ONNX: results/structured/model.onnx
Max difference between PyTorch and ONNX: 0.000001
âœ“ ONNX export verified successfully

âœ“ Exported to TorchScript: results/structured/model.pt
Max difference between PyTorch and TorchScript: 0.000000
âœ“ TorchScript export verified successfully

âœ“ Export complete. Files saved to results/structured
```

**è¼¸å‡ºæª”æ¡ˆ:**
- `model.onnx` - ONNX æ ¼å¼æ¨¡å‹
- `model.pt` - TorchScript æ ¼å¼æ¨¡å‹

**å¸¸è¦‹å•é¡Œ:**
- **ONNX è¼¸å‡ºå·®ç•°éå¤§:** æª¢æŸ¥ dynamic_axes è¨­ç½®
- **å°å‡ºå¤±æ•—:** é™ä½ opset_version (13 â†’ 11)

---

### âš¡ profile.py - æ€§èƒ½åˆ†æ (FLOPs vs Latency)

**å­¸ç¿’ç›®æ¨™:**
- **æ ¸å¿ƒå¯¦é©—: è­‰æ˜ FLOPs â‰  Latency**
- ç†è§£ memory bandwidth ç“¶é ¸
- åˆ†æ kernel launch overhead
- å»ºç«‹æ­£ç¢ºæ€§èƒ½è©•ä¼°æ–¹æ³•

**é—œéµæ¦‚å¿µ:**
- FLOPs åªæ˜¯ç†è«–è¨ˆç®—é‡
- Latency å— memory å½±éŸ¿æ›´å¤§
- 40% FLOPs æ¸›å°‘ â†’ åƒ… 20-30% latency æ¸›å°‘
- GPU warmup é¿å…é¦–æ¬¡æ¨ç†åå·®

**åŸ·è¡Œå‘½ä»¤:**
```bash
python pruning-bootcamp/scripts/profile.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --baseline results/baseline/best_model.pth \
  --pruned results/structured/pruned_model.pth \
  --measure-latency
```

**é æœŸè¼¸å‡º:**
```
=== Profiling Baseline Model ===
FLOPs: 0.56 G
Params: 11.17 M
Latency: 2.45 ms
Throughput: 408.16 FPS

=== Profiling Pruned Model ===
FLOPs: 0.28 G
Params: 5.58 M
Latency: 1.89 ms
Throughput: 529.10 FPS

============================================================
BASELINE vs PRUNED MODEL COMPARISON
============================================================

Metric              Baseline         Pruned       Change
------------------------------------------------------------
FLOPs (G)               0.56           0.28       50.0%
Params (M)             11.17           5.58       50.0%
Latency (ms)            2.45           1.89       22.9%
Throughput (FPS)      408.16         529.10       29.6%
Speedup                                  1.30x

============================================================
FLOPs vs LATENCY ANALYSIS
============================================================
FLOPs Reduction:    50.0%
Latency Reduction:  22.9%
Discrepancy:        27.1% (FLOPs â‰  Latency!)

âš   Significant discrepancy between FLOPs and Latency reduction!
   This demonstrates that FLOPs is not always a reliable proxy for latency.
   Factors: memory bandwidth, cache efficiency, kernel launch overhead, etc.
```

**å¸¸è¦‹å•é¡Œ:**
- **Latency æ¸¬é‡ä¸ç©©å®š:** é—œé–‰å…¶ä»– GPU ç¨‹åºã€å¢åŠ  num_runs
- **Speedup ä½æ–¼é æœŸ:** æ­£å¸¸ç¾è±¡ï¼é€™æ­£æ˜¯æœ¬è…³æœ¬è¦è­‰æ˜çš„

---

## 21-Day Curriculum

### **Week 1: Unstructured Pruning Fundamentals** (Day 1-7)

#### **Day 1-2: Environment Setup & Baseline**
-  Setup environment with UV package manager
-  Train baseline ResNet-18 on CIFAR-10
-  Establish FLOPs/Latency/Accuracy measurement pipeline
-  Normalize metrics for consistent comparison

**Scripts**: `train.py`, `eval.py`
**Tools**: `metrics.py`, `latency.py`

```bash
python pruning-bootcamp/scripts/train.py --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml
python pruning-bootcamp/scripts/eval.py --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth --measure-latency
```

#### **Day 3-4: L1 & Global Magnitude Pruning**
-  Implement L1 unstructured pruning (layerwise)
-  Implement global magnitude pruning
-  Fine-tune pruned models
- Compare layerwise vs global strategies

**Scripts**: `prune_unstructured.py`
**Tools**: `pruning_utils.py`

```bash
# L1 layerwise pruning
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method l1 --mode layerwise --sparsity 0.5

# Global magnitude pruning
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method global --mode global --sparsity 0.5
```

#### **Day 5-7: Sparsity Allocation & Training Strategies**
- Analyze layerwise vs global sparsity distribution
- = Implement iterative pruning (gradual sparsity increase)
- Compare one-shot vs iterative approaches
- Tune fine-tuning strategies (LR, epochs, schedules)

```bash
# Iterative pruning
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method global --iterative --num-iterations 5 --sparsity 0.5
```

**Key Insights**:
- Global pruning often outperforms layerwise (cross-layer importance)
- Iterative pruning reduces accuracy drop but requires more training
- Fine-tuning LR is critical (typically 1/10 of original)

---

#### âœ… Week 1 Quick Validation

é©—è­‰æ‚¨æ˜¯å¦å®Œæˆ Week 1 å­¸ç¿’ç›®æ¨™ï¼š

```bash
# å¿«é€Ÿæª¢æŸ¥æ¸…å–®
cd Pruning

# 1. æª¢æŸ¥ baseline æ¨¡å‹å­˜åœ¨
ls results/baseline/best_model.pth
ls results/baseline/metrics.yaml

# 2. é©—è­‰è‡³å°‘å®Œæˆä¸€ç¨®éçµæ§‹åŒ–å‰ªæ
ls results/unstructured/*/pruned_model.pth

# 3. å°æ¯”æº–ç¢ºç‡ (æ‡‰åœ¨ â‰¤2% drop)
python pruning-bootcamp/scripts/eval.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/unstructured/global_global_sp0.5/pruned_model.pth \
  --baseline-metrics results/baseline/metrics.yaml
```

**é æœŸçµæœ:**
- âœ… Baseline æº–ç¢ºç‡: ~93%
- âœ… Pruned æº–ç¢ºç‡: ~91-92%
- âš ï¸ FLOPs æ²’æ¸›å°‘ (éçµæ§‹åŒ–ç‰¹æ€§)

**å¦‚æœå¤±æ•—:**
- æº–ç¢ºç‡ < 90%: é™ä½ sparsity æˆ–å¢åŠ  fine-tune epochs
- æª”æ¡ˆä¸å­˜åœ¨: é‡æ–°åŸ·è¡Œå°æ‡‰è…³æœ¬

---

### **Week 2: Structured Pruning & Dependencies** (Day 8-14)

#### **Day 8-10: Channel/Filter Pruning**
-  Implement L1-norm channel pruning
-  Implement BatchNorm Scaling (Network Slimming)
- =
 Understand structured vs unstructured tradeoffs
- Measure actual FLOPs reduction (not just sparsity)

**Scripts**: `prune_structured.py`
**Key Concept**: Structured pruning actually reduces FLOPs/latency (vs unstructured needs sparse kernels)

```bash
# L1-based channel pruning
python pruning-bootcamp/scripts/prune_structured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method l1 --sparsity 0.4

# BatchNorm scaling (Network Slimming)
python pruning-bootcamp/scripts/prune_structured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method bn_scale --sparsity 0.5
```

#### **Day 11-12: Dependency Graph Handling**
- = Use `torch.fx` / `Torch-Pruning` for dependency tracking
- Handle skip connections and batch norm dependencies
-  Ensure tensor shape alignment after pruning
- = Debug common pruning errors (shape mismatches)

**Key Tools**: `torch_pruning` library handles dependency graphs automatically

**Common Issues**:
- Skip connections require matching channel counts
- BatchNorm layers depend on preceding Conv layers
- Global pooling requires consistent feature map sizes

#### **Day 13-14: FLOPs â‰  Latency Verification**
-  Export to ONNX/TorchScript for deployment
- Measure actual inference latency (CPU/GPU)
- =
 Analyze FLOPs vs Latency discrepancy
- Understand memory bandwidth bottlenecks

**Scripts**: `export_onnx.py`, `profile.py`

```bash
# Export to ONNX
python pruning-bootcamp/scripts/export_onnx.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/structured/pruned_model.pth \
  --format both --verify

# Profile latency
python pruning-bootcamp/scripts/profile.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --baseline results/baseline/best_model.pth \
  --pruned results/structured/pruned_model.pth \
  --measure-latency
```

**Key Insight**: 40% FLOPs reduction may only yield 20-30% latency reduction due to memory/overhead!

---

#### âœ… Week 2 Quick Validation

é©—è­‰ä¼æ¥­ç›®æ¨™æ˜¯å¦é”æˆï¼š

```bash
# æ ¸å¿ƒæª¢æŸ¥ï¼šGoal Met?
cd Pruning

# 1. æª¢æŸ¥çµæ§‹åŒ–å‰ªæçµæœ
cat results/structured/bn_scale_sp0.5/metrics.yaml | grep -E "goal_met|flops_reduction|accuracy_drop"

# 2. é©—è­‰ FLOPs çœŸæ­£æ¸›å°‘ (â‰¥40%)
python pruning-bootcamp/scripts/profile.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --baseline results/baseline/best_model.pth \
  --pruned results/structured/bn_scale_sp0.5/pruned_model.pth \
  --measure-latency \
  --save-results results/week2_validation.yaml

# 3. æª¢æŸ¥ ONNX å°å‡ºæˆåŠŸ
ls results/structured/bn_scale_sp0.5/*.onnx
```

**é æœŸçµæœ:**
- âœ… `goal_met: true`
- âœ… `flops_reduction_pct: â‰¥40.0`
- âœ… `accuracy_drop: â‰¤1.5`
- âœ… FLOPs vs Latency discrepancy åˆ†æå ±å‘Š

**å¦‚æœå¤±æ•—:**
- FLOPs æ¸›å°‘ä¸è¶³ (< 40%): æé«˜ sparsity (0.5 â†’ 0.6)
- æº–ç¢ºç‡ä¸‹é™éå¤§ (> 1.5%): ä½¿ç”¨ `bn_scale` æ–¹æ³•ã€å¢åŠ  fine-tune epochs
- Goal not met: èª¿æ•´ sparsity å¹³è¡¡é»

---

### **Week 3: Advanced Techniques & Automation** (Day 15-21)

#### **Day 15-16: Importance Estimation Methods**
- > Implement Movement Pruning (track weight movement toward zero)
- Implement Taylor Sensitivity (|weight  gradient|)
- Compare different importance metrics
- Analyze correlation between importance and actual impact

**Tools**: Already implemented in `pruning_utils.py`

```bash
# Taylor sensitivity pruning
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method taylor --sparsity 0.5

# Movement pruning
python pruning-bootcamp/scripts/prune_unstructured.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/baseline/best_model.pth \
  --method movement --sparsity 0.5
```

#### **Day 17-18: N:M Sparsity (Optional)**
- =' Understand hardware-friendly sparsity patterns
- Implement 2:4 sparsity (NVIDIA Ampere support)
-  Benchmark actual speedup on compatible hardware
- Compare with unstructured sparsity

**Key Concept**: N:M sparsity (e.g., 2:4 = 2 zeros per 4 consecutive elements) is hardware-accelerated on modern GPUs

**Tools**: `n_m_sparsity()` in `pruning_utils.py`

#### **Day 19: Automated Hyperparameter Sweeper**
- > Automate sparsity/LR/epoch search
- Grid search or Bayesian optimization
- Track Pareto frontier (accuracy vs FLOPs)
- Log all experiments systematically

**Example Sweeper** (implement as needed):
```python
# Create scripts/sweep.py for grid search
for sparsity in [0.3, 0.4, 0.5, 0.6]:
    for lr in [0.001, 0.01, 0.05]:
        for epochs in [20, 30, 40]:
            run_pruning_experiment(sparsity, lr, epochs)
            log_results_to_csv()
```

#### **Day 20-21: Final Report & Lessons Learned**
- Document full pipeline and results
- Create ablation study tables
- L Analyze failure cases (e.g., over-pruning, poor recovery)
- Summarize key learnings

**Report Template**:
1. **Baseline Results**: Accuracy, FLOPs, Latency
2. **Pruning Results**: Method comparison table
3. **Ablation Studies**: Sparsity levels, fine-tuning strategies
4. **Failure Cases**: Over-aggressive pruning, sensitivity analysis
5. **Recommendations**: Best practices for production deployment

---

#### âœ… Week 3 Final Validation

å®Œæˆ 21 å¤©è¨“ç·´ç‡Ÿæœ€çµ‚æª¢é©—ï¼š

```bash
# ç¶œåˆè©•ä¼°æ‰€æœ‰å‰ªææ–¹æ³•
cd Pruning

# 1. ç”Ÿæˆå®Œæ•´æ–¹æ³•å°æ¯”è¡¨
echo "Method,Sparsity,FLOPs_Reduction,Accuracy_Drop,Goal_Met" > results/final_comparison.csv
for method in l1 global taylor movement; do
  python pruning-bootcamp/scripts/eval.py \
    --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
    --checkpoint results/unstructured/${method}_*/pruned_model.pth \
    --baseline-metrics results/baseline/metrics.yaml
done

# 2. é©—è­‰æœ€ä½³çµæ§‹åŒ–å‰ªææ¨¡å‹
python pruning-bootcamp/scripts/eval.py \
  --config pruning-bootcamp/cfgs/cifar10_resnet18.yaml \
  --checkpoint results/structured/bn_scale_sp0.5/pruned_model.pth \
  --measure-latency

# 3. å®Œæ•´æ€§æª¢æŸ¥
ls results/baseline/best_model.pth
ls results/unstructured/*/pruned_model.pth
ls results/structured/*/pruned_model.pth
ls results/structured/*/*.onnx
```

**é æœŸæˆæœ:**
- âœ… Week 1: å®Œæˆ 4 ç¨®éçµæ§‹åŒ–å‰ªæå¯¦é©—
- âœ… Week 2: é”æˆä¼æ¥­ç›®æ¨™ (â‰¥40% FLOPs, â‰¤1.5% acc drop)
- âœ… Week 3: ç†è§£é€²éšæ–¹æ³•èˆ‡ FLOPs â‰  Latency
- âœ… å®Œæ•´å¯¦é©—è¨˜éŒ„èˆ‡ ablation study

**ç•¢æ¥­æ¨™æº–:**
1. Baseline è¨“ç·´å®Œæˆ (accuracy â‰¥ 90%)
2. è‡³å°‘ 2 ç¨®éçµæ§‹åŒ–æ–¹æ³•å°æ¯”
3. çµæ§‹åŒ–å‰ªæé”æˆç›®æ¨™ (`goal_met: true`)
4. ONNX å°å‡ºèˆ‡ latency profiling å®Œæˆ
5. æ’°å¯«å¯¦é©—å ±å‘Šç¸½çµå­¸ç¿’å¿ƒå¾—

**ä¸‹ä¸€æ­¥:**
- å˜—è©¦ ImageNet-mini è³‡æ–™é›†
- æ“´å±•åˆ° MobileNet/EfficientNet
- çµåˆé‡åŒ– (Quantization-aware pruning)
- éƒ¨ç½²åˆ°é‚Šç·£è¨­å‚™å¯¦æ¸¬

ğŸ“ **æ­å–œå®Œæˆ 21 å¤©å‰ªæè¨“ç·´ç‡Ÿï¼**

---

## ğŸ“ Project Structure

```
Pruning/
-- pruning-bootcamp/
|   -- datasets/
|   |   -- __init__.py
|   |   -- loader.py          # CIFAR-10, ImageNet-mini loaders
|   -- scripts/
|   |   -- train.py           # Baseline training
|   |   -- eval.py            # Evaluation with metrics
|   |   -- prune_unstructured.py  # Week 1: L1, Global, Taylor, Movement
|   |   -- prune_structured.py    # Week 2: Channel pruning, BN scaling
|   |   -- export_onnx.py     # ONNX/TorchScript export
|   |   -- profile.py         # Latency profiling
|   -- cfgs/
|   |   -- cifar10_resnet18.yaml
|   |   -- imagenetmini_resnet18.yaml
|   -- results/               # Experiment outputs (auto-generated)
|   -- tools/
|       -- __init__.py
|       -- metrics.py         # FLOPs, params, accuracy metrics
|       -- latency.py         # Latency measurement
|       -- pruning_utils.py   # Pruning algorithms
-- pyproject.toml             # UV package config
-- .gitignore
-- README.md                  # This file
```

---

## Tools & Utilities

### **metrics.py**
- `compute_flops()`: FLOPs calculation
- `compute_params()`: Parameter counting
- `compute_sparsity()`: Weight sparsity
- `normalize_metrics()`: Relative to baseline
- `compare_metrics()`: Baseline vs pruned comparison

### **latency.py**
- `measure_latency()`: Accurate GPU/CPU timing
- `measure_latency_with_stats()`: Mean, std, percentiles
- `profile_layer_latency()`: Per-layer profiling
- `compare_latencies()`: FLOPs vs latency analysis

### **pruning_utils.py**
- `layerwise_magnitude_pruning()`: L1 per-layer
- `global_magnitude_pruning()`: Global L1
- `taylor_importance_pruning()`: Taylor sensitivity
- `movement_pruning()`: Movement-based
- `n_m_sparsity()`: Hardware-friendly 2:4 sparsity
- `remove_pruning_reparameterization()`: Make pruning permanent
- `print_layerwise_sparsity()`: Sparsity visualization

---

## Expected Results

| Method | Sparsity | FLOPs Reduction | Accuracy Drop | Goal Met? |
|--------|----------|----------------|---------------|-----------|
| Baseline | 0% | 0% | 0% | - |
| L1 Layerwise | 50% | ~0% (unstructured) | ~2% | âŒ |
| Global Magnitude | 50% | ~0% (unstructured) | ~1.5% | âŒ |
| Channel Pruning (L1) | 50% | 35-45% | 1-2% |  |
| BatchNorm Scaling | 50% | 40-50% | 0.5-1.5% |  |
| Taylor Sensitivity | 50% | ~0% (unstructured) | ~1.2% | âŒ |
| Movement Pruning | 50% | ~0% (unstructured) | ~1% | âŒ |

**Note**: Unstructured methods achieve high sparsity but don't reduce FLOPs without sparse kernels.

---

## Key Learnings

### **Unstructured vs Structured**
- **Unstructured**: High sparsity, minimal accuracy drop, but requires specialized sparse kernels
- **Structured**: Lower sparsity tolerance, but **actual** FLOPs/latency reduction

### **FLOPs â‰  Latency**
- Memory bandwidth bottlenecks
- Kernel launch overhead
- Cache efficiency matters
- Always measure real latency!

### **Fine-tuning is Critical**
- LR typically 1/10 - 1/5 of original training LR
- Longer fine-tuning helps but has diminishing returns
- Iterative pruning reduces shock to the network

### **Dependency Graphs**
- Use `torch_pruning` to handle complex dependencies
- Manual pruning prone to shape mismatch errors
- Test thoroughly after pruning (forward pass with dummy data)

---

## Troubleshooting

### **Shape Mismatch Errors**
```python
# Always test after pruning
dummy_input = torch.randn(1, 3, 32, 32).to(device)
output = model(dummy_input)
print(f"Output shape: {output.shape}")  # Should match expected
```

### **Poor Recovery After Pruning**
- Reduce sparsity target
- Increase fine-tuning epochs
- Use iterative pruning instead of one-shot
- Check if you're pruning critical layers (e.g., first/last)

### **FLOPs Not Reducing**
- You're using unstructured pruning (only adds sparsity, doesn't reduce ops)
- Use structured pruning for actual FLOPs reduction

---

## References

### **Papers**
- **Network Slimming** (Liu et al., 2017): BatchNorm-based channel pruning
- **Movement Pruning** (Sanh et al., 2020): Prune weights moving toward zero
- **Rethinking Network Pruning** (Liu et al., 2019): Structured pruning insights
- **The Lottery Ticket Hypothesis** (Frankle & Carbin, 2019): Importance of initialization

### **Libraries**
- [Torch-Pruning](https://github.com/VainF/Torch-Pruning): Dependency-aware structured pruning
- [ONNX](https://onnx.ai/): Model export for deployment
- [TorchVision](https://pytorch.org/vision/): Pre-built models

---

## > Contributing

This is a self-contained learning repo. Extend it by:
1. Adding new pruning methods (e.g., lottery ticket, AutoML-based)
2. Supporting more architectures (MobileNet, EfficientNet)
3. Adding quantization-aware pruning
4. Implementing neural architecture search (NAS) integration

---

## License

MIT License - Free to use for educational and commercial purposes.

---

## Final Checklist

- [ ] Day 1-2: Train baseline, establish metrics
- [ ] Day 3-4: L1 & global magnitude pruning
- [ ] Day 5-7: Sparsity allocation, iterative pruning
- [ ] Day 8-10: Channel pruning, BatchNorm scaling
- [ ] Day 11-12: Dependency handling, shape alignment
- [ ] Day 13-14: ONNX export, FLOPs vs latency
- [ ] Day 15-16: Taylor & movement pruning
- [ ] Day 17-18: N:M sparsity (optional)
- [ ] Day 19: Automated sweeper
- [ ] Day 20-21: Final report, ablations, lessons learned

**Good luck on your pruning journey!**
