# CIFAR-10 ResNet-18 Configuration
# Goal: ≥40% FLOPs reduction with ≤1.5% accuracy drop

dataset:
  name: cifar10
  input_size: 32
  num_classes: 10
  batch_size: 128
  num_workers: 4
  data_dir: ./data

model:
  name: resnet18
  pretrained: false

# Baseline training
training:
  epochs: 100
  lr: 0.1
  momentum: 0.9
  weight_decay: 5e-4
  milestones: [50, 75]
  gamma: 0.1
  optimizer: sgd
  criterion: cross_entropy

# Pruning configuration
pruning:
  # Target metrics
  target_flops_reduction: 0.4  # 40% FLOPs reduction
  target_accuracy_drop: 1.5    # Max 1.5% accuracy drop

  # Unstructured pruning settings (Week 1)
  unstructured:
    method: global  # Options: l1, global, taylor, movement
    sparsity: 0.5
    mode: global    # Options: layerwise, global
    iterative: true
    num_iterations: 5

  # Structured pruning settings (Week 2)
  structured:
    method: bn_scale  # Options: l1, bn_scale (Network Slimming)
    sparsity: 0.5
    iterative: false

  # Fine-tuning settings
  finetune_epochs: 30
  finetune_lr: 0.01
  finetune_momentum: 0.9
  finetune_weight_decay: 5e-4

# Profiling settings
profiling:
  num_warmup: 50
  num_runs: 200
  measure_latency: true
  export_onnx: true
  export_torchscript: true

# Experimental settings (Week 3)
experimental:
  # N:M sparsity (hardware-friendly)
  nm_sparsity:
    enabled: false
    n: 2
    m: 4

  # Automated hyperparameter search
  sweeper:
    enabled: false
    sparsity_range: [0.3, 0.4, 0.5, 0.6, 0.7]
    lr_range: [0.001, 0.01, 0.05]
    epochs_range: [20, 30, 40]
