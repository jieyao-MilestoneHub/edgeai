# ImageNet-mini ResNet-18 Configuration
# Goal: ≥40% FLOPs reduction with ≤1.5% accuracy drop

dataset:
  name: imagenet-mini
  input_size: 224
  num_classes: 1000  # Adjust based on actual ImageNet-mini subset
  batch_size: 64     # Smaller batch size for larger images
  num_workers: 4
  data_dir: ./data/imagenet-mini

model:
  name: resnet18
  pretrained: false

# Baseline training
training:
  epochs: 90
  lr: 0.1
  momentum: 0.9
  weight_decay: 1e-4
  milestones: [30, 60]
  gamma: 0.1
  optimizer: sgd
  criterion: cross_entropy

# Pruning configuration
pruning:
  # Target metrics
  target_flops_reduction: 0.4  # 40% FLOPs reduction
  target_accuracy_drop: 1.5    # Max 1.5% accuracy drop

  # Unstructured pruning settings (Week 1)
  unstructured:
    method: global  # Options: l1, global, taylor, movement
    sparsity: 0.5
    mode: global    # Options: layerwise, global
    iterative: true
    num_iterations: 5

  # Structured pruning settings (Week 2)
  structured:
    method: bn_scale  # Options: l1, bn_scale (Network Slimming)
    sparsity: 0.5
    iterative: false

  # Fine-tuning settings
  finetune_epochs: 20
  finetune_lr: 0.01
  finetune_momentum: 0.9
  finetune_weight_decay: 1e-4

# Profiling settings
profiling:
  num_warmup: 50
  num_runs: 200
  measure_latency: true
  export_onnx: true
  export_torchscript: true

# Experimental settings (Week 3)
experimental:
  # N:M sparsity (hardware-friendly)
  nm_sparsity:
    enabled: false
    n: 2
    m: 4

  # Automated hyperparameter search
  sweeper:
    enabled: false
    sparsity_range: [0.3, 0.4, 0.5, 0.6, 0.7]
    lr_range: [0.001, 0.01, 0.05]
    epochs_range: [15, 20, 25]
