# LLM Multimodal Lab

> å¾ç†è«–åˆ°å¯¦è¸ï¼šå¤šæ¨¡æ…‹å¤§èªè¨€æ¨¡å‹çš„é‡åŒ–èˆ‡å¾®èª¿å¯¦é©—å®¤

## å°ˆæ¡ˆç°¡ä»‹

æœ¬å°ˆæ¡ˆæ˜¯ä¸€å€‹å®Œæ•´çš„å¤šæ¨¡æ…‹å¤§èªè¨€æ¨¡å‹(Vision-Language Models, VLMs)å­¸ç¿’è·¯å¾‘,å°ˆæ³¨æ–¼åœ¨è³‡æºå—é™çš„é‚Šç·£AIç’°å¢ƒä¸‹,ä½¿ç”¨åƒæ•¸é«˜æ•ˆå¾®èª¿(PEFT)å’Œé‡åŒ–æŠ€è¡“ä¾†å„ªåŒ–æ¨¡å‹ã€‚

**æ ¸å¿ƒç›®æ¨™**:
- æ·±å…¥ç†è§£é‡åŒ–æŠ€è¡“çš„æ•¸å­¸åŸç†(å¾LoRAåˆ°QLoRA)
- æŒæ¡åœ¨æ¶ˆè²»ç´šGPUä¸Šå¾®èª¿VLMsçš„å¯¦ç”¨æŠ€è¡“
- å­¸ç¿’é‡å°ç‰¹å®šä»»å‹™(å¦‚æ–‡æª”ç†è§£ã€è¦–è¦ºå•ç­”)çš„æ¨¡å‹å„ªåŒ–ç­–ç•¥

## ç‚ºä»€éº¼é¸æ“‡é€™å€‹å°ˆæ¡ˆ?

### çœŸå¯¦å ´æ™¯é©…å‹•

ä½ æ˜¯ä¸€ä½Edge AIå·¥ç¨‹å¸«,éœ€è¦åœ¨RTX 3070 (8GB VRAM)ä¸Šå¾®èª¿ä¸€å€‹è¦–è¦ºèªè¨€æ¨¡å‹ä¾†è™•ç†infographicå•ç­”ä»»å‹™ã€‚

**æŒ‘æˆ°**:
- âœ— æ¨™æº–7Bæ¨¡å‹éœ€è¦14GB+ VRAM
- âœ— å®Œæ•´å¾®èª¿éœ€è¦40GB+ VRAM
- âœ— æ¨è«–é€Ÿåº¦ç„¡æ³•æ»¿è¶³å¯¦æ™‚éœ€æ±‚

**è§£æ±ºæ–¹æ¡ˆ**: QLoRA
- âœ“ 4-bité‡åŒ–å£“ç¸®åˆ°3.5GB
- âœ“ LoRAåªè¨“ç·´0.1-1%åƒæ•¸
- âœ“ 8GB VRAMå®Œå…¨è¶³å¤ 

### å¾æ•¸å­¸åŸç†åˆ°å·¥ç¨‹å¯¦è¸

æœ¬å°ˆæ¡ˆ**ä¸åªæ˜¯**èª¿ç”¨HuggingFace API,è€Œæ˜¯:
- ç†è§£ä½ç§©åˆ†è§£çš„æ•¸å­¸åŸç†(`W' = Wâ‚€ + BA`)
- æ‰‹å¯«NF4é‡åŒ–ç®—æ³•
- å¯¦ä½œé‡åŒ–æ„ŸçŸ¥è¨“ç·´(QAT)
- è¨­è¨ˆæ··åˆç²¾åº¦ç­–ç•¥

## å°ˆæ¡ˆçµæ§‹

```
llm-multimodel-lab/
â”œâ”€â”€ docs/                          # ç†è«–æ–‡æª”èˆ‡æ·±åº¦è§£æ
â”‚   â”œâ”€â”€ 00_overview.md             # ç¸½è¦½:å¾LoRAåˆ°é‡åŒ–çš„å®Œæ•´æŠ€è¡“æ£§
â”‚   â”œâ”€â”€ 01_infographicvqa_qlora.md # ç¬¬ä¸€ç« :InfographicVQAèˆ‡QLoRAå¾®èª¿
â”‚   â”œâ”€â”€ 02_qat_advanced.md         # ç¬¬äºŒç« :é‡åŒ–æ„ŸçŸ¥è¨“ç·´(QAT)
â”‚   â””â”€â”€ 03_mixed_precision.md      # ç¬¬ä¸‰ç« :æ··åˆç²¾åº¦å„ªåŒ–
â”‚
â”œâ”€â”€ lab_tasks/                     # å¯¦ä½œä»»å‹™
â”‚   â”œâ”€â”€ task01_infographicvqa/     # ä»»å‹™1:InfographicVQAå¾®èª¿å¯¦æˆ°
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ data_exploration.ipynb
â”‚   â”‚   â”œâ”€â”€ finetune_pix2struct.py
â”‚   â”‚   â”œâ”€â”€ finetune_qwen_vl.py
â”‚   â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ task02_qat/                # ä»»å‹™2:é‡åŒ–æ„ŸçŸ¥è¨“ç·´
â”‚   â””â”€â”€ task03_deployment/         # ä»»å‹™3:æ¨¡å‹éƒ¨ç½²å„ªåŒ–
â”‚
â”œâ”€â”€ scripts/                       # å¯¦ç”¨å·¥å…·è…³æœ¬
â”‚   â”œâ”€â”€ memory_profiler.py         # VRAMä½¿ç”¨åˆ†æ
â”‚   â”œâ”€â”€ model_converter.py         # æ¨¡å‹æ ¼å¼è½‰æ›
â”‚   â””â”€â”€ benchmark.py               # æ€§èƒ½åŸºæº–æ¸¬è©¦
â”‚
â””â”€â”€ assets/                        # è³‡æºæ–‡ä»¶
    â”œâ”€â”€ images/                    # åœ–è¡¨èˆ‡æ¶æ§‹åœ–
    â””â”€â”€ datasets/                  # è³‡æ–™é›†èªªæ˜
```

## å­¸ç¿’è·¯å¾‘

### ç¬¬0ç« :ç†è«–åŸºç¤(å¿…è®€)
ğŸ“– é–±è®€ [`docs/00_overview.md`](docs/00_overview.md)

**æ ¸å¿ƒæ¦‚å¿µ**:
- ç‚ºä»€éº¼éœ€è¦é‡åŒ–?è¨˜æ†¶é«”/é€Ÿåº¦/ç²¾åº¦çš„trade-off
- LoRAåŸç†:`W' = Wâ‚€ + BA`
- QLoRAæŠ€è¡“:NF4é‡åŒ–ã€é›™é‡é‡åŒ–ã€åˆ†é å„ªåŒ–å™¨
- PTQ vs QATçš„å·®ç•°

**æ™‚é–“**: 1-2å°æ™‚

---

### ç¬¬1ç« :InfographicVQAå¾®èª¿å¯¦æˆ°
ğŸ“– é–±è®€ [`docs/01_infographicvqa_qlora.md`](docs/01_infographicvqa_qlora.md)
ğŸ’» å¯¦ä½œ [`lab_tasks/task01_infographicvqa/`](lab_tasks/task01_infographicvqa/)

**å­¸ç¿’ç›®æ¨™**:
1. ç†è§£InfographicVQAè³‡æ–™é›†çš„ç‰¹æ€§èˆ‡æŒ‘æˆ°
2. é¸æ“‡é©åˆ8GB VRAMçš„æ¨¡å‹(Pix2Struct/Qwen-VL-3B)
3. æŒæ¡QLoRAçš„é…ç½®èˆ‡èª¿å„ªæŠ€å·§
4. å¯¦ä½œå®Œæ•´çš„å¾®èª¿pipeline

**ç¡¬é«”éœ€æ±‚**: RTX 3070 8GB (æˆ–Google Colabå…è²»T4)

**é æœŸæˆæœ**:
- æˆåŠŸå¾®èª¿å‡ºInfographicVQAå•ç­”æ¨¡å‹
- ç†è§£è¨˜æ†¶é«”å„ªåŒ–æŠ€å·§(gradient checkpointingã€mixed precision)
- æŒæ¡ANLSè©•ä¼°æŒ‡æ¨™

---

### ç¬¬2ç« :é‡åŒ–æ„ŸçŸ¥è¨“ç·´(é€²éš)
ğŸ“– é–±è®€ [`docs/02_qat_advanced.md`](docs/02_qat_advanced.md)
ğŸ’» å¯¦ä½œ [`lab_tasks/task02_qat/`](lab_tasks/task02_qat/)

**å­¸ç¿’ç›®æ¨™**:
1. å¯¦ä½œStraight-Through Estimator(STE)
2. ç†è§£Fake Quantizationæ©Ÿåˆ¶
3. å°æ¯”PTQ vs QATçš„ç²¾åº¦å·®ç•°
4. å­¸ç¿’QATè¶…åƒæ•¸èª¿æ•´

**é—œéµæ´å¯Ÿ**:
> QATåœ¨è¨“ç·´æ™‚æ¨¡æ“¬é‡åŒ–,è®“æ¨¡å‹é©æ‡‰ä½ç²¾åº¦,å¯å°‡INT4ç²¾åº¦æå¤±å¾6.7%é™è‡³2.3%

---

### ç¬¬3ç« :æ··åˆç²¾åº¦å„ªåŒ–(é€²éš)
ğŸ“– é–±è®€ [`docs/03_mixed_precision.md`](docs/03_mixed_precision.md)
ğŸ’» å¯¦ä½œ [`lab_tasks/task03_deployment/`](lab_tasks/task03_deployment/)

**å­¸ç¿’ç›®æ¨™**:
1. æ•æ„Ÿåº¦åˆ†æ:è­˜åˆ¥å°é‡åŒ–æ•æ„Ÿçš„å±¤
2. è¨­è¨ˆæ··åˆç²¾åº¦ç­–ç•¥(ä¸åŒå±¤ä¸åŒbitæ•¸)
3. è¨ˆç®—SQNR(Signal-to-Quantization-Noise Ratio)
4. éƒ¨ç½²å„ªåŒ–:æ¨¡å‹èåˆã€æ¨è«–åŠ é€Ÿ

---

## å¿«é€Ÿé–‹å§‹

### ç’°å¢ƒéœ€æ±‚

**ç¡¬é«”**:
- GPU: RTX 3070 8GB (æˆ–æ›´é«˜)
- RAM: 32GB+ (æ¨è–¦)
- å„²å­˜: 50GB+

**è»Ÿé«”**:
- Python 3.9+
- CUDA 11.8+
- PyTorch 2.0+

### å®‰è£

```bash
# å…‹éš†å°ˆæ¡ˆ
cd model_application/llm-multimodel-lab

# å®‰è£task01ä¾è³´
cd lab_tasks/task01_infographicvqa
pip install -r requirements.txt

# æ¸¬è©¦ç’°å¢ƒ
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import bitsandbytes; print('bitsandbytes OK')"
```

### ç¬¬ä¸€å€‹å¯¦é©—(15åˆ†é˜)

```bash
# é€²å…¥task01
cd lab_tasks/task01_infographicvqa

# æ¢ç´¢è³‡æ–™é›†(Jupyter Notebook)
jupyter notebook data_exploration.ipynb

# æˆ–ç›´æ¥é–‹å§‹å¾®èª¿Pix2Struct
python finetune_pix2struct.py \
  --model_name google/pix2struct-infographics-vqa-base \
  --output_dir ./outputs/pix2struct \
  --num_epochs 3 \
  --batch_size 4

# è©•ä¼°æ¨¡å‹
python evaluate.py --model_path ./outputs/pix2struct
```

## æ ¸å¿ƒæŠ€è¡“æ£§

### é‡åŒ–æŠ€è¡“

| æ–¹æ³• | ç²¾åº¦ | è¨˜æ†¶é«”ç¯€çœ | è¨“ç·´æ™‚é–“ | é©ç”¨å ´æ™¯ |
|------|------|----------|---------|---------|
| **FP16** | Baseline | 0% | 1Ã— | å……è¶³VRAM |
| **INT8 PTQ** | -0.5~2% | 50% | 0 (ç„¡éœ€è¨“ç·´) | å¿«é€Ÿéƒ¨ç½² |
| **INT4 QLoRA** | -3~7% | 75% | 1.2Ã— | å—é™VRAM |
| **INT4 QAT** | -1~3% | 75% | 1.5Ã— | ç²¾åº¦æ•æ„Ÿ |
| **Mixed Precision** | -1~2% | 60% | 1.3Ã— | æœ€ä½³å¹³è¡¡ |

### æ¨¡å‹é¸æ“‡æŒ‡å—

**é‡å°InfographicVQAä»»å‹™(8GB VRAM)**:

1. **google/pix2struct-infographics-vqa-base** (282M)
   - âœ“ å°ˆç‚ºInfographicè¨­è¨ˆ
   - âœ“ QLORAå¾Œåƒ…éœ€4-5GB
   - âœ“ å·²åœ¨ç›®æ¨™è³‡æ–™é›†ä¸Šé è¨“ç·´

2. **Qwen/Qwen2.5-VL-3B-Instruct** (3B)
   - âœ“ æœ€ä½³æ–‡æª”ç†è§£èƒ½åŠ›
   - âœ“ æ”¯æŒ32ké•·ä¸Šä¸‹æ–‡
   - âœ“ QLORAå¾Œç´„6-8GB

3. **google/paligemma2-3b-pt-896** (3B)
   - âœ“ é«˜è§£æåº¦(896Ã—896)
   - âœ“ é©åˆç´°ç¯€è­˜åˆ¥
   - âœ“ QLORAå¾Œç´„6-7GB

## å¸¸è¦‹å•é¡Œ

### ã€Œ8GB VRAMçœŸçš„å¤ å—?ã€

**å¤ **,ä½†éœ€è¦å„ªåŒ–æŠ€è¡“:
- âœ“ ä½¿ç”¨4-bit QLoRAé‡åŒ–
- âœ“ å•Ÿç”¨gradient checkpointing
- âœ“ ä½¿ç”¨å°batch size(2-4)
- âœ“ ä½¿ç”¨mixed precision(BF16)

å¯¦æ¸¬:Qwen2.5-VL-3B with QLoRAåœ¨8GBä¸Šbatch=2å¯ç©©å®šè¨“ç·´

---

### ã€Œæ²’æœ‰GPUèƒ½å­¸å—?ã€

**ç†è«–éƒ¨åˆ†å¯ä»¥**,å¯¦ä½œå»ºè­°ä½¿ç”¨:
- Google Colab (å…è²»T4, 15GB VRAM)
- Kaggle Notebooks (å…è²»P100)
- RunPod/Vast.ai (æŒ‰å°æ™‚ç§Ÿç”¨)

---

### ã€Œå’ŒLLMé‡åŒ–æœ‰ä»€éº¼ä¸åŒ?ã€

VLMé‡åŒ–æ›´è¤‡é›œ:
- **é¡å¤–çš„è¦–è¦ºç·¨ç¢¼å™¨**:éœ€è¦è™•ç†image encoderé‡åŒ–
- **å¤šæ¨¡æ…‹èåˆå±¤**:cross-attentionå±¤å°é‡åŒ–æ•æ„Ÿ
- **æ›´é•·çš„åºåˆ—**:image tokenså¢åŠ è¨˜æ†¶é«”å£“åŠ›

æœ¬å°ˆæ¡ˆå°ˆæ³¨æ–¼é€™äº›VLMç‰¹æœ‰çš„æŒ‘æˆ°

---

## åƒè€ƒè³‡æº

### æ ¸å¿ƒè«–æ–‡

1. **LoRA**: [Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
2. **QLoRA**: [Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
3. **InfographicVQA**: [InfographicVQA Dataset](https://arxiv.org/abs/2104.12756)
4. **Pix2Struct**: [Screenshot Parsing as Pretraining](https://arxiv.org/abs/2210.03347)

### å¯¦ç”¨å·¥å…·

- [HuggingFace PEFT](https://github.com/huggingface/peft)
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
- [Unsloth](https://github.com/unslothai/unsloth) - æ›´å¿«çš„QLoRAè¨“ç·´

---

## è²¢ç»æŒ‡å—

æœ¬å°ˆæ¡ˆæ­¡è¿è²¢ç»!ç‰¹åˆ¥æ˜¯:
- æ–°çš„ä»»å‹™ç¯„ä¾‹(ChartQAã€DocVQAç­‰)
- è¨˜æ†¶é«”å„ªåŒ–æŠ€å·§
- æ¨¡å‹è©•æ¸¬çµæœ
- æ–‡æª”æ”¹é€²

---

## æˆæ¬Š

MIT License - è©³è¦‹ [LICENSE](LICENSE)

---

## è‡´è¬

æœ¬å°ˆæ¡ˆéˆæ„Ÿä¾†è‡ªæ–¼edge AIå¯¦éš›éƒ¨ç½²éœ€æ±‚,æ„Ÿè¬ä»¥ä¸‹é–‹æºå°ˆæ¡ˆ:
- HuggingFace Transformers & PEFT
- TimDettmers/bitsandbytes
- Qwenã€Google Researchåœ˜éšŠ

---

**æº–å‚™å¥½äº†å—?**

å¾é€™è£¡é–‹å§‹ â†’ [`docs/00_overview.md`](docs/00_overview.md)
