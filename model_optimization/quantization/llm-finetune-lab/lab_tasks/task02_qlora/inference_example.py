"""
QLoRA Inference Example - Qwen2.5-3B Text Generation

ç¤ºç¯„å¦‚ä½•ä½¿ç”¨è¨“ç·´å¥½çš„ QLoRA æ¨¡å‹é€²è¡Œæ–‡æœ¬ç”Ÿæˆæ¨è«–

æ”¯æ´ä¸‰ç¨®æ¨¡å¼ï¼š
1. Interactive (äº’å‹•æ¨¡å¼): å³æ™‚è¼¸å…¥ prompt ä¸¦ç”Ÿæˆæ–‡æœ¬
2. Demo (ç¯„ä¾‹æ¨¡å¼): å±•ç¤ºé è¨­ç¯„ä¾‹çš„ç”Ÿæˆçµæœ
3. Text (å–®æ¬¡ç”Ÿæˆ): å°å–®ä¸€ prompt é€²è¡Œç”Ÿæˆ

ä½œè€…: LLM Tuning Lab
æˆæ¬Š: MIT
"""

import argparse
from pathlib import Path
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from quantization_utils import create_bnb_config


# ====================================================================================
# 1) è¼‰å…¥æ¨¡å‹èˆ‡ LoRA Adapter
# ====================================================================================

def load_model_with_lora(
    base_model_name: str,
    adapter_dir: str,
    use_4bit: bool = True,
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
):
    """
    è¼‰å…¥å¸¶æœ‰ LoRA adapter çš„é‡åŒ–æ¨¡å‹

    Args:
        base_model_name: åŸºç¤æ¨¡å‹åç¨±ï¼ˆä¾‹ï¼š\"Qwen/Qwen2.5-3B-Instruct\"ï¼‰
        adapter_dir: LoRA adapter ç›®éŒ„
        use_4bit: æ˜¯å¦ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆæ¨è«–æ™‚ä¹Ÿå¯ç¯€çœé¡¯å­˜ï¼‰
        device: è¨­å‚™ï¼ˆcuda/cpuï¼‰

    Returns:
        (model, tokenizer): è¼‰å…¥å¥½çš„æ¨¡å‹èˆ‡ tokenizer

    è¼‰å…¥æµç¨‹ï¼š
        1. è¼‰å…¥é‡åŒ–çš„åŸºç¤æ¨¡å‹ï¼ˆ4-bit NF4ï¼‰
        2. å¥—ç”¨ LoRA adapter æ¬Šé‡
        3. è¼‰å…¥ tokenizer

    ç¯„ä¾‹ï¼š
        >>> model, tokenizer = load_model_with_lora(
        ...     \"Qwen/Qwen2.5-3B-Instruct\",
        ...     \"./output_qlora_qwen_3b\"
        ... )
    """
    print(f"ğŸ“¥ Loading base model: {base_model_name}")

    if use_4bit and torch.cuda.is_available():
        # ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆç¯€çœé¡¯å­˜ï¼‰
        bnb_config = create_bnb_config()
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        print(f"   âœ… Base model loaded (4-bit NF4)")
    else:
        # ä¸ä½¿ç”¨é‡åŒ–ï¼ˆCPU æˆ–æ¸¬è©¦ç”¨ï¼‰
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
        )
        print(f"   âœ… Base model loaded (FP16)")

    # è¼‰å…¥ LoRA adapter
    print(f"ğŸ”§ Loading LoRA adapter from: {adapter_dir}")
    model = PeftModel.from_pretrained(base_model, adapter_dir)
    print(f"   âœ… LoRA adapter loaded")

    # è¼‰å…¥ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        trust_remote_code=True
    )

    # è¨­å®š padding tokenï¼ˆæŸäº›æ¨¡å‹éœ€è¦ï¼‰
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(f"âœ… Model and tokenizer ready!")

    return model, tokenizer


# ====================================================================================
# 2) æ–‡æœ¬ç”Ÿæˆå‡½æ•¸
# ====================================================================================

def generate_text(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 128,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
    do_sample: bool = True,
) -> str:
    """
    ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬

    Args:
        model: æ¨¡å‹
        tokenizer: Tokenizer
        prompt: è¼¸å…¥ prompt
        max_new_tokens: æœ€å¤§ç”Ÿæˆé•·åº¦ï¼ˆé»˜èª 128ï¼‰
        temperature: æº«åº¦åƒæ•¸ï¼ˆæ§åˆ¶éš¨æ©Ÿæ€§ï¼Œé»˜èª 0.7ï¼‰
            - è¶Šé«˜è¶Šéš¨æ©Ÿï¼ˆ1.0+ï¼‰
            - è¶Šä½è¶Šç¢ºå®šï¼ˆ0.1-0.5ï¼‰
        top_p: Nucleus sampling åƒæ•¸ï¼ˆé»˜èª 0.9ï¼‰
            - ç´¯ç©æ¦‚ç‡é–¾å€¼
            - åªå¾ç´¯ç©æ¦‚ç‡é”åˆ° top_p çš„ tokens ä¸­æ¡æ¨£
        top_k: Top-K sampling åƒæ•¸ï¼ˆé»˜èª 50ï¼‰
            - åªå¾æ¦‚ç‡æœ€é«˜çš„ k å€‹ tokens ä¸­æ¡æ¨£
        repetition_penalty: é‡è¤‡æ‡²ç½°ï¼ˆé»˜èª 1.1ï¼‰
            - > 1.0: æ‡²ç½°é‡è¤‡
            - = 1.0: ä¸æ‡²ç½°
        do_sample: æ˜¯å¦ä½¿ç”¨æ¡æ¨£ï¼ˆé»˜èª Trueï¼‰
            - True: éš¨æ©Ÿæ¡æ¨£ï¼ˆæ›´å¤šæ¨£åŒ–ï¼‰
            - False: Greedy decodingï¼ˆæ›´ç¢ºå®šï¼‰

    Returns:
        ç”Ÿæˆçš„æ–‡æœ¬

    ç”Ÿæˆç­–ç•¥èªªæ˜ï¼š
        - temperature: æ§åˆ¶åˆ†å¸ƒå¹³æ»‘åº¦
          * é«˜æº«ï¼ˆ1.0+ï¼‰: æ›´éš¨æ©Ÿï¼Œæ›´æœ‰å‰µæ„
          * ä½æº«ï¼ˆ0.1-0.5ï¼‰: æ›´ç¢ºå®šï¼Œæ›´ä¿å®ˆ
        - top_p (nucleus sampling): å‹•æ…‹æˆªæ–·
          * 0.9: å¾ç´¯ç©æ¦‚ç‡ 90% çš„ tokens ä¸­æ¡æ¨£
          * æ›´é©åˆé–‹æ”¾å¼ç”Ÿæˆ
        - top_k: å›ºå®šæˆªæ–·
          * 50: åªè€ƒæ…®æ¦‚ç‡æœ€é«˜çš„ 50 å€‹ tokens
          * é˜²æ­¢é¸åˆ°æ¥µä½æ¦‚ç‡çš„ tokens

    ç¯„ä¾‹ï¼š
        >>> text = generate_text(
        ...     model, tokenizer,
        ...     prompt=\"Explain quantum computing in simple terms.\",
        ...     temperature=0.7,
        ...     max_new_tokens=100
        ... )
    """
    # è¨­å®šæ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
    model.eval()

    # Tokenize è¼¸å…¥
    inputs = tokenizer(prompt, return_tensors="pt")

    # ç§»åˆ°æ¨¡å‹æ‰€åœ¨è¨­å‚™
    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # ç”Ÿæˆ
    with torch.inference_mode():  # æ¨è«–æ¨¡å¼ï¼ˆç¯€çœè¨˜æ†¶é«”ï¼‰
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            do_sample=do_sample,
            pad_token_id=tokenizer.eos_token_id,
        )

    # Decode è¼¸å‡º
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_text


# ====================================================================================
# 3) äº’å‹•æ¨¡å¼
# ====================================================================================

def interactive_mode(model, tokenizer, config):
    """
    äº’å‹•æ¨¡å¼ï¼šè®“ä½¿ç”¨è€…è¼¸å…¥ prompt é€²è¡Œå³æ™‚ç”Ÿæˆ

    Args:
        model: æ¨¡å‹
        tokenizer: Tokenizer
        config: é…ç½®å­—å…¸ï¼ˆåŒ…å«ç”Ÿæˆåƒæ•¸ï¼‰

    ä½¿ç”¨èªªæ˜ï¼š
        - è¼¸å…¥ prompt é€²è¡Œç”Ÿæˆ
        - è¼¸å…¥ 'quit', 'exit', 'q' çµæŸ
        - è¼¸å…¥ 'config' æŸ¥çœ‹ç•¶å‰é…ç½®
        - è¼¸å…¥ 'reset' é‡ç½®é…ç½®

    ç¯„ä¾‹å°è©±ï¼š
        >>> è«‹è¼¸å…¥ prompt: Explain machine learning
        >>> [ç”Ÿæˆçµæœ]
        >>> è«‹è¼¸å…¥ prompt: quit
        >>> ğŸ‘‹ Goodbye!
    """
    print("\n" + "="*60)
    print("ğŸ’¬ Interactive Text Generation")
    print("="*60)
    print("è¼¸å…¥ prompt é€²è¡Œæ–‡æœ¬ç”Ÿæˆ")
    print("æŒ‡ä»¤:")
    print("  - 'quit', 'exit', 'q': çµæŸ")
    print("  - 'config': æŸ¥çœ‹ç•¶å‰é…ç½®")
    print("  - 'reset': é‡ç½®é…ç½®ç‚ºé»˜èªå€¼")
    print("-"*60)

    # é»˜èªé…ç½®
    gen_config = {
        'max_new_tokens': config.get('max_new_tokens', 128),
        'temperature': config.get('temperature', 0.7),
        'top_p': config.get('top_p', 0.9),
        'top_k': config.get('top_k', 50),
        'repetition_penalty': config.get('repetition_penalty', 1.1),
    }

    while True:
        try:
            prompt = input("\nè«‹è¼¸å…¥ prompt: ").strip()

            # æª¢æŸ¥é€€å‡ºæŒ‡ä»¤
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ Goodbye!")
                break

            # æª¢æŸ¥é…ç½®æŒ‡ä»¤
            if prompt.lower() == 'config':
                print("\nğŸ“‹ ç•¶å‰é…ç½®:")
                for key, value in gen_config.items():
                    print(f"   {key}: {value}")
                continue

            # æª¢æŸ¥é‡ç½®æŒ‡ä»¤
            if prompt.lower() == 'reset':
                gen_config = {
                    'max_new_tokens': 128,
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'top_k': 50,
                    'repetition_penalty': 1.1,
                }
                print("\nâœ… é…ç½®å·²é‡ç½®ç‚ºé»˜èªå€¼")
                continue

            # æª¢æŸ¥ç©ºè¼¸å…¥
            if not prompt:
                print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„ prompt")
                continue

            # ç”Ÿæˆ
            print(f"\nğŸ¤– ç”Ÿæˆä¸­...")
            generated = generate_text(
                model, tokenizer, prompt,
                **gen_config
            )

            # é¡¯ç¤ºçµæœ
            print(f"\n{'='*60}")
            print(f"ğŸ“ ç”Ÿæˆçµæœ:")
            print(f"{'='*60}")
            print(generated)
            print(f"{'='*60}")

            # é¡¯ç¤ºçµ±è¨ˆ
            input_length = len(tokenizer.encode(prompt))
            output_length = len(tokenizer.encode(generated))
            print(f"\nğŸ“Š çµ±è¨ˆ:")
            print(f"   è¼¸å…¥é•·åº¦: {input_length} tokens")
            print(f"   è¼¸å‡ºé•·åº¦: {output_length} tokens")
            print(f"   æ–°ç”Ÿæˆ: {output_length - input_length} tokens")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")
            print("   è«‹é‡è©¦æˆ–è¼¸å…¥ 'quit' çµæŸ")


# ====================================================================================
# 4) ç¯„ä¾‹æ¨¡å¼
# ====================================================================================

def demo_mode(model, tokenizer, config):
    """
    ç¯„ä¾‹æ¨¡å¼ï¼šå±•ç¤ºé è¨­ç¯„ä¾‹çš„ç”Ÿæˆçµæœ

    Args:
        model: æ¨¡å‹
        tokenizer: Tokenizer
        config: é…ç½®å­—å…¸

    å±•ç¤ºå…§å®¹ï¼š
        - å¤šå€‹ä¸åŒé¡å‹çš„ prompt
        - å±•ç¤ºæ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›
        - æ¶µè“‹ä¸åŒé ˜åŸŸï¼ˆè§£é‡‹ã€å‰µä½œã€ç¿»è­¯ç­‰ï¼‰
    """
    print("\n" + "="*60)
    print("ğŸ“ Demo Examples")
    print("="*60)

    # é è¨­ç¯„ä¾‹ï¼ˆæ¶µè“‹ä¸åŒé¡å‹ï¼‰
    examples = [
        {
            "prompt": "Explain quantum computing in simple terms:",
            "category": "è§£é‡‹ (Explanation)"
        },
        {
            "prompt": "Write a short poem about artificial intelligence:",
            "category": "å‰µä½œ (Creative Writing)"
        },
        {
            "prompt": "Translate the following to French: 'Hello, how are you?'",
            "category": "ç¿»è­¯ (Translation)"
        },
        {
            "prompt": "What are the benefits of regular exercise?",
            "category": "çŸ¥è­˜å•ç­” (Q&A)"
        },
        {
            "prompt": "Once upon a time in a distant galaxy,",
            "category": "çºŒå¯« (Story Continuation)"
        },
    ]

    for i, example in enumerate(examples, 1):
        print(f"\n{'='*60}")
        print(f"ç¯„ä¾‹ {i}/{len(examples)} - {example['category']}")
        print(f"{'='*60}")
        print(f"\nğŸ“¥ Prompt:")
        print(f"{example['prompt']}")

        # ç”Ÿæˆ
        print(f"\nğŸ¤– ç”Ÿæˆä¸­...")
        generated = generate_text(
            model, tokenizer,
            example['prompt'],
            max_new_tokens=config.get('max_new_tokens', 128),
            temperature=config.get('temperature', 0.7),
            top_p=config.get('top_p', 0.9),
        )

        # é¡¯ç¤ºçµæœ
        print(f"\nğŸ“¤ ç”Ÿæˆçµæœ:")
        print(f"{'-'*60}")
        print(generated)
        print(f"{'-'*60}")

        # ç­‰å¾…ç”¨æˆ¶ï¼ˆé™¤äº†æœ€å¾Œä¸€å€‹ç¯„ä¾‹ï¼‰
        if i < len(examples):
            input("\næŒ‰ Enter ç¹¼çºŒä¸‹ä¸€å€‹ç¯„ä¾‹...")


# ====================================================================================
# 5) å–®æ¬¡ç”Ÿæˆæ¨¡å¼
# ====================================================================================

def text_mode(model, tokenizer, prompt: str, config):
    """
    å–®æ¬¡ç”Ÿæˆæ¨¡å¼ï¼šå°å–®ä¸€ prompt é€²è¡Œç”Ÿæˆ

    Args:
        model: æ¨¡å‹
        tokenizer: Tokenizer
        prompt: è¼¸å…¥ prompt
        config: é…ç½®å­—å…¸

    é©ç”¨å ´æ™¯ï¼š
        - è…³æœ¬èª¿ç”¨
        - API æ•´åˆ
        - æ‰¹æ¬¡è™•ç†
    """
    print("\n" + "="*60)
    print("ğŸ“ Text Generation")
    print("="*60)

    print(f"\nğŸ“¥ Prompt:")
    print(f"{prompt}")

    # ç”Ÿæˆ
    print(f"\nğŸ¤– ç”Ÿæˆä¸­...")
    generated = generate_text(
        model, tokenizer, prompt,
        max_new_tokens=config.get('max_new_tokens', 128),
        temperature=config.get('temperature', 0.7),
        top_p=config.get('top_p', 0.9),
    )

    # é¡¯ç¤ºçµæœ
    print(f"\nğŸ“¤ ç”Ÿæˆçµæœ:")
    print(f"{'='*60}")
    print(generated)
    print(f"{'='*60}")

    # é¡¯ç¤ºçµ±è¨ˆ
    input_length = len(tokenizer.encode(prompt))
    output_length = len(tokenizer.encode(generated))
    print(f"\nğŸ“Š çµ±è¨ˆ:")
    print(f"   è¼¸å…¥é•·åº¦: {input_length} tokens")
    print(f"   è¼¸å‡ºé•·åº¦: {output_length} tokens")
    print(f"   æ–°ç”Ÿæˆ: {output_length - input_length} tokens")


# ====================================================================================
# 6) ä¸»å‡½æ•¸
# ====================================================================================

def main(args):
    # æª¢æŸ¥ adapter ç›®éŒ„
    adapter_path = Path(args.adapter_dir)
    if not adapter_path.exists():
        print(f"âŒ Adapter directory not found: {args.adapter_dir}")
        print("   è«‹å…ˆè¨“ç·´æ¨¡å‹: python train_qlora.py")
        return

    # è¼‰å…¥æ¨¡å‹
    print("="*60)
    print("ğŸš€ QLoRA Inference - Qwen2.5-3B")
    print("="*60)

    model, tokenizer = load_model_with_lora(
        args.model_name,
        args.adapter_dir,
        use_4bit=args.use_4bit
    )

    # ç”Ÿæˆé…ç½®
    gen_config = {
        'max_new_tokens': args.max_new_tokens,
        'temperature': args.temperature,
        'top_p': args.top_p,
        'top_k': args.top_k,
        'repetition_penalty': args.repetition_penalty,
    }

    # æ ¹æ“šæ¨¡å¼åŸ·è¡Œ
    if args.mode == "interactive":
        interactive_mode(model, tokenizer, gen_config)

    elif args.mode == "demo":
        demo_mode(model, tokenizer, gen_config)

    elif args.mode == "text":
        if not args.prompt:
            print("âŒ è«‹ä½¿ç”¨ --prompt åƒæ•¸æä¾›è¼¸å…¥æ–‡æœ¬")
            print("   ç¯„ä¾‹: python inference_example.py --mode text --prompt \"Explain AI\"")
            return
        text_mode(model, tokenizer, args.prompt, gen_config)

    else:
        print(f"âŒ æœªçŸ¥æ¨¡å¼: {args.mode}")
        print("   å¯ç”¨æ¨¡å¼: interactive, demo, text")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="QLoRA Text Generation Inference"
    )

    # æ¨¡å‹ç›¸é—œ
    parser.add_argument(
        "--model_name",
        type=str,
        default="Qwen/Qwen2.5-3B-Instruct",
        help="Base model name (default: Qwen/Qwen2.5-3B-Instruct)"
    )
    parser.add_argument(
        "--adapter_dir",
        type=str,
        default="./output_qlora_qwen_3b",
        help="LoRA adapter directory (default: ./output_qlora_qwen_3b)"
    )
    parser.add_argument(
        "--use_4bit",
        action="store_true",
        default=True,
        help="Use 4-bit quantization for inference (default: True)"
    )

    # æ¨¡å¼é¸æ“‡
    parser.add_argument(
        "--mode",
        type=str,
        default="interactive",
        choices=["interactive", "demo", "text"],
        help="Inference mode (default: interactive)"
    )
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="Input prompt for text mode"
    )

    # ç”Ÿæˆåƒæ•¸
    parser.add_argument(
        "--max_new_tokens",
        type=int,
        default=128,
        help="Maximum number of tokens to generate (default: 128)"
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="Temperature for sampling (default: 0.7)"
    )
    parser.add_argument(
        "--top_p",
        type=float,
        default=0.9,
        help="Top-p (nucleus) sampling parameter (default: 0.9)"
    )
    parser.add_argument(
        "--top_k",
        type=int,
        default=50,
        help="Top-k sampling parameter (default: 50)"
    )
    parser.add_argument(
        "--repetition_penalty",
        type=float,
        default=1.1,
        help="Repetition penalty (default: 1.1)"
    )

    args = parser.parse_args()
    main(args)
