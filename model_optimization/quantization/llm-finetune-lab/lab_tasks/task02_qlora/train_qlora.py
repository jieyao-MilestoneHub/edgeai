"""
QLoRA Training Script - Qwen2.5-3B Language Model Fine-tuning

å®Œæ•´çš„ QLoRA è¨“ç·´æµç¨‹ï¼Œæ”¯æ´ï¼š
- å¾ config.yaml è®€å–é…ç½®
- è‡ªå‹•å¥—ç”¨ 4-bit NF4 é‡åŒ–èˆ‡ LoRA
- Wikitext-2 èªè¨€æ¨¡å‹è¨“ç·´ï¼ˆå¯æ›¿æ›ç‚ºè‡ªè¨‚è³‡æ–™é›†ï¼‰
- å®Œæ•´çš„è¨“ç·´èˆ‡è©•ä¼°å¾ªç’°
- Loss/Perplexity è©•ä¼°æŒ‡æ¨™
- è¨“ç·´é€²åº¦è¦–è¦ºåŒ–
- LoRA adapter å„²å­˜èˆ‡è¼‰å…¥
- è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§

è¨˜æ†¶é«”éœ€æ±‚ï¼š
- GTX 4060 (8GB): âœ… å¯è¨“ç·´ï¼ˆbatch_size=1, gradient_accumulation=16ï¼‰
- RTX 3090 (24GB): âœ… å¯è¨“ç·´ï¼ˆbatch_size=4, gradient_accumulation=4ï¼‰
- RTX 4090 (24GB): âœ… å¯è¨“ç·´ï¼ˆbatch_size=4, gradient_accumulation=4ï¼‰

ä½œè€…: LLM Tuning Lab
æˆæ¬Š: MIT
"""

import os
import yaml
import argparse
from pathlib import Path
from typing import Dict, Any

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import (
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
)
import matplotlib.pyplot as plt
from tqdm import tqdm

# å¾åŒç›®éŒ„å°å…¥æˆ‘å€‘å¯¦ä½œçš„ QLoRA æ¨¡çµ„
from quantization_utils import (
    load_model_and_tokenizer,
    create_lora_config,
    prepare_model_for_training,
    print_memory_stats,
)


# ====================================================================================
# 1) è¼‰å…¥é…ç½®
# ====================================================================================

def load_config(config_path: str = "config.yaml") -> Dict[str, Any]:
    """
    è¼‰å…¥ YAML é…ç½®æª”

    Args:
        config_path: é…ç½®æª”æ¡ˆè·¯å¾‘ï¼ˆé»˜èª config.yamlï¼‰

    Returns:
        é…ç½®å­—å…¸

    ç¯„ä¾‹ï¼š
        >>> config = load_config("config.yaml")
        >>> print(config['model']['name'])
        Qwen/Qwen2.5-3B-Instruct
    """
    print(f"ğŸ“‹ Loading config from: {config_path}")
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    print(f"âœ… Config loaded successfully!")
    return config


# ====================================================================================
# 2) æº–å‚™è³‡æ–™é›†
# ====================================================================================

def prepare_dataset(config: Dict[str, Any], tokenizer):
    """
    æº–å‚™ä¸¦é è™•ç†è³‡æ–™é›†

    Args:
        config: é…ç½®å­—å…¸
        tokenizer: Tokenizer ç‰©ä»¶

    Returns:
        tokenized_datasets: Tokenize å¾Œçš„è³‡æ–™é›†

    è™•ç†æµç¨‹ï¼š
        1. è¼‰å…¥è³‡æ–™é›†ï¼ˆWikitext-2ï¼‰
        2. Tokenizationï¼ˆè½‰æ›ç‚º input_idsï¼‰
        3. è¨­å®š labelsï¼ˆèªè¨€æ¨¡å‹ä»»å‹™ï¼šlabels = input_idsï¼‰
        4. ç§»é™¤æœªä½¿ç”¨çš„æ¬„ä½

    æ³¨æ„ï¼š
        - Wikitext-2 æ˜¯èªè¨€æ¨¡å‹åŸºæº–è³‡æ–™é›†
        - å¯æ›¿æ›ç‚ºè‡ªè¨‚è³‡æ–™é›†ï¼ˆä¿®æ”¹ dataset_name å’Œ tokenize_functionï¼‰
    """
    print(f"\nğŸ“š Loading dataset: {config['data']['dataset_name']}/{config['data']['dataset_config']}")

    # è¼‰å…¥è³‡æ–™é›†
    dataset = load_dataset(
        config['data']['dataset_name'],
        config['data']['dataset_config']
    )

    print(f"   Train samples: {len(dataset['train'])}")
    print(f"   Validation samples: {len(dataset['validation'])}")

    # Tokenization å‡½æ•¸
    def tokenize_function(examples):
        """
        å°‡æ–‡æœ¬è½‰æ›ç‚º token IDs

        èªè¨€æ¨¡å‹ä»»å‹™ï¼š
            - input_ids: æ¨¡å‹è¼¸å…¥
            - labels: é æ¸¬ç›®æ¨™ï¼ˆèªè¨€æ¨¡å‹ä¸­ labels = input_idsï¼‰
        """
        result = tokenizer(
            examples["text"],
            truncation=True,
            max_length=config['data']['max_length'],
            padding="max_length",  # å›ºå®šé•·åº¦ï¼ˆè¨“ç·´æ•ˆç‡ï¼‰
        )
        # èªè¨€æ¨¡å‹ä»»å‹™ï¼šlabels = input_ids
        result["labels"] = result["input_ids"].copy()
        return result

    # æ‰¹æ¬¡è™•ç†è³‡æ–™é›†
    print(f"ğŸ”„ Tokenizing dataset (max_length={config['data']['max_length']})...")
    tokenized_datasets = dataset.map(
        tokenize_function,
        batched=True,
        num_proc=config['data']['preprocessing_num_proc'],
        remove_columns=dataset["train"].column_names,  # ç§»é™¤åŸå§‹æ–‡æœ¬æ¬„ä½
        desc="Tokenizing",
    )

    print(f"âœ… Dataset tokenized successfully!")

    return tokenized_datasets


# ====================================================================================
# 3) è¨ˆç®—è©•ä¼°æŒ‡æ¨™ï¼ˆPerplexityï¼‰
# ====================================================================================

def compute_metrics(eval_pred):
    """
    è¨ˆç®—è©•ä¼°æŒ‡æ¨™ï¼ˆç”¨æ–¼ Trainerï¼‰

    Args:
        eval_pred: (predictions, labels) tuple

    Returns:
        metrics: åŒ…å« perplexity çš„å­—å…¸

    Perplexity (å›°æƒ‘åº¦)ï¼š
        - èªè¨€æ¨¡å‹çš„æ¨™æº–è©•ä¼°æŒ‡æ¨™
        - å…¬å¼ï¼šperplexity = exp(loss)
        - è¶Šä½è¶Šå¥½ï¼ˆè¡¨ç¤ºæ¨¡å‹é æ¸¬è¶Šæº–ç¢ºï¼‰
        - ä¾‹ï¼šperplexity=10 è¡¨ç¤ºæ¯æ­¥å¹³å‡åœ¨ 10 å€‹å€™é¸ä¸­é¸æ“‡
    """
    predictions, labels = eval_pred
    # æ³¨æ„ï¼šTrainer æœƒè‡ªå‹•è¨ˆç®— lossï¼Œé€™è£¡åªéœ€è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
    # å¯¦éš›ä¸Š compute_metrics åœ¨é€™è£¡ä¸æœƒè¢«å‘¼å«ï¼Œå› ç‚ºæˆ‘å€‘ä½¿ç”¨é»˜èªçš„ loss
    # ä¿ç•™æ­¤å‡½æ•¸ä½œç‚ºç¯„ä¾‹
    return {}


# ====================================================================================
# 4) ç¹ªè£½è¨“ç·´æ›²ç·š
# ====================================================================================

def plot_training_curves(
    log_history: list,
    save_path: str = "training_curves.png"
):
    """
    ç¹ªè£½è¨“ç·´æ›²ç·šï¼ˆLoss å’Œ Learning Rateï¼‰

    Args:
        log_history: Trainer çš„ log history
        save_path: å„²å­˜è·¯å¾‘

    ç¹ªè£½å…§å®¹ï¼š
        - å·¦åœ–ï¼šTraining Loss å’Œ Validation Loss
        - å³åœ–ï¼šLearning Rate è®ŠåŒ–

    æ³¨æ„ï¼š
        - Trainer æœƒè‡ªå‹•è¨˜éŒ„ log_history
        - å¯å¾ trainer.state.log_history å–å¾—
    """
    print(f"\nğŸ“Š Plotting training curves...")

    # æå–è³‡æ–™
    train_loss = []
    eval_loss = []
    learning_rates = []
    steps = []

    for entry in log_history:
        if 'loss' in entry:  # Training loss
            train_loss.append((entry['step'], entry['loss']))
        if 'eval_loss' in entry:  # Validation loss
            eval_loss.append((entry['step'], entry['eval_loss']))
        if 'learning_rate' in entry:  # Learning rate
            learning_rates.append((entry['step'], entry['learning_rate']))

    # ç¹ªè£½
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    # Loss æ›²ç·š
    if train_loss:
        steps_train, losses_train = zip(*train_loss)
        ax1.plot(steps_train, losses_train, 'b-o', label='Train Loss', linewidth=2, markersize=4)
    if eval_loss:
        steps_eval, losses_eval = zip(*eval_loss)
        ax1.plot(steps_eval, losses_eval, 'r-s', label='Eval Loss', linewidth=2, markersize=4)

    ax1.set_xlabel('Steps', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')
    ax1.legend(fontsize=11)
    ax1.grid(True, alpha=0.3)

    # Learning Rate æ›²ç·š
    if learning_rates:
        steps_lr, lrs = zip(*learning_rates)
        ax2.plot(steps_lr, lrs, 'g-', label='Learning Rate', linewidth=2)
        ax2.set_xlabel('Steps', fontsize=12)
        ax2.set_ylabel('Learning Rate', fontsize=12)
        ax2.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')
        ax2.legend(fontsize=11)
        ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… Training curves saved to: {save_path}")


# ====================================================================================
# 5) ä¸»è¨“ç·´å‡½æ•¸
# ====================================================================================

def main(args):
    # ========== 1. è¼‰å…¥é…ç½® ==========
    config_path = args.config if args.config else "config.yaml"
    config = load_config(config_path)

    # å‘½ä»¤åˆ—åƒæ•¸è¦†å¯«é…ç½®
    if args.rank is not None:
        config['lora']['rank'] = args.rank
    if args.alpha is not None:
        config['lora']['alpha'] = args.alpha
    if args.num_epochs is not None:
        config['training']['num_epochs'] = args.num_epochs
    if args.output_dir is not None:
        config['output']['dir'] = args.output_dir

    # ========== 2. è¨­å®šè£ç½®èˆ‡éš¨æ©Ÿç¨®å­ ==========
    if not torch.cuda.is_available():
        print("âŒ CUDA not available. QLoRA requires GPU.")
        print("   Please check your CUDA installation and GPU availability.")
        return

    device = torch.device('cuda')
    print(f"\nğŸ–¥ï¸  Device Information:")
    print(f"   Device: {device}")
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # è¨­å®šéš¨æ©Ÿç¨®å­ï¼ˆå¯é‡ç¾æ€§ï¼‰
    torch.manual_seed(config['advanced']['seed'])
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(config['advanced']['seed'])

    # å•Ÿç”¨ TF32 åŠ é€Ÿï¼ˆAmpere æ¶æ§‹åŠä»¥ä¸Šï¼‰
    if config['advanced']['allow_tf32']:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        print(f"   TF32 acceleration: Enabled")

    # ========== 3. è¼‰å…¥æ¨¡å‹èˆ‡ Tokenizer ==========
    print("\n" + "="*60)
    print("ğŸš€ QLoRA Training - Qwen2.5-3B")
    print("="*60)

    model, tokenizer = load_model_and_tokenizer(config['model']['name'])
    print_memory_stats("After loading model")

    # ========== 4. å¥—ç”¨ LoRA ==========
    print(f"\nğŸ”§ Applying LoRA:")
    print(f"   Rank: {config['lora']['rank']}")
    print(f"   Alpha: {config['lora']['alpha']}")
    print(f"   Dropout: {config['lora']['dropout']}")
    print(f"   Target modules: {', '.join(config['lora']['target_modules'])}")

    lora_config = create_lora_config(
        r=config['lora']['rank'],
        alpha=config['lora']['alpha'],
        dropout=config['lora']['dropout'],
        target_modules=config['lora']['target_modules'],
    )

    model = prepare_model_for_training(
        model,
        lora_config,
        use_gradient_checkpointing=config['training']['gradient_checkpointing']
    )

    print_memory_stats("After applying LoRA")

    # ========== 5. æº–å‚™è³‡æ–™é›† ==========
    tokenized_datasets = prepare_dataset(config, tokenizer)

    # Data Collatorï¼ˆè™•ç†å‹•æ…‹ padding å’Œ labelsï¼‰
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False  # å› æœèªè¨€æ¨¡å‹ï¼ˆä¸ä½¿ç”¨ masked language modelingï¼‰
    )

    # ========== 6. è¨­å®šè¨“ç·´åƒæ•¸ ==========
    output_dir = Path(config['output']['dir'])
    output_dir.mkdir(parents=True, exist_ok=True)

    training_args = TrainingArguments(
        # è¼¸å‡ºç›®éŒ„
        output_dir=str(output_dir),
        overwrite_output_dir=config['output']['overwrite_output_dir'],

        # è¨“ç·´åƒæ•¸
        num_train_epochs=config['training']['num_epochs'],
        per_device_train_batch_size=config['training']['batch_size'],
        per_device_eval_batch_size=config['training']['batch_size'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],

        # å­¸ç¿’ç‡
        learning_rate=config['training']['learning_rate'],
        weight_decay=config['training']['weight_decay'],
        warmup_ratio=config['training']['warmup_ratio'],
        lr_scheduler_type=config['training']['lr_scheduler_type'],

        # ç²¾åº¦è¨­å®š
        fp16=config['training']['fp16'],
        bf16=config['training']['bf16'],

        # å„ªåŒ–å™¨
        optim=config['training']['optim'],  # paged_adamw_8bit
        max_grad_norm=config['training']['max_grad_norm'],

        # æ—¥èªŒèˆ‡å„²å­˜
        logging_dir=config['output']['logging_dir'],
        logging_steps=config['training']['logging_steps'],
        save_steps=config['training']['save_steps'],
        save_total_limit=config['training']['save_total_limit'],

        # è©•ä¼°
        eval_strategy="steps",
        eval_steps=config['training']['eval_steps'],

        # å…¶ä»–
        report_to="tensorboard",
        seed=config['advanced']['seed'],
        gradient_checkpointing=config['training']['gradient_checkpointing'],
        ddp_find_unused_parameters=config['advanced']['ddp_find_unused_parameters'],
    )

    print(f"\nğŸ“ Training Configuration:")
    print(f"   Epochs: {config['training']['num_epochs']}")
    print(f"   Batch size: {config['training']['batch_size']}")
    print(f"   Gradient accumulation: {config['training']['gradient_accumulation_steps']}")
    print(f"   Effective batch size: {config['training']['batch_size'] * config['training']['gradient_accumulation_steps']}")
    print(f"   Learning rate: {config['training']['learning_rate']}")
    print(f"   Warmup ratio: {config['training']['warmup_ratio']}")
    print(f"   Optimizer: {config['training']['optim']}")
    print(f"   Precision: {'BF16' if config['training']['bf16'] else 'FP16' if config['training']['fp16'] else 'FP32'}")
    print(f"   Gradient checkpointing: {config['training']['gradient_checkpointing']}")

    # ========== 7. å»ºç«‹ Trainer ==========
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets[config['data']['train_split']],
        eval_dataset=tokenized_datasets[config['data']['eval_split']],
        data_collator=data_collator,
    )

    # ========== 8. é–‹å§‹è¨“ç·´ ==========
    print("\n" + "="*60)
    print("ğŸš€ Starting Training...")
    print("="*60)

    print_memory_stats("Before training")

    try:
        # è¨“ç·´
        train_result = trainer.train()

        print("\n" + "="*60)
        print("âœ… Training Completed!")
        print("="*60)

        # é¡¯ç¤ºè¨“ç·´çµæœ
        print(f"\nğŸ“Š Training Results:")
        print(f"   Final Train Loss: {train_result.training_loss:.4f}")
        print(f"   Training Time: {train_result.metrics['train_runtime']:.2f} seconds")
        print(f"   Samples/Second: {train_result.metrics['train_samples_per_second']:.2f}")

        # ========== 9. æœ€çµ‚è©•ä¼° ==========
        print(f"\nğŸ“ˆ Running Final Evaluation...")
        eval_result = trainer.evaluate()
        print(f"   Final Eval Loss: {eval_result['eval_loss']:.4f}")

        # è¨ˆç®— Perplexity
        perplexity = torch.exp(torch.tensor(eval_result['eval_loss']))
        print(f"   Final Perplexity: {perplexity:.2f}")

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Training interrupted by user!")
        print("   Saving current state...")

    # ========== 10. å„²å­˜çµæœ ==========
    print("\n" + "="*60)
    print("ğŸ’¾ Saving Results...")
    print("="*60)

    # 10.1 å„²å­˜ LoRA adapter
    print(f"ğŸ“¦ Saving LoRA adapter...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ… LoRA adapter saved to: {output_dir}")

    # 10.2 ç¹ªè£½è¨“ç·´æ›²ç·š
    if config['output']['save_training_curves']:
        plot_training_curves(
            trainer.state.log_history,
            save_path=str(output_dir / "training_curves.png")
        )

    # 10.3 å„²å­˜è¨“ç·´æ—¥èªŒ
    if config['output']['save_training_log']:
        log_path = output_dir / "training_log.txt"
        with open(log_path, 'w', encoding='utf-8') as f:
            f.write("QLoRA Training Log - Qwen2.5-3B\n")
            f.write("="*60 + "\n\n")
            f.write(f"Model: {config['model']['name']}\n")
            f.write(f"Dataset: {config['data']['dataset_name']}/{config['data']['dataset_config']}\n")
            f.write(f"Task: Causal Language Modeling\n\n")

            f.write(f"LoRA Config:\n")
            f.write(f"  Rank: {config['lora']['rank']}\n")
            f.write(f"  Alpha: {config['lora']['alpha']}\n")
            f.write(f"  Dropout: {config['lora']['dropout']}\n")
            f.write(f"  Target modules: {', '.join(config['lora']['target_modules'])}\n\n")

            f.write(f"Training Config:\n")
            f.write(f"  Epochs: {config['training']['num_epochs']}\n")
            f.write(f"  Batch size: {config['training']['batch_size']}\n")
            f.write(f"  Gradient accumulation: {config['training']['gradient_accumulation_steps']}\n")
            f.write(f"  Learning rate: {config['training']['learning_rate']}\n\n")

            if 'train_result' in locals():
                f.write(f"Results:\n")
                f.write(f"  Final Train Loss: {train_result.training_loss:.4f}\n")
                f.write(f"  Final Eval Loss: {eval_result['eval_loss']:.4f}\n")
                f.write(f"  Final Perplexity: {perplexity:.2f}\n")
                f.write(f"  Training Time: {train_result.metrics['train_runtime']:.2f}s\n")

        print(f"âœ… Training log saved to: {log_path}")

    print_memory_stats("After training")

    # ========== 11. å®Œæˆ ==========
    print("\n" + "="*60)
    print("ğŸ‰ All Done!")
    print("="*60)
    print(f"\nğŸ“ Output Directory: {output_dir.absolute()}")
    print(f"\nâœ¨ Files created:")
    print(f"   - adapter_config.json (LoRA é…ç½®)")
    print(f"   - adapter_model.safetensors (LoRA æ¬Šé‡)")
    print(f"   - training_curves.png (è¨“ç·´æ›²ç·š)")
    print(f"   - training_log.txt (è¨“ç·´æ—¥èªŒ)")
    print(f"\nğŸ’¡ Next steps:")
    print(f"   - Run inference: python inference_example.py")
    print(f"   - View TensorBoard: tensorboard --logdir {config['output']['logging_dir']}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Train Qwen2.5-3B with QLoRA on Language Modeling Task"
    )
    parser.add_argument(
        "--config",
        type=str,
        default="config.yaml",
        help="Path to config file (default: config.yaml)"
    )
    parser.add_argument(
        "--rank",
        type=int,
        default=None,
        help="LoRA rank (overrides config)"
    )
    parser.add_argument(
        "--alpha",
        type=float,
        default=None,
        help="LoRA alpha (overrides config)"
    )
    parser.add_argument(
        "--num_epochs",
        type=int,
        default=None,
        help="Number of epochs (overrides config)"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Output directory (overrides config)"
    )

    args = parser.parse_args()
    main(args)
